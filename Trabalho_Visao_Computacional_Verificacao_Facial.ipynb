{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4c4ae182",
      "metadata": {
        "id": "4c4ae182"
      },
      "source": [
        "\n",
        "# Verifica√ß√£o Facial ‚Äî Colab/Local (Webcam + Preview)  \n",
        "- **Detecta ambiente Colab/Local**:  \n",
        "  - `IN_COLAB` (True/False)\n",
        "- **Detectores de face**:  \n",
        "  - `haar` (OpenCV CascadeClassifier)  \n",
        "  - `dnn_ssd_resnet10` (OpenCV DNN com Caffe)\n",
        "- **Runner interativo**:\n",
        "  - **Inclus√£o (novo)**: coleta amostras de um novo usu√°rio e treina LBPH  \n",
        "  - **Autentica√ß√£o (auth)**: modos **1:1** (usu√°rio esperado) e **1:N** (identifica√ß√£o)\n",
        "  - **Liveness**: checagem simples de energia do sinal\n",
        "- **Avalia√ß√£o offline** renovada:\n",
        "  - Suporta **.zip / .tar / .tar.gz / .tgz** e **extra√ß√£o recursiva** (ex.: ZIP contendo TAR.GZ).\n",
        "  - **Remoto default** (POSITIVES): **Caltech Face 1999** (`faces.tar`, CaltechDATA).\n",
        "  - **Fallback NEGATIVES**: **Caltech-101** (extrai `BACKGROUND_Google`; se ausente, usa outras categorias ‚â† ‚Äúface‚Äù).\n",
        "  - **C√¢mera tamb√©m em Remoto/Default** quando `negatives/` estiver vazio (op√ß√£o interativa).\n",
        "  - Funciona com **single-class**: converte todas as imagens extra√≠das em `positives/` e completa `negatives/` automaticamente.\n",
        "- **M√©tricas e gr√°ficos**: matriz de confus√£o, accuracy, precision, recall, F1, tempo m√©dio; tabela comparativa (se `pandas` dispon√≠vel).\n",
        "- **Dicas de tuning**: `conf_threshold` para DNN, `minNeighbors/scaleFactor/minSize` para Haar, filtros p√≥s-detec√ß√£o (raz√£o w/h e tamanho).\n",
        "- **Downloads autom√°ticos**:\n",
        "  - `deploy.prototxt` e `res10_300x300_ssd_iter_140000.caffemodel` (DNN SSD-ResNet10)\n",
        "  - Dataset remoto padr√£o (CBCL/MIT) reorganizado em `positives/` e `negatives/`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c0088b9",
      "metadata": {
        "id": "5c0088b9"
      },
      "source": [
        "## 1) Instala√ß√£o (reinicie o runtime ap√≥s rodar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d0a9722c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0a9722c",
        "outputId": "2e8fb52d-3778-4448-de00-2cb35b52cb66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python==4.8.1.78 in /usr/local/lib/python3.12/dist-packages (4.8.1.78)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "OpenCV: 4.8.1\n",
            "NumPy: 1.26.4\n",
            "cv2.face OK\n"
          ]
        }
      ],
      "source": [
        "# Recomendado: ap√≥s executar, v√° em Runtime > Restart runtime\n",
        "#!pip uninstall -y opencv-python opencv-contrib-python numpy\n",
        "!pip install --no-cache-dir numpy==1.26.4 opencv-contrib-python==4.8.1.78 matplotlib\n",
        "\n",
        "import cv2, numpy as np\n",
        "print(\"OpenCV:\", cv2.__version__)\n",
        "print(\"NumPy:\", np.__version__)\n",
        "try:\n",
        "    _ = cv2.face.LBPHFaceRecognizer_create()\n",
        "    print(\"cv2.face OK\")\n",
        "except Exception as e:\n",
        "    print(\"Falha cv2.face:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c0cfa7d",
      "metadata": {
        "id": "0c0cfa7d"
      },
      "source": [
        "## 2) Imports, diret√≥rios e par√¢metros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "207dadc2",
      "metadata": {
        "id": "207dadc2"
      },
      "outputs": [],
      "source": [
        "import base64, json, time, uuid, math\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Javascript, display\n",
        "\n",
        "# Estrutura de pastas\n",
        "DATA_DIR = Path(\"cv_colab_data\")\n",
        "ENROLL_DIR = DATA_DIR/\"enroll\"\n",
        "EVIDENCE_DIR = DATA_DIR/\"evidence\"\n",
        "for d in (DATA_DIR, ENROLL_DIR, EVIDENCE_DIR):\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Par√¢metros globais (ajuste conforme sua calibra√ß√£o/ambiente)\n",
        "SERVICE_THRESHOLD = 55.0       # ser√° ajustado automaticamente na calibra√ß√£o\n",
        "LIVENESS_MIN_ENERGY = 8.0      # energia m√≠nima m√©dia para ser \"live\"\n",
        "\n",
        "def show_bgr(img, title=\"preview\"):\n",
        "    rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.imshow(rgb)\n",
        "    plt.title(title)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1196b8d6",
      "metadata": {
        "id": "1196b8d6"
      },
      "source": [
        "## 3) Pr√©-processamento (CLAHE + blur)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "799dc6d4",
      "metadata": {
        "id": "799dc6d4"
      },
      "outputs": [],
      "source": [
        "def preprocess_face_gray(gray_200x200):\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    g = clahe.apply(gray_200x200)\n",
        "    g = cv2.GaussianBlur(g, (3,3), 0)\n",
        "    return g"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "babfdb10",
      "metadata": {
        "id": "babfdb10"
      },
      "source": [
        "## 4) Webcam persistente + Preview - Verifica√ß√£o Facial Dual (Colab ‚Üî Local)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "21d7e319",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21d7e319",
        "outputId": "6c4ff5bd-8e88-4989-b9af-dbe6b631f9c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IN_COLAB = True\n"
          ]
        }
      ],
      "source": [
        "import time, base64, uuid\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Detecta ambiente\n",
        "IN_COLAB = True\n",
        "try:\n",
        "    from google.colab import output  # s√≥ existe no Colab\n",
        "    from IPython.display import Javascript, display\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "print(\"IN_COLAB =\", IN_COLAB)\n",
        "\n",
        "# ---------- (opcional) janela processada no Colab ----------\n",
        "def update_display_img(frame_bgr):\n",
        "    \"\"\"\n",
        "    Atualiza a <img id='output'> no Colab. Em ambiente local, essa fun√ß√£o n√£o faz nada\n",
        "    (use cv2.imshow no seu fluxo local).\n",
        "    \"\"\"\n",
        "    if not IN_COLAB:\n",
        "        return\n",
        "    ok, buffer = cv2.imencode('.jpg', frame_bgr)\n",
        "    if not ok:\n",
        "        return\n",
        "    b64 = base64.b64encode(buffer).decode('utf-8')\n",
        "    output.eval_js(f\"window.updateProcessed && window.updateProcessed('{b64}')\")\n",
        "\n",
        "# ---------- Colab: stream persistente via JS ----------\n",
        "if IN_COLAB:\n",
        "    def _ensure_camera_ready(show_raw=False, width=640, height=480):\n",
        "        js = f\"\"\"\n",
        "        (async () => {{\n",
        "          let container = document.getElementById('camera-container');\n",
        "          if (!container) {{\n",
        "            container = document.createElement('div');\n",
        "            container.id = 'camera-container';\n",
        "            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n",
        "            document.body.appendChild(container);\n",
        "            container.innerHTML = `\n",
        "              <div id=\"cam-left\" style=\"display:{'flex' if show_raw else 'none'}; flex-direction:column; align-items:center; gap:6px;\">\n",
        "                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:{width}px; border:1px solid #ddd; border-radius:8px;\"></video>\n",
        "                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n",
        "              </div>\n",
        "              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n",
        "                <img id=\"output\" style=\"max-width:{width}px; border:1px solid #ddd; border-radius:8px;\">\n",
        "                <small style=\"color:#555\">Frame processado</small>\n",
        "              </div>\n",
        "              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n",
        "            `;\n",
        "          }} else {{\n",
        "            const left = document.getElementById('cam-left');\n",
        "            if (left) left.style.display = { '\"flex\"' if show_raw else '\"none\"' };\n",
        "          }}\n",
        "\n",
        "          if (!window._colabStream || window._colabStreamInactive) {{\n",
        "            try {{\n",
        "              window._colabStream = await navigator.mediaDevices.getUserMedia({{ video: {{width:{width}, height:{height}}}, audio:false }});\n",
        "              window._colabStreamInactive = false;\n",
        "            }} catch (e) {{\n",
        "              console.error('getUserMedia failed', e);\n",
        "              return false;\n",
        "            }}\n",
        "          }}\n",
        "          const video = document.getElementById('webcam');\n",
        "          if (video && video.srcObject !== window._colabStream) {{\n",
        "            video.srcObject = window._colabStream;\n",
        "          }}\n",
        "\n",
        "          const canvas = document.getElementById('canvas');\n",
        "          window.captureFrame = () => {{\n",
        "            const ctx = canvas.getContext('2d');\n",
        "            const vw = (video && video.videoWidth) ? video.videoWidth : {width};\n",
        "            const vh = (video && video.videoHeight) ? video.videoHeight : {height};\n",
        "            canvas.width = vw; canvas.height = vh;\n",
        "            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n",
        "            return canvas.toDataURL('image/jpeg', 0.9);\n",
        "          }};\n",
        "          window.updateProcessed = (b64) => {{\n",
        "            const img = document.getElementById('output');\n",
        "            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n",
        "          }};\n",
        "          window.stopColabCamera = () => {{\n",
        "            try {{\n",
        "              if (window._colabStream && !window._colabStreamInactive) {{\n",
        "                window._colabStream.getTracks().forEach(t => t.stop());\n",
        "                window._colabStreamInactive = true;\n",
        "              }}\n",
        "            }} catch (e) {{ console.warn(e); }}\n",
        "            const c = document.getElementById('camera-container');\n",
        "            if (c) c.remove();\n",
        "          }};\n",
        "          return true;\n",
        "        }} )();\n",
        "        \"\"\"\n",
        "        display(Javascript(js))\n",
        "        time.sleep(0.2)\n",
        "\n",
        "    def _b64_to_image(data_url_or_b64):\n",
        "        if data_url_or_b64 is None:\n",
        "            return None\n",
        "        s = data_url_or_b64\n",
        "        if isinstance(s, bytes):\n",
        "            s = s.decode(\"utf-8\")\n",
        "        if s.startswith(\"data:image\"):\n",
        "            s = s.split(\",\")[1]\n",
        "        arr = np.frombuffer(base64.b64decode(s), dtype=np.uint8)\n",
        "        return cv2.imdecode(arr, cv2.IMREAD_COLOR)\n",
        "\n",
        "    def capture_one(width=640, height=480, show_raw=False):\n",
        "        _ensure_camera_ready(show_raw=show_raw, width=width, height=height)\n",
        "        data_url = output.eval_js(\"window.captureFrame ? window.captureFrame() : null\")\n",
        "        return _b64_to_image(data_url)\n",
        "\n",
        "    def capture_sequence(n=24, delay_ms=80, width=640, height=480, show_raw=False, preview_callback=None):\n",
        "        _ensure_camera_ready(show_raw=show_raw, width=width, height=height)\n",
        "        frames = []\n",
        "        for i in range(n):\n",
        "            data_url = output.eval_js(\"window.captureFrame ? window.captureFrame() : null\")\n",
        "            img = _b64_to_image(data_url)\n",
        "            if img is not None:\n",
        "                frames.append(img)\n",
        "                if callable(preview_callback):\n",
        "                    preview_callback(img, i)\n",
        "            time.sleep(max(0, delay_ms/1000.0))\n",
        "        return frames\n",
        "\n",
        "    def stop_camera_ui():\n",
        "        display(Javascript(\"if (window.stopColabCamera) window.stopColabCamera();\"))\n",
        "        time.sleep(0.2)\n",
        "\n",
        "# ---------- Local: OpenCV VideoCapture ----------\n",
        "else:\n",
        "    def capture_one(cam_index=0, width=640, height=480, show_raw=False):\n",
        "        cap = cv2.VideoCapture(cam_index)\n",
        "        cap.set(cv2.CAP_PROP_FRAME_WIDTH,  width)\n",
        "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
        "        ret, frame = cap.read()\n",
        "        cap.release()\n",
        "        if not ret:\n",
        "            raise RuntimeError(\"N√£o conseguiu capturar frame da webcam local\")\n",
        "        return frame\n",
        "\n",
        "    def capture_sequence(n=24, delay_ms=80, cam_index=0, width=640, height=480, show_raw=False, preview_callback=None):\n",
        "        cap = cv2.VideoCapture(cam_index)\n",
        "        cap.set(cv2.CAP_PROP_FRAME_WIDTH,  width)\n",
        "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
        "        frames = []\n",
        "        for i in range(n):\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                continue\n",
        "            frames.append(frame)\n",
        "            if callable(preview_callback):\n",
        "                preview_callback(frame, i)\n",
        "            # preview simples local\n",
        "            cv2.imshow(\"preview\", frame)\n",
        "            if cv2.waitKey(delay_ms) & 0xFF == ord(\"q\"):\n",
        "                break\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()\n",
        "        return frames\n",
        "\n",
        "    def stop_camera_ui():\n",
        "        # no local, apenas fecha janelas\n",
        "        cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a56dd9cb",
      "metadata": {
        "id": "a56dd9cb"
      },
      "source": [
        "## 5) Detec√ß√£o de faces (Haar/DNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "23539590",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23539590",
        "outputId": "3a80ea9a-cf51-411f-9a94-9d2738753c63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Detec√ß√£o] Modelo atual: haar | MODELS_DIR=cv_colab_data/models\n"
          ]
        }
      ],
      "source": [
        "# ============================\n",
        "# 5) Detec√ß√£o de faces (Haar/DNN)\n",
        "# ============================\n",
        "#\n",
        "# Suporta dois detectores:\n",
        "#   - \"haar\": Haar Cascade (cv2.CascadeClassifier)\n",
        "#   - \"dnn_ssd_resnet10\": DNN (SSD ResNet10) via OpenCV DNN (Caffe)\n",
        "#\n",
        "# Escolha tempor√°ria (at√© o Runner setar automaticamente):\n",
        "#   - Ajuste DETECTION_MODEL = \"haar\" | \"dnn_ssd_resnet10\"\n",
        "#   - ou exporte a env: DETECTION_MODEL=haar | dnn_ssd_resnet10\n",
        "#\n",
        "# Uso: faces = detect_faces(img_bgr, conf_threshold=0.5)\n",
        "# Retorno: lista de (x, y, w, h, score)\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# ======================\n",
        "# Configura√ß√£o do modelo\n",
        "# ======================\n",
        "DETECTION_MODEL = os.environ.get(\"DETECTION_MODEL\", \"haar\").strip().lower()\n",
        "\n",
        "# Honra o DATA_DIR definido em outra c√©lula; se n√£o existir, cria um default\n",
        "try:\n",
        "    DATA_DIR  # definido na sua c√©lula de diret√≥rios\n",
        "except NameError:\n",
        "    DATA_DIR = Path(\"cv_colab_data\")\n",
        "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "MODELS_DIR = DATA_DIR / \"models\"\n",
        "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Haar: caminho padr√£o do OpenCV (com fallback)\n",
        "try:\n",
        "    import cv2.data as cvd\n",
        "    HAAR_PATH = str(Path(cvd.haarcascades) / \"haarcascade_frontalface_default.xml\")\n",
        "except Exception:\n",
        "    # fallback: se quiser manter tudo em DATA_DIR/models, pode copiar o xml pra l√°\n",
        "    HAAR_PATH = str(MODELS_DIR / \"haarcascade_frontalface_default.xml\")\n",
        "\n",
        "# DNN (SSD ResNet10): caminhos dentro de DATA_DIR/models\n",
        "DNN_PROTO_PATH   = MODELS_DIR / \"deploy.prototxt\"\n",
        "# usamos o modelo Caffe \"n√£o-fp16\", dispon√≠vel publicamente:\n",
        "DNN_WEIGHTS_PATH = MODELS_DIR / \"res10_300x300_ssd_iter_140000.caffemodel\"\n",
        "\n",
        "# URLs oficiais/alternativas\n",
        "DNN_PROTO_URL   = \"https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt\"\n",
        "DNN_WEIGHTS_URL = \"https://github.com/opencv/opencv_3rdparty/raw/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel\"\n",
        "\n",
        "# --------------\n",
        "# Inicializa√ß√£o\n",
        "# --------------\n",
        "_haar_cascade = None\n",
        "_dnn_net = None\n",
        "\n",
        "def _download_file(url: str, dest: Path) -> bool:\n",
        "    \"\"\"\n",
        "    Baixa com urllib; se falhar e houver 'wget', tenta wget.\n",
        "    Retorna True se o arquivo existir ao final.\n",
        "    \"\"\"\n",
        "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "    try:\n",
        "        import urllib.request\n",
        "        print(f\"Baixando {dest.name} ‚Ä¶\")\n",
        "        urllib.request.urlretrieve(url, str(dest))\n",
        "        return dest.exists()\n",
        "    except Exception as e:\n",
        "        print(f\"Aviso: urllib falhou ({e}). Tentando wget (se dispon√≠vel)‚Ä¶\")\n",
        "        try:\n",
        "            code = os.system(f\"wget -q -O {dest} {url}\")\n",
        "            return dest.exists() and code == 0\n",
        "        except Exception as e2:\n",
        "            print(f\"Aviso: wget tamb√©m falhou ({e2}).\")\n",
        "            return dest.exists()\n",
        "\n",
        "def _download_if_missing():\n",
        "    ok = True\n",
        "    if not Path(DNN_PROTO_PATH).exists():\n",
        "        ok = _download_file(DNN_PROTO_URL, DNN_PROTO_PATH) and ok\n",
        "    if not Path(DNN_WEIGHTS_PATH).exists():\n",
        "        ok = _download_file(DNN_WEIGHTS_URL, DNN_WEIGHTS_PATH) and ok\n",
        "    if not ok:\n",
        "        print(\n",
        "            \"[DNN] N√£o foi poss√≠vel garantir todos os arquivos.\\n\"\n",
        "            f\"  Prototxt: {DNN_PROTO_PATH}\\n\"\n",
        "            f\"  Pesos   : {DNN_WEIGHTS_PATH}\\n\"\n",
        "            \"  ‚Üí Baixe manualmente ou defina DNN_PROTO_PATH / DNN_WEIGHTS_PATH.\"\n",
        "        )\n",
        "\n",
        "def _init_haar():\n",
        "    global _haar_cascade\n",
        "    if _haar_cascade is None:\n",
        "        if not os.path.exists(HAAR_PATH):\n",
        "            raise FileNotFoundError(f\"Haar cascade n√£o encontrado em: {HAAR_PATH}\")\n",
        "        _haar_cascade = cv2.CascadeClassifier(HAAR_PATH)\n",
        "\n",
        "def _init_dnn():\n",
        "    _download_if_missing()\n",
        "    global _dnn_net\n",
        "    if _dnn_net is None:\n",
        "        if not (Path(DNN_PROTO_PATH).exists() and Path(DNN_WEIGHTS_PATH).exists()):\n",
        "            raise FileNotFoundError(\n",
        "                \"Arquivos do DNN n√£o encontrados.\\n\"\n",
        "                f\"  Prototxt: {DNN_PROTO_PATH}\\n\"\n",
        "                f\"  Pesos   : {DNN_WEIGHTS_PATH}\\n\"\n",
        "                \"Ajuste DNN_PROTO_PATH / DNN_WEIGHTS_PATH ou salve os arquivos em DATA_DIR/'models'.\"\n",
        "            )\n",
        "        _dnn_net = cv2.dnn.readNetFromCaffe(str(DNN_PROTO_PATH), str(DNN_WEIGHTS_PATH))\n",
        "\n",
        "# --------------------\n",
        "# Fun√ß√£o de detec√ß√£o\n",
        "# --------------------\n",
        "def detect_faces(image_bgr, conf_threshold: float = 0.5):\n",
        "    \"\"\"\n",
        "    Retorna lista de detec√ß√µes: (x, y, w, h, score)\n",
        "    \"\"\"\n",
        "    model = DETECTION_MODEL\n",
        "    if model == \"haar\":\n",
        "        _init_haar()\n",
        "        gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)\n",
        "        rects = _haar_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "        return [(int(x), int(y), int(w), int(h), 1.0) for (x, y, w, h) in rects]\n",
        "\n",
        "    elif model in (\"dnn\", \"dnn_ssd_resnet10\", \"ssd\", \"resnet10\"):\n",
        "        _init_dnn()\n",
        "        (h, w) = image_bgr.shape[:2]\n",
        "        blob = cv2.dnn.blobFromImage(\n",
        "            cv2.resize(image_bgr, (300, 300)), 1.0, (300, 300),\n",
        "            (104.0, 177.0, 123.0)\n",
        "        )\n",
        "        _dnn_net.setInput(blob)\n",
        "        detections = _dnn_net.forward()\n",
        "        boxes = []\n",
        "        for i in range(detections.shape[2]):\n",
        "            confidence = float(detections[0, 0, i, 2])\n",
        "            if confidence >= conf_threshold:\n",
        "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "                x, y = max(0, startX), max(0, startY)\n",
        "                ww, hh = max(0, endX - startX), max(0, endY - startY)\n",
        "                boxes.append((x, y, ww, hh, confidence))\n",
        "        return boxes\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Modelo '{model}' n√£o suportado. Use 'haar' ou 'dnn_ssd_resnet10'.\")\n",
        "\n",
        "# --------------------\n",
        "# Helper de visualiza√ß√£o\n",
        "# --------------------\n",
        "def draw_faces(image_bgr, faces, color=(0,255,0), thickness=2):\n",
        "    out = image_bgr.copy()\n",
        "    for (x,y,w,h,score) in faces:\n",
        "        cv2.rectangle(out, (x,y), (x+w, y+h), color, thickness)\n",
        "        cv2.putText(out, f\"{score:.2f}\", (x, max(0,y-5)), cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 1, cv2.LINE_AA)\n",
        "    return out\n",
        "\n",
        "print(f\"[Detec√ß√£o] Modelo atual: {DETECTION_MODEL} | MODELS_DIR={MODELS_DIR}\")\n",
        "\n",
        "# Teste r√°pido (opcional)\n",
        "# img = capture_one(show_raw=True)\n",
        "# faces = detect_faces(img)\n",
        "# vis = draw_faces(img, faces)\n",
        "# show_bgr(vis, \"faces detectadas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5b79479",
      "metadata": {
        "id": "c5b79479"
      },
      "source": [
        "## 6) Config de preview e evid√™ncias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "af35c8bc",
      "metadata": {
        "id": "af35c8bc"
      },
      "outputs": [],
      "source": [
        "# Cores das caixas (BGR)\n",
        "PREVIEW_BOX_COLOR = (0, 255, 0)     # verde\n",
        "CALIB_BOX_COLOR   = (255, 165, 0)   # laranja\n",
        "FINAL_BOX_COLOR   = (0, 255, 255)   # amarelo\n",
        "SAVE_EVIDENCE     = True\n",
        "\n",
        "def draw_box(img_bgr, bbox, color, thickness=2, label=None):\n",
        "    x,y,w,h = bbox\n",
        "    cv2.rectangle(img_bgr, (x,y), (x+w, y+h), color, thickness)\n",
        "    if label:\n",
        "        cv2.putText(img_bgr, label, (x, max(0, y-8)), cv2.FONT_HERSHEY_SIMPLEX, 0.55, color, 2, cv2.LINE_AA)\n",
        "\n",
        "def maybe_save_evidence(result: dict, vis_bgr, prefix=\"auth\"):\n",
        "    if not SAVE_EVIDENCE:\n",
        "        return\n",
        "    status = result.get(\"status\", \"\")\n",
        "    if status in (\"error\", \"route_review\"):\n",
        "        ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "        name = f\"{prefix}_{status}_{ts}_{uuid.uuid4().hex[:6]}.jpg\"\n",
        "        cv2.imwrite(str(EVIDENCE_DIR / name), vis_bgr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5ae218f",
      "metadata": {
        "id": "d5ae218f"
      },
      "source": [
        "## 7) Enrollment com preview (fun√ß√£o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "bf25ffcf",
      "metadata": {
        "id": "bf25ffcf"
      },
      "outputs": [],
      "source": [
        "def enroll_user_with_preview(user_id=\"novo_usuario_preview\", n_samples=30, interval_ms=100):\n",
        "    user_dir = ENROLL_DIR / user_id\n",
        "    user_dir.mkdir(parents=True, exist_ok=True)\n",
        "    setup = capture_one(show_raw=True)  # inicializa UI (mostra preview bruto)\n",
        "\n",
        "    saved = 0\n",
        "    attempts = 0\n",
        "    print(f\"Coletando {n_samples} amostras para '{user_id}'‚Ä¶ Olhe para a c√¢mera.\")\n",
        "    while saved < n_samples and attempts < n_samples*3:\n",
        "        attempts += 1\n",
        "        fr = capture_one(show_raw=True)\n",
        "        if fr is None:\n",
        "            time.sleep(0.05);\n",
        "            continue\n",
        "        faces = detect_faces(fr)\n",
        "        vis = fr.copy()\n",
        "        if len(faces):\n",
        "            (x,y,w,h,score) = max(faces, key=lambda f: f[2]*f[3]) # Unpack 5 values\n",
        "            draw_box(vis, (x,y,w,h), PREVIEW_BOX_COLOR, label=\"enroll\")\n",
        "            face = fr[y:y+h, x:x+w]\n",
        "            g = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
        "            g = cv2.resize(g, (200,200))\n",
        "            g = preprocess_face_gray(g)\n",
        "            cv2.imwrite(str(user_dir/f\"{user_id}_{uuid.uuid4().hex[:6]}.jpg\"), g)\n",
        "            saved += 1\n",
        "        update_display_img(vis)\n",
        "        time.sleep(max(0, interval_ms/1000.0))\n",
        "    print(f\"‚úÖ Enrollment conclu√≠do: {saved}/{n_samples} amostras salvas.\")\n",
        "    if saved < max(10, int(0.5*n_samples)):\n",
        "        print(\"‚ö†Ô∏è Poucas amostras √∫teis. Considere refazer com melhor enquadramento/ilumina√ß√£o.\")\n",
        "    # n√£o fecha a UI aqui para reaproveitar no pr√≥ximo passo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0f00c3b",
      "metadata": {
        "id": "a0f00c3b"
      },
      "source": [
        "## 8) Modelo LBPH em mem√≥ria (cache global)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "be1e62e6",
      "metadata": {
        "id": "be1e62e6"
      },
      "outputs": [],
      "source": [
        "REC_CACHE = None\n",
        "LABEL_MAP_CACHE = None\n",
        "INV_LABEL_CACHE = None\n",
        "\n",
        "def _load_images_and_labels():\n",
        "    images, labels = [], []\n",
        "    label_map = {}\n",
        "    next_label = 0\n",
        "    for ud in sorted(ENROLL_DIR.glob(\"*\")):\n",
        "        if not ud.is_dir():\n",
        "            continue\n",
        "        uid = ud.name\n",
        "        label_map[uid] = next_label\n",
        "        for p in ud.glob(\"*.jpg\"):\n",
        "            g = cv2.imread(str(p), cv2.IMREAD_GRAYSCALE)\n",
        "            if g is None:\n",
        "                continue\n",
        "            g = cv2.resize(g, (200,200))\n",
        "            g = preprocess_face_gray(g)\n",
        "            images.append(g); labels.append(next_label)\n",
        "        next_label += 1\n",
        "    return images, np.array(labels), label_map\n",
        "\n",
        "def train_lbph_in_memory(neighbors=16):\n",
        "    global REC_CACHE, LABEL_MAP_CACHE, INV_LABEL_CACHE\n",
        "    images, labels, label_map = _load_images_and_labels()\n",
        "    if len(images) == 0:\n",
        "        raise RuntimeError(\"Sem amostras. Fa√ßa o enrollment antes.\")\n",
        "    rec = cv2.face.LBPHFaceRecognizer_create(radius=2, neighbors=neighbors, grid_x=8, grid_y=8)\n",
        "    rec.train(images, labels)\n",
        "    REC_CACHE = rec\n",
        "    LABEL_MAP_CACHE = label_map\n",
        "    INV_LABEL_CACHE = {v:k for k,v in label_map.items()}\n",
        "    print(f\"Modelo LBPH treinado em mem√≥ria. Usu√°rios: {list(label_map.keys())}\")\n",
        "    return rec\n",
        "\n",
        "def get_recognizer(neighbors=16, force_retrain=False):\n",
        "    global REC_CACHE, LABEL_MAP_CACHE, INV_LABEL_CACHE\n",
        "    if force_retrain or REC_CACHE is None or LABEL_MAP_CACHE is None or INV_LABEL_CACHE is None:\n",
        "        print(\"‚Üª Treinando LBPH (mem√≥ria)‚Ä¶\")\n",
        "        return train_lbph_in_memory(neighbors=neighbors)\n",
        "    return REC_CACHE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de3a1cce",
      "metadata": {
        "id": "de3a1cce"
      },
      "source": [
        "## 9) Calibra√ß√£o autom√°tica do limiar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6a2c5e8c",
      "metadata": {
        "id": "6a2c5e8c"
      },
      "outputs": [],
      "source": [
        "def _detect_face_gray200(img_bgr):\n",
        "    faces = detect_faces(img_bgr)\n",
        "    if len(faces)==0:\n",
        "        return None, None\n",
        "    (x,y,w,h,score) = max(faces, key=lambda f: f[2]*f[3]) # Unpack 5 values including score\n",
        "    face = img_bgr[y:y+h, x:x+w]\n",
        "    g = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
        "    g = cv2.resize(g, (200,200))\n",
        "    g = preprocess_face_gray(g)\n",
        "    return g, (x,y,w,h)\n",
        "\n",
        "def calibrate_threshold(samples=15, neighbors=16):\n",
        "    global SERVICE_THRESHOLD\n",
        "    rec = get_recognizer(neighbors=neighbors)\n",
        "    dists = []\n",
        "    print(f\"üìè Calibrando limiar (coletando {samples} dist√¢ncias)‚Ä¶\")\n",
        "    for i in range(samples):\n",
        "        fr = capture_one(show_raw=True)\n",
        "        if fr is None:\n",
        "            time.sleep(0.05);\n",
        "            continue\n",
        "        g200, bbox = _detect_face_gray200(fr)\n",
        "        vis = fr.copy()\n",
        "        if bbox is not None:\n",
        "            (x,y,w,h) = bbox\n",
        "            draw_box(vis, (x,y,w,h), CALIB_BOX_COLOR, label=\"calib\")\n",
        "            _, dist = rec.predict(g200)\n",
        "            dists.append(dist)\n",
        "            print(f\"[{i+1}/{samples}] dist={dist:.1f}\")\n",
        "        update_display_img(vis)\n",
        "        time.sleep(0.08)\n",
        "    if dists:\n",
        "        p95 = float(np.percentile(dists, 95))\n",
        "        SERVICE_THRESHOLD = round(p95 + 5.0, 1)\n",
        "        print(f\"üéØ Novo SERVICE_THRESHOLD = {SERVICE_THRESHOLD} (p95={p95:.1f} + margem)\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Calibra√ß√£o insuficiente; threshold mantido.\")\n",
        "    return SERVICE_THRESHOLD"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5468bd2f",
      "metadata": {
        "id": "5468bd2f"
      },
      "source": [
        "## 10) Liveness passivo com preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4ff8dc03",
      "metadata": {
        "id": "4ff8dc03"
      },
      "outputs": [],
      "source": [
        "def liveness_passive_preview(n=24, delay_ms=80, min_energy=LIVENESS_MIN_ENERGY):\n",
        "    prev = None\n",
        "    energy = 0.0\n",
        "    used = 0\n",
        "    for i in range(n):\n",
        "        fr = capture_one(show_raw=True)\n",
        "        if fr is None:\n",
        "            time.sleep(0.05)\n",
        "            continue\n",
        "        faces = detect_faces(fr)\n",
        "        vis = fr.copy()\n",
        "        if len(faces):\n",
        "            (x,y,w,h,score) = max(faces, key=lambda f: f[2]*f[3]) # Unpack 5 values\n",
        "            draw_box(vis, (x,y,w,h), PREVIEW_BOX_COLOR, label=\"live\")\n",
        "            face = fr[y:y+h, x:x+w]\n",
        "            g = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
        "            g = cv2.resize(g, (160,160))\n",
        "            g = preprocess_face_gray(g)\n",
        "            if prev is not None:\n",
        "                diff = cv2.absdiff(g, prev)\n",
        "                energy += float(np.mean(diff))\n",
        "            prev = g; used += 1\n",
        "        update_display_img(vis)\n",
        "        time.sleep(max(0, delay_ms/1000.0))\n",
        "    avg = energy / max(1, used)\n",
        "    verdict = \"live\" if avg >= min_energy else \"spoof\"\n",
        "    print(f\"[Liveness] energia m√©dia: {avg:.2f} -> {verdict}\")\n",
        "    return verdict, avg"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3eff28f",
      "metadata": {
        "id": "b3eff28f"
      },
      "source": [
        "## 11) Autentica√ß√£o 1:1 (com liveness e preview)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6ea82543",
      "metadata": {
        "id": "6ea82543"
      },
      "outputs": [],
      "source": [
        "def authenticate_1v1_preview(expected_user=\"novo_usuario_preview\", neighbors=16, require_liveness=True):\n",
        "    thr = SERVICE_THRESHOLD\n",
        "    if require_liveness:\n",
        "        verdict, energy = liveness_passive_preview(n=24, delay_ms=80, min_energy=LIVENESS_MIN_ENERGY)\n",
        "        if verdict != \"live\":\n",
        "            print(f\"üõë Liveness falhou (energia={energy:.2f}).\")\n",
        "            result = {\"status\":\"route_review\", \"mode\":\"1v1\", \"reason\":\"liveness_fail\", \"energy\":energy}\n",
        "            maybe_save_evidence(result, np.zeros((10,10,3), dtype=np.uint8), prefix=\"auth_1v1\")\n",
        "            return result\n",
        "\n",
        "    rec = get_recognizer(neighbors=neighbors)\n",
        "    fr = capture_one(show_raw=True)\n",
        "    if fr is None:\n",
        "        result = {\"status\":\"error\", \"mode\":\"1v1\", \"reason\":\"capture_fail\"}\n",
        "        return result\n",
        "\n",
        "    g200, bbox = _detect_face_gray200(fr)\n",
        "    vis = fr.copy()\n",
        "    if bbox is None:\n",
        "        update_display_img(vis)\n",
        "        result = {\"status\":\"error\", \"mode\":\"1v1\", \"reason\":\"no_face\"}\n",
        "        maybe_save_evidence(result, vis, prefix=\"auth_1v1\")\n",
        "        return result\n",
        "\n",
        "    (x,y,w,h) = bbox\n",
        "    draw_box(vis, (x,y,w,h), FINAL_BOX_COLOR, label=\"auth 1:1\")\n",
        "    update_display_img(vis)\n",
        "\n",
        "    label, conf = rec.predict(g200)\n",
        "    pred_user = INV_LABEL_CACHE.get(label, \"desconhecido\")\n",
        "    print(f\"üîê 1:1 ‚Äî esperado={expected_user} | predito={pred_user} | dist={conf:.1f} | thr={thr}\")\n",
        "\n",
        "    if pred_user == expected_user and conf <= thr:\n",
        "        result = {\"status\":\"approved\", \"mode\":\"1v1\", \"user\":pred_user, \"dist\":float(conf), \"threshold\":thr}\n",
        "    else:\n",
        "        result = {\"status\":\"route_review\", \"mode\":\"1v1\", \"reason\":\"no_match\", \"pred\":pred_user, \"dist\":float(conf), \"threshold\":thr}\n",
        "        maybe_save_evidence(result, vis, prefix=\"auth_1v1\")\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eff1f2a9",
      "metadata": {
        "id": "eff1f2a9"
      },
      "source": [
        "## 12) Autentica√ß√£o 1:N (com liveness e preview)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "353349b8",
      "metadata": {
        "id": "353349b8"
      },
      "outputs": [],
      "source": [
        "def authenticate_1vN_preview(neighbors=16, require_liveness=True):\n",
        "    thr = SERVICE_THRESHOLD\n",
        "    if require_liveness:\n",
        "        verdict, energy = liveness_passive_preview(n=24, delay_ms=80, min_energy=LIVENESS_MIN_ENERGY)\n",
        "        if verdict != \"live\":\n",
        "            print(f\"üõë Liveness falhou (energia={energy:.2f}).\")\n",
        "            result = {\"status\":\"route_review\", \"mode\":\"1vN\", \"reason\":\"liveness_fail\", \"energy\":energy}\n",
        "            maybe_save_evidence(result, np.zeros((10,10,3), dtype=np.uint8), prefix=\"auth_1vN\")\n",
        "            return result\n",
        "\n",
        "    rec = get_recognizer(neighbors=neighbors)\n",
        "    fr = capture_one(show_raw=True)\n",
        "    if fr is None:\n",
        "        result = {\"status\":\"error\", \"mode\":\"1vN\", \"reason\":\"capture_fail\"}\n",
        "        return result\n",
        "\n",
        "    g200, bbox = _detect_face_gray200(fr)\n",
        "    vis = fr.copy()\n",
        "    if bbox is None:\n",
        "        update_display_img(vis)\n",
        "        result = {\"status\":\"error\", \"mode\":\"1vN\", \"reason\":\"no_face\"}\n",
        "        maybe_save_evidence(result, vis, prefix=\"auth_1vN\")\n",
        "        return result\n",
        "\n",
        "    (x,y,w,h) = bbox\n",
        "    draw_box(vis, (x,y,w,h), FINAL_BOX_COLOR, label=\"auth 1:N\")\n",
        "    update_display_img(vis)\n",
        "\n",
        "    label, conf = rec.predict(g200)  # menor = melhor\n",
        "    user_pred = INV_LABEL_CACHE.get(label, \"desconhecido\")\n",
        "    print(f\"üß≠ 1:N ‚Äî predito={user_pred} | dist={conf:.1f} | thr={thr}\")\n",
        "\n",
        "    if conf <= thr:\n",
        "        result = {\"status\":\"approved\", \"mode\":\"1vN\", \"user\":user_pred, \"dist\":float(conf), \"threshold\":thr}\n",
        "    else:\n",
        "        result = {\"status\":\"route_review\", \"mode\":\"1vN\", \"reason\":\"no_match\", \"pred\":user_pred, \"dist\":float(conf), \"threshold\":thr}\n",
        "        maybe_save_evidence(result, vis, prefix=\"auth_1vN\")\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vxUzE2bnhcOQ",
      "metadata": {
        "id": "vxUzE2bnhcOQ"
      },
      "source": [
        "## 13) Bloco de utilidades de avalia√ß√£o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "PhKjkZ4ghdSl",
      "metadata": {
        "id": "PhKjkZ4ghdSl"
      },
      "outputs": [],
      "source": [
        "# =============================================\n",
        "# Fun√ß√£o: avalia√ß√£o OFFLINE com DATASET AUTOM√ÅTICO\n",
        "#  - positives: ENROLL_DIR/<user> (ou todos, se n√£o houver alvo)\n",
        "#  - negatives: persistidos em DATA_DIR/\"negatives\" (captura da c√¢mera se faltarem)\n",
        "#  - usa detect_faces() da c√©lula 5\n",
        "# Retorna: dict com m√©tricas e info (ou None se falhar)\n",
        "# =============================================\n",
        "import os, time, shutil\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "try:\n",
        "    import pandas as pd\n",
        "except Exception:\n",
        "    pd = None\n",
        "\n",
        "def run_offline_eval_from_enrollment(\n",
        "    neg_target_min: int = 30,\n",
        "    capture_batch: int = 30,\n",
        "    capture_sleep_sec: float = 0.15,\n",
        "    eval_max_images: int | None = None,\n",
        "    conf_threshold: float = 0.5,\n",
        "    use_camera: bool = True,\n",
        "    negatives_src_dir: str | Path | None = None,\n",
        "    dataset_dir_override: str | Path | None = None, # Adicionado\n",
        "    default_dataset_dir: str | Path | None = None,  # Adicionado\n",
        "):\n",
        "    \"\"\"\n",
        "    Executa avalia√ß√£o offline ap√≥s o fluxo principal do Runner.\n",
        "    - Garante arquivos do DNN (se DETECTION_MODEL = dnn_ssd_resnet10).\n",
        "    - Monta dataset em DATA_DIR/'dataset_auto' (positives de ENROLL_DIR, negatives persistentes).\n",
        "    - Roda matriz de confus√£o e m√©tricas (robusto a apenas 1 classe).\n",
        "    \"\"\"\n",
        "    # ===== caminhos vindos de c√©lulas anteriores =====\n",
        "    try:\n",
        "        DATA_DIR\n",
        "    except NameError:\n",
        "        # fallback seguro\n",
        "        globals()[\"DATA_DIR\"] = Path(\"cv_colab_data\")\n",
        "        DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    try:\n",
        "        ENROLL_DIR\n",
        "    except NameError:\n",
        "        globals()[\"ENROLL_DIR\"] = DATA_DIR / \"enroll\"\n",
        "        ENROLL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    NEG_STORE_DIR = DATA_DIR / \"negatives\"      # negativos persistentes\n",
        "    DATASET_AUTO  = DATA_DIR / \"dataset_auto\"   # dataset gerado automaticamente\n",
        "    POS_DIR_AUTO  = DATASET_AUTO / \"positives\"\n",
        "    NEG_DIR_AUTO  = DATASET_AUTO / \"negatives\"\n",
        "    NEG_STORE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    NAME_MAP = {True: \"face\", False: \"no_face\"}\n",
        "\n",
        "    # ===== helpers =====\n",
        "    def _list_images(dirpath: Path):\n",
        "        exts = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\"}\n",
        "        if not dirpath.is_dir():\n",
        "            return []\n",
        "        return sorted([str(p) for p in dirpath.iterdir() if p.suffix.lower() in exts])\n",
        "\n",
        "    def _copy_all_images(src: Path, dst: Path):\n",
        "        dst.mkdir(parents=True, exist_ok=True)\n",
        "        count = 0\n",
        "        for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.bmp\",\"*.tif\",\"*.tiff\"):\n",
        "            for p in src.rglob(ext):\n",
        "                out = dst / f\"{p.stem}_{count}{p.suffix.lower()}\"\n",
        "                try:\n",
        "                    shutil.copy2(p, out)\n",
        "                    count += 1\n",
        "                except Exception:\n",
        "                    pass\n",
        "        return count\n",
        "\n",
        "    def _capture_negatives_persistent(store_dir: Path, frames=30, sleep=0.15):\n",
        "        if not use_camera:\n",
        "            return 0\n",
        "        cap = cv2.VideoCapture(0)\n",
        "        if not cap.isOpened():\n",
        "            print(\"‚úò N√£o foi poss√≠vel abrir a c√¢mera para capturar negativos.\")\n",
        "            return 0\n",
        "        print(f\"üé• Capturando {frames} negativos (aponte para parede/quadro vazio ou saia do frame)‚Ä¶\")\n",
        "        count = 0\n",
        "        for i in range(frames):\n",
        "            ok, frame = cap.read()\n",
        "            if not ok:\n",
        "                break\n",
        "            out = store_dir / f\"neg_{int(time.time())}_{i:03d}.jpg\"\n",
        "            cv2.imwrite(str(out), frame)\n",
        "            time.sleep(sleep)\n",
        "            count += 1\n",
        "        cap.release()\n",
        "        print(f\"‚úî Negativos capturados (persistentes): {count}\")\n",
        "        return count\n",
        "\n",
        "    def _ensure_dnn_files_if_needed():\n",
        "        det = os.environ.get(\"DETECTION_MODEL\", \"\").lower()\n",
        "        if det not in (\"dnn_ssd_resnet10\", \"dnn\", \"ssd\", \"resnet10\"):\n",
        "            return\n",
        "        MODELS_DIR = DATA_DIR / \"models\"\n",
        "        PROTO   = MODELS_DIR / \"deploy.prototxt\"\n",
        "        WEIGHTS = MODELS_DIR / \"res10_300x300_ssd_iter_140000.caffemodel\"\n",
        "        if PROTO.exists() and WEIGHTS.exists():\n",
        "            return\n",
        "        MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "        try:\n",
        "            import urllib.request\n",
        "            if not PROTO.exists():\n",
        "                print(\"Baixando deploy.prototxt ‚Ä¶\")\n",
        "                urllib.request.urlretrieve(\n",
        "                    \"https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt\",\n",
        "                    str(PROTO)\n",
        "                )\n",
        "            if not WEIGHTS.exists():\n",
        "                print(\"Baixando res10_300x300_ssd_iter_140000.caffemodel ‚Ä¶\")\n",
        "                urllib.request.urlretrieve(\n",
        "                    \"https://github.com/opencv/opencv_3rdparty/raw/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel\",\n",
        "                    str(WEIGHTS)\n",
        "                )\n",
        "            print(\"‚úî DNN pronto em\", MODELS_DIR)\n",
        "        except Exception as e:\n",
        "            print(\"‚úò Falha ao baixar arquivos do DNN automaticamente:\", e)\n",
        "            print(\"  ‚Üí Baixe manualmente para:\", MODELS_DIR)\n",
        "\n",
        "    def _evaluate_current_detector(dataset_dir: Path, max_images=None, conf_threshold=0.5):\n",
        "        pos_imgs = _list_images(dataset_dir / \"positives\")\n",
        "        neg_imgs = _list_images(dataset_dir / \"negatives\")\n",
        "        if max_images:\n",
        "            pos_imgs = pos_imgs[:max_images]\n",
        "            neg_imgs = neg_imgs[:max_images]\n",
        "\n",
        "        y_true, y_pred, times = [], [], []\n",
        "        import time as _t\n",
        "        # usa detect_faces() da c√©lula 5\n",
        "        for p in pos_imgs:\n",
        "            img = cv2.imread(p)\n",
        "            if img is None:\n",
        "                continue\n",
        "            t0 = _t.perf_counter()\n",
        "            boxes = detect_faces(img, conf_threshold=conf_threshold)\n",
        "            times.append(_t.perf_counter() - t0)\n",
        "            y_true.append(True)\n",
        "            y_pred.append(len(boxes) > 0)\n",
        "\n",
        "        for p in neg_imgs:\n",
        "            img = cv2.imread(p)\n",
        "            if img is None:\n",
        "                continue\n",
        "            t0 = _t.perf_counter()\n",
        "            boxes = detect_faces(img, conf_threshold=conf_threshold)\n",
        "            times.append(_t.perf_counter() - t0)\n",
        "            y_true.append(False)\n",
        "            y_pred.append(len(boxes) > 0)\n",
        "\n",
        "        if not y_true:\n",
        "            raise ValueError(\"Dataset vazio para avalia√ß√£o.\")\n",
        "\n",
        "        # classes realmente presentes\n",
        "        present = sorted(set(y_true))\n",
        "        cm = confusion_matrix(y_true, y_pred, labels=present)\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "        def _safe(fn):\n",
        "            try:\n",
        "                return fn(y_true, y_pred, zero_division=0)\n",
        "            except Exception:\n",
        "                return float(\"nan\")\n",
        "\n",
        "        if set(present) == {True, False}:\n",
        "            prec = precision_score(y_true, y_pred, zero_division=0)\n",
        "            rec  = recall_score(y_true, y_pred, zero_division=0)\n",
        "            f1   = f1_score(y_true, y_pred, zero_division=0)\n",
        "            print(\"\\n=== Relat√≥rio (2 classes) ===\")\n",
        "            print(classification_report(\n",
        "                y_true, y_pred, labels=present,\n",
        "                target_names=[NAME_MAP[c] for c in present], zero_division=0\n",
        "            ))\n",
        "        else:\n",
        "            prec = _safe(precision_score)\n",
        "            rec  = _safe(recall_score)\n",
        "            f1   = _safe(f1_score)\n",
        "            faltante = \"no_face\" if present == [True] else \"face\"\n",
        "            print(f\"\\n[AVISO] Apenas 1 classe presente (faltando: {faltante}). M√©tricas completas n√£o se aplicam.\")\n",
        "\n",
        "        avg_time = float(np.mean(times)) if times else float(\"nan\")\n",
        "        mdl = os.environ.get(\"DETECTION_MODEL\", \"(desconhecido)\")\n",
        "\n",
        "        print(f\"\\n=== Resultados: {mdl} ===\")\n",
        "        print(f\"Accuracy : {acc:.4f}\")\n",
        "        print(f\"Precision: {prec if np.isfinite(prec) else 'N/A'}\")\n",
        "        print(f\"Recall   : {rec if np.isfinite(rec) else 'N/A'}\")\n",
        "        print(f\"F1-score : {f1 if np.isfinite(f1) else 'N/A'}\")\n",
        "        print(f\"Tempo m√©dio por imagem: {avg_time:.4f} s\")\n",
        "\n",
        "        # Plot\n",
        "        plt.figure(figsize=(4, 3))\n",
        "        plt.imshow(cm, interpolation='nearest')\n",
        "        plt.title(f\"Matriz de Confus√£o - {mdl}\")\n",
        "        plt.colorbar()\n",
        "        ticks = np.arange(len(present))\n",
        "        plt.xticks(ticks, [NAME_MAP[c] for c in present], rotation=45)\n",
        "        plt.yticks(ticks, [NAME_MAP[c] for c in present])\n",
        "        thresh = cm.max() / 2.0 if cm.size else 0\n",
        "        for i in range(cm.shape[0]):\n",
        "            for j in range(cm.shape[1]):\n",
        "                plt.text(j, i, f\"{cm[i, j]:d}\",\n",
        "                         ha=\"center\",\n",
        "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        plt.ylabel('Verdadeiro')\n",
        "        plt.xlabel('Predito')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        metrics = {\n",
        "            \"model\": mdl, \"accuracy\": float(acc),\n",
        "            \"precision\": (float(prec) if np.isfinite(prec) else None),\n",
        "            \"recall\": (float(rec) if np.isfinite(rec) else None),\n",
        "            \"f1\": (float(f1) if np.isfinite(f1) else None),\n",
        "            \"avg_time\": avg_time, \"n_samples\": len(y_true),\n",
        "            \"classes_presentes\": [NAME_MAP[c] for c in present],\n",
        "            \"confusion_matrix\": cm,\n",
        "        }\n",
        "\n",
        "        if pd is not None:\n",
        "            try:\n",
        "                df = pd.DataFrame([{\n",
        "                    \"modelo\": metrics[\"model\"], \"accuracy\": metrics[\"accuracy\"],\n",
        "                    \"precision\": metrics[\"precision\"], \"recall\": metrics[\"recall\"],\n",
        "                    \"f1\": metrics[\"f1\"], \"avg_time\": metrics[\"avg_time\"],\n",
        "                    \"n\": metrics[\"n_samples\"], \"classes_presentes\": metrics[\"classes_presentes\"]\n",
        "                }])\n",
        "                display(df)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    # ===== 1) DNN (se necess√°rio) =====\n",
        "    _ensure_dnn_files_if_needed()\n",
        "\n",
        "    # ===== Use dataset_override if provided =====\n",
        "    if dataset_dir_override is not None:\n",
        "        print(f\"‚Ñπ Usando dataset override: {dataset_dir_override}\")\n",
        "        try:\n",
        "            metrics = _evaluate_current_detector(Path(dataset_dir_override), max_images=eval_max_images, conf_threshold=conf_threshold)\n",
        "            return metrics\n",
        "        except Exception as e:\n",
        "            print(\"‚úò Falha na avalia√ß√£o offline com dataset override:\", e)\n",
        "            return None\n",
        "\n",
        "    # ===== Use default_dataset_dir if provided =====\n",
        "    if default_dataset_dir is not None:\n",
        "        print(f\"‚Ñπ Usando dataset DEFAULT: {default_dataset_dir}\")\n",
        "        try:\n",
        "            metrics = _evaluate_current_detector(Path(default_dataset_dir), max_images=eval_max_images, conf_threshold=conf_threshold)\n",
        "            return metrics\n",
        "        except Exception as e:\n",
        "            print(\"‚úò Falha na avalia√ß√£o offline com dataset default:\", e)\n",
        "            return None\n",
        "\n",
        "    # ===== 2) usu√°rio alvo para positives (para dataset_auto) =====\n",
        "    target_user = None\n",
        "    if 'NEW_USER_ID' in globals() and NEW_USER_ID:\n",
        "        target_user = NEW_USER_ID\n",
        "    elif 'AUTH_MODE_1V1' in globals() and AUTH_MODE_1V1 and 'EXPECTED_USER_1V1' in globals() and EXPECTED_USER_1V1:\n",
        "        target_user = EXPECTED_USER_1V1\n",
        "\n",
        "    # ===== 3) (re)criar dataset_auto =====\n",
        "    if DATASET_AUTO.exists():\n",
        "        shutil.rmtree(DATASET_AUTO)\n",
        "    POS_DIR_AUTO.mkdir(parents=True, exist_ok=True)\n",
        "    NEG_DIR_AUTO.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # ===== 4) positives (para dataset_auto) =====\n",
        "    npos = 0\n",
        "    if target_user:\n",
        "        src_pos = ENROLL_DIR / target_user\n",
        "        if src_pos.is_dir():\n",
        "            npos = _copy_all_images(src_pos, POS_DIR_AUTO)\n",
        "            print(f\"‚úî Positives de '{target_user}': {npos}\")\n",
        "        else:\n",
        "            print(f\"‚úò N√£o encontrei ENROLL_DIR para '{target_user}': {src_pos}\")\n",
        "    else:\n",
        "        if ENROLL_DIR.is_dir():\n",
        "            for sub in ENROLL_DIR.iterdir():\n",
        "                if sub.is_dir():\n",
        "                    npos += _copy_all_images(sub, POS_DIR_AUTO)\n",
        "        print(f\"‚úî Positives (todos): {npos}\")\n",
        "\n",
        "    # ===== 5) negatives persistentes (para dataset_auto) =====\n",
        "    neg_existing = len(_list_images(NEG_STORE_DIR))\n",
        "    # op√ß√£o: copiar de pasta externa, se fornecida\n",
        "    if negatives_src_dir is not None and Path(negatives_src_dir).is_dir():\n",
        "        added = _copy_all_images(Path(negatives_src_dir), NEG_STORE_DIR)\n",
        "        neg_existing += added\n",
        "        print(f\"‚úî Negativos importados de '{negatives_src_dir}': +{added} (total={neg_existing})\")\n",
        "\n",
        "    if neg_existing < neg_target_min:\n",
        "        falta = neg_target_min - neg_existing\n",
        "        batch = max(capture_batch, falta)\n",
        "        print(f\"‚Ñπ Negativos existentes: {neg_existing}. Capturando {batch} para atingir >= {neg_target_min} ‚Ä¶\")\n",
        "        _capture_negatives_persistent(NEG_STORE_DIR, frames=batch, sleep=capture_sleep_sec)\n",
        "\n",
        "    # copiar negativos persistentes ‚Üí dataset_auto\n",
        "    nneg = _copy_all_images(NEG_STORE_DIR, NEG_DIR_AUTO)\n",
        "    print(f\"‚úî Negatives adicionados ao dataset_auto: {nneg}\")\n",
        "\n",
        "    # ===== 6) avalia√ß√£o (dataset_auto) =====\n",
        "    try:\n",
        "        metrics = _evaluate_current_detector(DATASET_AUTO, max_images=eval_max_images, conf_threshold=conf_threshold)\n",
        "        return metrics\n",
        "    except Exception as e:\n",
        "        print(\"‚úò Falha na avalia√ß√£o offline:\", e)\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "250ccea5",
      "metadata": {
        "id": "250ccea5"
      },
      "source": [
        "## 14) Pipelines compactos (1:1 e 1:N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "51b5a590",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "51b5a590",
        "outputId": "136aedf3-420b-4709-8657-bea3819d0f95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selecione o detector de faces a ser usado na c√©lula 5 (detect_faces).\n",
            "Op√ß√µes: 'haar' (CascadeClassifier) ou 'dnn_ssd_resnet10' (OpenCV DNN SSD ResNet10)\n",
            "Detector [dnn_ssd_resnet10/haar]: haar\n",
            "[Runner] Detector selecionado: haar\n",
            "Deseja incluir um novo usu√°rio no modelo ou apenas autenticar um j√° existente?\n",
            "Digite 'novo' para incluir, ou 'auth' para apenas autenticar.\n",
            "Modo [novo/auth]: novo\n",
            "Digite o identificador do novo usu√°rio (ou deixe em branco para gerar autom√°tico): teste\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coletando 30 amostras para 'teste'‚Ä¶ Olhe para a c√¢mera.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Enrollment conclu√≠do: 30/30 amostras salvas.\n",
            "‚Üª Treinando LBPH (mem√≥ria)‚Ä¶\n",
            "Modelo LBPH treinado em mem√≥ria. Usu√°rios: ['teste']\n",
            "üìè Calibrando limiar (coletando 15 dist√¢ncias)‚Ä¶\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/15] dist=68.2\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2/15] dist=97.3\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3/15] dist=67.8\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4/15] dist=68.6\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[5/15] dist=66.3\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[6/15] dist=63.3\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[7/15] dist=62.8\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[8/15] dist=63.5\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[9/15] dist=62.4\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[10/15] dist=66.6\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[11/15] dist=71.7\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[12/15] dist=64.9\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[13/15] dist=68.5\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[14/15] dist=64.6\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[15/15] dist=64.4\n",
            "üéØ Novo SERVICE_THRESHOLD = 84.4 (p95=79.4 + margem)\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Liveness] energia m√©dia: 8.83 -> live\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîê 1:1 ‚Äî esperado=teste | predito=teste | dist=63.7 | thr=84.4\n"
          ]
        },
        {
          "data": {
            "application/javascript": "if (window.stopColabCamera) window.stopColabCamera();",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'status': 'approved',\n",
              " 'mode': '1v1',\n",
              " 'user': 'teste',\n",
              " 'dist': 63.742280503879435,\n",
              " 'threshold': 84.4}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# =============================\n",
        "# Runner: inclus√£o (novo) ou autentica√ß√£o (auth) com escolha 1:1 / 1:N\n",
        "# + escolha do detector de faces (haar | dnn_ssd_resnet10)\n",
        "# =============================\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Aqui apenas definimos qual detector ser√° usado pelo pipeline j√° existente.\n",
        "print(\"Selecione o detector de faces a ser usado na c√©lula 5 (detect_faces).\")\n",
        "print(\"Op√ß√µes: 'haar' (CascadeClassifier) ou 'dnn_ssd_resnet10' (OpenCV DNN SSD ResNet10)\")\n",
        "_detector_choice = input(\"Detector [dnn_ssd_resnet10/haar]: \").strip().lower()\n",
        "if _detector_choice not in (\"\", \"haar\", \"dnn_ssd_resnet10\", \"dnn\", \"ssd\", \"resnet10\"):\n",
        "    print(\"Op√ß√£o inv√°lida; usando padr√£o: dnn_ssd_resnet10\")\n",
        "    _detector_choice = \"dnn_ssd_resnet10\"\n",
        "if _detector_choice in (\"\", \"dnn\", \"ssd\", \"resnet10\"):\n",
        "    _detector_choice = \"dnn_ssd_resnet10\"\n",
        "\n",
        "# Propaga para a c√©lula 5: vari√°vel global e vari√°vel de ambiente (para debug/diagn√≥stico)\n",
        "try:\n",
        "    DETECTION_MODEL  # verifica se existe a global da c√©lula 5\n",
        "    globals()[\"DETECTION_MODEL\"] = _detector_choice\n",
        "except NameError:\n",
        "    # Se a c√©lula 5 ainda n√£o foi executada, definimos aqui para n√£o quebrar;\n",
        "    # quando a c√©lula 5 for rodada, ela ler√° este valor do ambiente.\n",
        "    pass\n",
        "os.environ[\"DETECTION_MODEL\"] = _detector_choice\n",
        "print(f\"[Runner] Detector selecionado: {os.environ['DETECTION_MODEL']}\")\n",
        "\n",
        "# (Opcional) Valida√ß√£o r√°pida de arquivos quando DNN √© escolhido (evita erro tardio)\n",
        "if os.environ[\"DETECTION_MODEL\"] == \"dnn_ssd_resnet10\":\n",
        "    proto = os.environ.get(\"DNN_PROTO_PATH\", \"models/deploy.prototxt\")\n",
        "    weights = os.environ.get(\"DNN_WEIGHTS_PATH\", \"models/res10_300x300_ssd_iter_140000_fp16.caffemodel\")\n",
        "    if not (Path(proto).exists() and Path(weights).exists()):\n",
        "        print(\"[Aviso DNN] Arquivos do DNN n√£o encontrados.\")\n",
        "        print(f\"  Prototxt: {proto}\")\n",
        "        print(f\"  Pesos   : {weights}\")\n",
        "        print(\"  ‚Üí Ajuste DNN_PROTO_PATH / DNN_WEIGHTS_PATH ou coloque os arquivos em 'models/'.\")\n",
        "\n",
        "N_SAMPLES          = 30                      # amostras para enrollment (30‚Äì60 recomendado)\n",
        "FRAME_DELAY_MS     = 100                     # intervalo entre capturas (ms)\n",
        "LBPH_NEIGHBORS     = 16                      # 8 ou 16 costumam ir bem\n",
        "DO_LIVENESS_TEST   = True                    # liveness antes da autentica√ß√£o\n",
        "\n",
        "# Pergunta inicial\n",
        "print(\"Deseja incluir um novo usu√°rio no modelo ou apenas autenticar um j√° existente?\")\n",
        "print(\"Digite 'novo' para incluir, ou 'auth' para apenas autenticar.\")\n",
        "modo = input(\"Modo [novo/auth]: \").strip().lower()\n",
        "\n",
        "# Defini√ß√µes padr√£o\n",
        "NEW_USER_ID = None\n",
        "EXPECTED_USER_1V1 = None\n",
        "AUTH_MODE_1V1 = True  # default (ser√° perguntado quando for 'auth')\n",
        "\n",
        "if modo == \"novo\":\n",
        "    typed_name = input(\"Digite o identificador do novo usu√°rio (ou deixe em branco para gerar autom√°tico): \").strip()\n",
        "    if not typed_name:\n",
        "        # Gera nome autom√°tico com o pr√≥ximo √≠ndice baseado nas pastas j√° existentes\n",
        "        try:\n",
        "            existing_dirs = sorted([d.name for d in ENROLL_DIR.iterdir() if d.is_dir()])\n",
        "        except Exception:\n",
        "            existing_dirs = []\n",
        "        idx = len(existing_dirs)\n",
        "        typed_name = f\"novo_usuario_preview_{idx+1}\"\n",
        "        print(f\"[auto] Nome atribu√≠do: {typed_name}\")\n",
        "    NEW_USER_ID = typed_name\n",
        "    EXPECTED_USER_1V1 = NEW_USER_ID  # ap√≥s incluir, autentica 1:1 (mesma pessoa)\n",
        "\n",
        "elif modo == \"auth\":\n",
        "    # Escolha do tipo de autentica√ß√£o\n",
        "    print(\"Qual tipo de autentica√ß√£o deseja usar?\")\n",
        "    print(\"Digite '1' para 1:1 (comparar com um usu√°rio espec√≠fico) ou 'N' para 1:N (identificar entre os cadastrados).\")\n",
        "    auth_choice = input(\"Tipo [1/N]: \").strip().lower()\n",
        "    if auth_choice == \"1\":\n",
        "        AUTH_MODE_1V1 = True\n",
        "        EXPECTED_USER_1V1 = input(\"Digite o identificador do usu√°rio a ser autenticado (1:1): \").strip()\n",
        "        if not EXPECTED_USER_1V1:\n",
        "            raise ValueError(\"Para autentica√ß√£o 1:1 √© necess√°rio informar o identificador esperado.\")\n",
        "    else:\n",
        "        AUTH_MODE_1V1 = False\n",
        "        EXPECTED_USER_1V1 = None  # 1:N n√£o requer nome\n",
        "else:\n",
        "    raise ValueError(\"Op√ß√£o inv√°lida. Use 'novo' ou 'auth'.\")\n",
        "\n",
        "display_result = {\"status\":\"error\", \"reason\":\"not_executed\"}\n",
        "try:\n",
        "    if modo == \"novo\":\n",
        "        # 1) enrollment do novo usu√°rio\n",
        "        enroll_user_with_preview(user_id=NEW_USER_ID, n_samples=N_SAMPLES, interval_ms=FRAME_DELAY_MS)\n",
        "        # 2) re-treino do modelo em mem√≥ria\n",
        "        _ = get_recognizer(neighbors=LBPH_NEIGHBORS, force_retrain=True)\n",
        "        # 3) calibra√ß√£o de threshold\n",
        "        calibrate_threshold(samples=15, neighbors=LBPH_NEIGHBORS)\n",
        "    else:\n",
        "        # Apenas autentica√ß√£o: garante que o modelo est√° carregado (re-treina se necess√°rio)\n",
        "        try:\n",
        "            _ = get_recognizer(neighbors=LBPH_NEIGHBORS, force_retrain=False)\n",
        "        except Exception:\n",
        "            _ = get_recognizer(neighbors=LBPH_NEIGHBORS, force_retrain=True)\n",
        "\n",
        "    # 4) autentica√ß√£o (sempre roda)\n",
        "    if AUTH_MODE_1V1:\n",
        "        display_result = authenticate_1v1_preview(expected_user=EXPECTED_USER_1V1,\n",
        "                                                  neighbors=LBPH_NEIGHBORS,\n",
        "                                                  require_liveness=DO_LIVENESS_TEST)\n",
        "    else:\n",
        "        display_result = authenticate_1vN_preview(neighbors=LBPH_NEIGHBORS,\n",
        "                                                  require_liveness=DO_LIVENESS_TEST)\n",
        "\n",
        "finally:\n",
        "    # sempre encerra a pr√©-visualiza√ß√£o no Colab / janelas no local\n",
        "    try:\n",
        "        stop_camera_ui()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "display_result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C9h-DdbkOaqp",
      "metadata": {
        "id": "C9h-DdbkOaqp"
      },
      "source": [
        "## 15) Execu√ß√£o de AVALIA√á√ÉO offline (ap√≥s o fluxo de c√¢mera)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "RxMyTuepD2e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RxMyTuepD2e2",
        "outputId": "43fed9ef-4fb5-4496-baca-351b0eb9b806"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rodar avalia√ß√£o offline (Autom√°tico/Default/Remoto)? [s/N]: s\n",
            "Fonte do dataset:\n",
            "  1) Autom√°tico (enrollment + negatives persistentes/c√¢mera)\n",
            "  2) DEFAULT (cv_colab_data/dataset_default)\n",
            "  3) REMOTO/LOCAL existente (URL .zip/.tar/.tar.gz/.tgz ou caminho local)\n",
            "Escolha [1/2/3]: 3\n",
            "Informe URL (.zip/.tar/.tar.gz/.tgz) OU caminho local com positives/ e negatives/ (Enter = usar default https://data.caltech.edu/records/6rjah-hdv18/files/faces.tar): \n",
            "Baixando dataset remoto: https://data.caltech.edu/records/6rjah-hdv18/files/faces.tar\n",
            "‚úî Arquivo salvo em: /tmp/tmp7mp31qeu/dataset_remote.tar\n",
            "[debug] Arquivos extra√≠dos: 452 (mostrando at√© 12)\n",
            "   - cv_colab_data/dataset_remote/image_0354.jpg\n",
            "   - cv_colab_data/dataset_remote/image_0024.jpg\n",
            "   - cv_colab_data/dataset_remote/image_0350.jpg\n",
            "   - cv_colab_data/dataset_remote/image_0025.jpg\n",
            "   - cv_colab_data/dataset_remote/image_0406.jpg\n",
            "   - cv_colab_data/dataset_remote/image_0280.jpg\n",
            "   - cv_colab_data/dataset_remote/image_0032.jpg\n",
            "   - cv_colab_data/dataset_remote/image_0095.jpg\n",
            "   - cv_colab_data/dataset_remote/image_0059.jpg\n",
            "   - cv_colab_data/dataset_remote/image_0360.jpg\n",
            "   - cv_colab_data/dataset_remote/image_0033.jpg\n",
            "   - cv_colab_data/dataset_remote/image_0048.jpg\n",
            "[single-class] Convertendo todas as imagens extra√≠das em 'positives/' e completando 'negatives/'‚Ä¶\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2285713670.py:239: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tf.extractall(str(out_dir))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[single-class] Positives criados: 450\n",
            "[negatives] Nenhum negative encontrado. Baixando Caltech-101‚Ä¶\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2285713670.py:109: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tf.extractall(str(out_dir)); return True\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[negatives] Encontrado BACKGROUND_Google ‚Äî usando como negativos.\n",
            "[negatives] Copiados 467 negatives ‚Üí cv_colab_data/dataset_remote/negatives\n",
            "‚úî dataset 'single-class' preparado em: cv_colab_data/dataset_remote\n",
            "Usar c√¢mera para completar negativos (tamb√©m em Remoto/Default)? [S/n]: s\n",
            "Pasta extra com negativos (vazio = nenhuma): \n",
            "Limitar n¬∫ de imagens por classe? (vazio = sem limite): \n",
            "Conf threshold (vazio = 0.5): \n",
            "‚Ñπ Usando dataset override: cv_colab_data/dataset_remote\n",
            "\n",
            "=== Relat√≥rio (2 classes) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     no_face       0.99      0.90      0.95       467\n",
            "        face       0.91      0.99      0.95       450\n",
            "\n",
            "    accuracy                           0.95       917\n",
            "   macro avg       0.95      0.95      0.95       917\n",
            "weighted avg       0.95      0.95      0.95       917\n",
            "\n",
            "\n",
            "=== Resultados: haar ===\n",
            "Accuracy : 0.9466\n",
            "Precision: 0.9066937119675457\n",
            "Recall   : 0.9933333333333333\n",
            "F1-score : 0.9480381760339343\n",
            "Tempo m√©dio por imagem: 0.1237 s\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x300 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"        )\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"modelo\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"haar\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.9465648854961832,\n        \"max\": 0.9465648854961832,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9465648854961832\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.9066937119675457,\n        \"max\": 0.9066937119675457,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9066937119675457\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.9933333333333333,\n        \"max\": 0.9933333333333333,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9933333333333333\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.9480381760339343,\n        \"max\": 0.9480381760339343,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9480381760339343\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.12374462504253199,\n        \"max\": 0.12374462504253199,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.12374462504253199\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 917,\n        \"max\": 917,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          917\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"classes_presentes\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-df7f833e-80d0-4e6e-802a-965aa9d8ba1f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>modelo</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1</th>\n",
              "      <th>avg_time</th>\n",
              "      <th>n</th>\n",
              "      <th>classes_presentes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>haar</td>\n",
              "      <td>0.946565</td>\n",
              "      <td>0.906694</td>\n",
              "      <td>0.993333</td>\n",
              "      <td>0.948038</td>\n",
              "      <td>0.123745</td>\n",
              "      <td>917</td>\n",
              "      <td>[no_face, face]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df7f833e-80d0-4e6e-802a-965aa9d8ba1f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-df7f833e-80d0-4e6e-802a-965aa9d8ba1f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-df7f833e-80d0-4e6e-802a-965aa9d8ba1f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "  modelo  accuracy  precision    recall        f1  avg_time    n  \\\n",
              "0   haar  0.946565   0.906694  0.993333  0.948038  0.123745  917   \n",
              "\n",
              "  classes_presentes  \n",
              "0   [no_face, face]  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# =============================\n",
        "# Execu√ß√£o de AVALIA√á√ÉO offline (ap√≥s o fluxo de c√¢mera)\n",
        "# - Fonte: Autom√°tico | Default | Remoto/Local\n",
        "# - Remoto default (positivos): Caltech Face 1999 (faces.tar)\n",
        "# - Fallback de negativos: Caltech-101 (extrai camadas internas; usa BACKGROUND_Google ou outras classes n√£o-face)\n",
        "# - Captura por c√¢mera tamb√©m em Remoto/Default quando negatives/ estiver vazio\n",
        "# =============================\n",
        "from pathlib import Path\n",
        "import os, re, shutil, tarfile, zipfile, urllib.request, tempfile, glob\n",
        "\n",
        "# ---------- diret√≥rios base ----------\n",
        "try:\n",
        "    DATA_DIR\n",
        "except NameError:\n",
        "    DATA_DIR = Path(\"cv_colab_data\")\n",
        "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DEFAULT_DATASET_DIR = DATA_DIR / \"dataset_default\"\n",
        "(DEFAULT_DATASET_DIR / \"positives\").mkdir(parents=True, exist_ok=True)\n",
        "(DEFAULT_DATASET_DIR / \"negatives\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------- URLs defaults ----------\n",
        "# Positivos (rostos) ‚Äî Caltech Face 1999 (CaltechDATA)\n",
        "REMOTE_DATASET_URL_DEFAULT = \"https://data.caltech.edu/records/6rjah-hdv18/files/faces.tar\"\n",
        "# Negativos fallback ‚Äî Caltech-101 (CaltechDATA, zip que cont√©m .tar.gz internamente)\n",
        "CALTECH101_URL = \"https://data.caltech.edu/records/mzrjq-6wc02/files/caltech-101.zip\"\n",
        "\n",
        "# ---------- helpers utilit√°rios ----------\n",
        "def _debug_list_some_files(root: Path, n=12):\n",
        "    files = [p for p in root.rglob(\"*\") if p.is_file()]\n",
        "    print(f\"[debug] Arquivos extra√≠dos: {len(files)} (mostrando at√© {n})\")\n",
        "    for p in files[:n]:\n",
        "        print(\"   -\", p.as_posix())\n",
        "\n",
        "def _find_dataset_root_with_pos_neg(root: Path) -> Path | None:\n",
        "    \"\"\"Procura um diret√≥rio que contenha positives/ e negatives/.\"\"\"\n",
        "    if (root / \"positives\").is_dir() and (root / \"negatives\").is_dir():\n",
        "        return root\n",
        "    for p in root.rglob(\"*\"):\n",
        "        if p.is_dir() and (p / \"positives\").is_dir() and (p / \"negatives\").is_dir():\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "def _collect_images(root: Path):\n",
        "    return [Path(x) for x in glob.glob(str(root / \"**\" / \"*\"), recursive=True)\n",
        "            if os.path.isfile(x) and Path(x).suffix.lower() in (\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\",\".pgm\")]\n",
        "\n",
        "def _reorganize_faces_nonfaces(src_root: Path) -> Path | None:\n",
        "    \"\"\"\n",
        "    Heur√≠stica para separar positivos/negativos:\n",
        "      - negativos: 'nonface', 'non-face', 'non_face', 'non', 'neg', 'background', 'bg'\n",
        "      - positivos: cont√©m 'face' e N√ÉO cont√©m termos de negativo (evita 'interface')\n",
        "    \"\"\"\n",
        "    neg_patterns = (r\"non[\\-_ ]?face\", r\"\\bnon\\b\", r\"\\bneg(ative)?s?\\b\", r\"background\", r\"\\bbg\\b\")\n",
        "    def is_neg(path_str: str):\n",
        "        low = path_str.lower()\n",
        "        return any(re.search(p, low) for p in neg_patterns)\n",
        "\n",
        "    pos_dir = src_root / \"positives\"; pos_dir.mkdir(exist_ok=True)\n",
        "    neg_dir = src_root / \"negatives\"; neg_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    copied_pos = copied_neg = 0\n",
        "    for f in _collect_images(src_root):\n",
        "        s = f.as_posix().lower()\n",
        "        if is_neg(s):\n",
        "            dst = neg_dir / f.name\n",
        "            try: shutil.copy2(f, dst); copied_neg += 1\n",
        "            except: pass\n",
        "        elif (\"face\" in s) and not is_neg(s) and (\"interface\" not in s):\n",
        "            dst = pos_dir / f.name\n",
        "            try: shutil.copy2(f, dst); copied_pos += 1\n",
        "            except: pass\n",
        "\n",
        "    if copied_pos + copied_neg == 0:\n",
        "        return None\n",
        "    print(f\"[reorganize] positives={copied_pos}, negatives={copied_neg}\")\n",
        "    return src_root\n",
        "\n",
        "# ---------- negativos: Caltech-101 com extra√ß√£o recursiva ----------\n",
        "def _ensure_negatives_from_caltech101(neg_dir: Path, max_to_copy: int | None = None):\n",
        "    \"\"\"\n",
        "    Se 'neg_dir' estiver vazio, baixa Caltech-101 (CaltechDATA, .zip) e preenche negativos:\n",
        "      1) Extrai recursivamente camadas internas (.zip/.tar/.tgz/.tar.gz).\n",
        "      2) Prefer√™ncia: pasta BACKGROUND_Google.\n",
        "      3) Se n√£o houver, usa outras categorias != 'face' como negativos.\n",
        "    \"\"\"\n",
        "    if any(neg_dir.iterdir()):\n",
        "        return\n",
        "\n",
        "    print(\"[negatives] Nenhum negative encontrado. Baixando Caltech-101‚Ä¶\")\n",
        "    import zipfile, tarfile\n",
        "    tmp = Path(tempfile.mkdtemp())\n",
        "    try:\n",
        "        # download\n",
        "        guessed_suffix = \"\".join(Path(CALTECH101_URL.split(\"?\")[0]).suffixes) or \".zip\"\n",
        "        arc = tmp / (\"caltech101\" + guessed_suffix)\n",
        "        urllib.request.urlretrieve(CALTECH101_URL, str(arc))\n",
        "\n",
        "        extract_root = tmp / \"caltech101_extracted\"\n",
        "        extract_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        def _extract_once(archive_path: Path, out_dir: Path):\n",
        "            low = archive_path.name.lower()\n",
        "            if low.endswith(\".zip\"):\n",
        "                with zipfile.ZipFile(str(archive_path), \"r\") as zf:\n",
        "                    zf.extractall(str(out_dir)); return True\n",
        "            if low.endswith((\".tar.gz\", \".tgz\", \".tar\")):\n",
        "                with tarfile.open(str(archive_path), \"r:*\") as tf:\n",
        "                    tf.extractall(str(out_dir)); return True\n",
        "            return False\n",
        "\n",
        "        # primeira camada\n",
        "        if not _extract_once(arc, extract_root):\n",
        "            print(\"[negatives] Formato n√£o reconhecido no primeiro n√≠vel, abortando fallback.\")\n",
        "            return\n",
        "\n",
        "        # extrai camadas internas at√© 2 n√≠veis\n",
        "        def _extract_nested_archives(root: Path, max_depth: int = 2):\n",
        "            for _ in range(max_depth):\n",
        "                inner = [p for p in root.rglob(\"*\") if p.is_file() and (\n",
        "                    p.name.lower().endswith(\".zip\") or\n",
        "                    p.name.lower().endswith(\".tar\") or\n",
        "                    p.name.lower().endswith(\".tar.gz\") or\n",
        "                    p.name.lower().endswith(\".tgz\") or\n",
        "                    p.name.lower().endswith(\".gz\")\n",
        "                )]\n",
        "                any_new = False\n",
        "                for a in inner:\n",
        "                    base = a.name\n",
        "                    for suf in (\".tar.gz\", \".tgz\", \".zip\", \".tar\", \".gz\"):\n",
        "                        if base.endswith(suf):\n",
        "                            base = base[: -len(suf)]\n",
        "                    out = a.parent / (base + \"_extracted\")\n",
        "                    out.mkdir(exist_ok=True)\n",
        "                    try:\n",
        "                        if _extract_once(a, out):\n",
        "                            any_new = True\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                if not any_new:\n",
        "                    break\n",
        "\n",
        "        _extract_nested_archives(extract_root, max_depth=2)\n",
        "\n",
        "        # localizar BACKGROUND_Google\n",
        "        bg = None\n",
        "        for p in extract_root.rglob(\"*\"):\n",
        "            if p.is_dir() and p.name.lower() == \"background_google\":\n",
        "                bg = p; break\n",
        "\n",
        "        exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
        "        copied = 0\n",
        "        def _copy_all_images(src_dir: Path):\n",
        "            nonlocal copied\n",
        "            for f in src_dir.rglob(\"*\"):\n",
        "                if f.is_file() and f.suffix.lower() in exts:\n",
        "                    if max_to_copy is not None and copied >= max_to_copy:\n",
        "                        return\n",
        "                    dst = neg_dir / f.name\n",
        "                    try: shutil.copy2(f, dst); copied += 1\n",
        "                    except: pass\n",
        "\n",
        "        if bg is not None:\n",
        "            print(\"[negatives] Encontrado BACKGROUND_Google ‚Äî usando como negativos.\")\n",
        "            _copy_all_images(bg)\n",
        "        else:\n",
        "            print(\"[negatives] BACKGROUND_Google n√£o encontrado. Usando outras categorias como negativos.\")\n",
        "            # escolher diret√≥rio com mais subpastas (normalmente '101_ObjectCategories')\n",
        "            candidate_roots = [p for p in extract_root.rglob(\"*\") if p.is_dir()]\n",
        "            root = max(candidate_roots, key=lambda d: sum(1 for _ in d.iterdir() if _.is_dir()), default=extract_root)\n",
        "            for cat in root.iterdir():\n",
        "                if not cat.is_dir(): continue\n",
        "                name = cat.name.lower()\n",
        "                if \"face\" in name:  # n√£o usar categorias de rosto como negativos\n",
        "                    continue\n",
        "                _copy_all_images(cat)\n",
        "                if max_to_copy is not None and copied >= max_to_copy:\n",
        "                    break\n",
        "\n",
        "        print(f\"[negatives] Copiados {copied} negatives ‚Üí {neg_dir}\")\n",
        "        if copied == 0:\n",
        "            print(\"[negatives] Aviso: n√£o foi poss√≠vel obter imagens negativas do Caltech-101 (verifique o pacote).\")\n",
        "\n",
        "    finally:\n",
        "        try: shutil.rmtree(tmp)\n",
        "        except: pass\n",
        "\n",
        "# ---------- negativos via c√¢mera (agora tamb√©m em Remoto/Default) ----------\n",
        "def _ensure_negatives_via_camera(neg_dir: Path, frames: int = 40, sleep_sec: float = 0.15):\n",
        "    import time, cv2\n",
        "    neg_dir.mkdir(parents=True, exist_ok=True)\n",
        "    cap = cv2.VideoCapture(0)\n",
        "    if not cap.isOpened():\n",
        "        print(\"‚úò C√¢mera indispon√≠vel para capturar negativos.\")\n",
        "        return 0\n",
        "    print(f\"üé• Capturando {frames} negativos‚Ä¶ (aponte para parede/fundo vazio)\")\n",
        "    saved = 0\n",
        "    for i in range(frames):\n",
        "        ok, frame = cap.read()\n",
        "        if not ok: break\n",
        "        cv2.imwrite(str(neg_dir / f\"neg_cam_{int(time.time())}_{i:03d}.jpg\"), frame)\n",
        "        time.sleep(sleep_sec); saved += 1\n",
        "    cap.release()\n",
        "    print(f\"‚úî Negativos capturados: {saved}\")\n",
        "    return saved\n",
        "\n",
        "# ---------- download/extra√ß√£o do dataset remoto (faces.tar, etc.) ----------\n",
        "def _download_and_unpack_archive(url: str, dest_root: Path, *, fallback_to_default=True) -> Path:\n",
        "    \"\"\"\n",
        "    Baixa .zip/.tar/.tar.gz/.tgz para dest_root/dataset_remote e tenta:\n",
        "      1) Encontrar positives/ e negatives/;\n",
        "      2) Reorganizar por heur√≠stica (face vs non/background);\n",
        "      3) Completar negatives com Caltech-101, se necess√°rio;\n",
        "      4) Fallback POSITIVOS default (Caltech Face 1999);\n",
        "      5) √öltimo recurso: 'single-class' (todas imagens -> positives) + completar negatives.\n",
        "    Retorna a pasta final contendo positives/ e negatives/.\n",
        "    \"\"\"\n",
        "    dest_root.mkdir(parents=True, exist_ok=True)\n",
        "    tmp_dir = Path(tempfile.mkdtemp())\n",
        "    try:\n",
        "        suffixes = \"\".join(Path(url.split(\"?\")[0]).suffixes) or \".bin\"\n",
        "        archive_path = tmp_dir / f\"dataset_remote{suffixes}\"\n",
        "        print(f\"Baixando dataset remoto: {url}\")\n",
        "        urllib.request.urlretrieve(url, str(archive_path))\n",
        "        print(f\"‚úî Arquivo salvo em: {archive_path}\")\n",
        "\n",
        "        out_dir = dest_root / \"dataset_remote\"\n",
        "        if out_dir.exists(): shutil.rmtree(out_dir)\n",
        "        out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # extrair primeira camada\n",
        "        lower = archive_path.name.lower()\n",
        "        try:\n",
        "            if lower.endswith(\".zip\"):\n",
        "                with zipfile.ZipFile(str(archive_path), \"r\") as zf:\n",
        "                    zf.extractall(str(out_dir))\n",
        "            elif lower.endswith((\".tar.gz\", \".tgz\", \".tar\")):\n",
        "                with tarfile.open(str(archive_path), \"r:*\") as tf:\n",
        "                    tf.extractall(str(out_dir))\n",
        "            else:\n",
        "                try:\n",
        "                    with zipfile.ZipFile(str(archive_path), \"r\") as zf:\n",
        "                        zf.extractall(str(out_dir))\n",
        "                except zipfile.BadZipFile:\n",
        "                    with tarfile.open(str(archive_path), \"r:*\") as tf:\n",
        "                        tf.extractall(str(out_dir))\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Falha ao extrair: {e}\")\n",
        "\n",
        "        # extrair camadas internas at√© 2 n√≠veis (se houver)\n",
        "        def _extract_nested_archives(root: Path, max_depth: int = 2):\n",
        "            for _ in range(max_depth):\n",
        "                inner = [p for p in root.rglob(\"*\") if p.is_file() and (\n",
        "                    p.name.lower().endswith(\".zip\") or\n",
        "                    p.name.lower().endswith(\".tar\") or\n",
        "                    p.name.lower().endswith(\".tar.gz\") or\n",
        "                    p.name.lower().endswith(\".tgz\") or\n",
        "                    p.name.lower().endswith(\".gz\")\n",
        "                )]\n",
        "                any_new = False\n",
        "                for a in inner:\n",
        "                    base = a.name\n",
        "                    for suf in (\".tar.gz\", \".tgz\", \".zip\", \".tar\", \".gz\"):\n",
        "                        if base.endswith(suf):\n",
        "                            base = base[: -len(suf)]\n",
        "                    out = a.parent / (base + \"_extracted\")\n",
        "                    out.mkdir(exist_ok=True)\n",
        "                    try:\n",
        "                        if a.name.lower().endswith(\".zip\"):\n",
        "                            with zipfile.ZipFile(str(a), \"r\") as zf: zf.extractall(str(out))\n",
        "                            any_new = True\n",
        "                        else:\n",
        "                            with tarfile.open(str(a), \"r:*\") as tf: tf.extractall(str(out))\n",
        "                            any_new = True\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                if not any_new:\n",
        "                    break\n",
        "\n",
        "        _extract_nested_archives(out_dir, max_depth=2)\n",
        "        _debug_list_some_files(out_dir)\n",
        "\n",
        "        # j√° existe positives/negatives?\n",
        "        ds = _find_dataset_root_with_pos_neg(out_dir)\n",
        "        if ds:\n",
        "            print(f\"‚úî dataset com positives/negatives encontrado em: {ds}\")\n",
        "            _ensure_negatives_from_caltech101(ds / \"negatives\")\n",
        "            return ds\n",
        "\n",
        "        # tenta reorganizar por heur√≠stica\n",
        "        ds = _reorganize_faces_nonfaces(out_dir)\n",
        "        if ds:\n",
        "            _ensure_negatives_from_caltech101(ds / \"negatives\")\n",
        "            if _find_dataset_root_with_pos_neg(ds):\n",
        "                print(f\"‚úî dataset reorganizado em: {ds}\")\n",
        "                return ds\n",
        "\n",
        "        # fallback: positivos default\n",
        "        if fallback_to_default and (url != REMOTE_DATASET_URL_DEFAULT):\n",
        "            print(\"[fallback] Estrutura n√£o reconhecida. Baixando POSITIVOS default (Caltech Face 1999)‚Ä¶\")\n",
        "            return _download_and_unpack_archive(REMOTE_DATASET_URL_DEFAULT, dest_root, fallback_to_default=False)\n",
        "\n",
        "        # √∫ltimo recurso: single-class (tudo -> positives) + completar negatives\n",
        "        print(\"[single-class] Convertendo todas as imagens extra√≠das em 'positives/' e completando 'negatives/'‚Ä¶\")\n",
        "        pos_dir = out_dir / \"positives\"\n",
        "        neg_dir = out_dir / \"negatives\"\n",
        "        pos_dir.mkdir(exist_ok=True); neg_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        imgs = _collect_images(out_dir)\n",
        "        copied = 0\n",
        "        for f in imgs:\n",
        "            # evitar arquivos j√° nas pastas alvo\n",
        "            try:\n",
        "                if f.is_relative_to(pos_dir) or f.is_relative_to(neg_dir):\n",
        "                    continue\n",
        "            except AttributeError:\n",
        "                # Python < 3.9: fallback\n",
        "                if str(pos_dir) in str(f.parent) or str(neg_dir) in str(f.parent):\n",
        "                    continue\n",
        "            try:\n",
        "                shutil.copy2(f, pos_dir / f.name); copied += 1\n",
        "            except: pass\n",
        "        print(f\"[single-class] Positives criados: {copied}\")\n",
        "\n",
        "        _ensure_negatives_from_caltech101(neg_dir)\n",
        "\n",
        "        if _find_dataset_root_with_pos_neg(out_dir):\n",
        "            print(f\"‚úî dataset 'single-class' preparado em: {out_dir}\")\n",
        "            return out_dir\n",
        "\n",
        "        raise ValueError(\"N√£o foi poss√≠vel montar positives/ e negatives/ automaticamente.\")\n",
        "    finally:\n",
        "        try: shutil.rmtree(tmp_dir)\n",
        "        except: pass\n",
        "\n",
        "# compat com vers√µes antigas do c√≥digo\n",
        "_download_and_unpack_zip = _download_and_unpack_archive\n",
        "\n",
        "# ---------- fluxo interativo ----------\n",
        "try:\n",
        "    run_eval_auto = input(\n",
        "        \"Rodar avalia√ß√£o offline (Autom√°tico/Default/Remoto)? [s/N]: \"\n",
        "    ).strip().lower() in (\"s\",\"sim\",\"y\",\"yes\")\n",
        "except Exception:\n",
        "    run_eval_auto = False\n",
        "\n",
        "if run_eval_auto:\n",
        "    print(\"Fonte do dataset:\")\n",
        "    print(\"  1) Autom√°tico (enrollment + negatives persistentes/c√¢mera)\")\n",
        "    print(f\"  2) DEFAULT ({DEFAULT_DATASET_DIR})\")\n",
        "    print(\"  3) REMOTO/LOCAL existente (URL .zip/.tar/.tar.gz/.tgz ou caminho local)\")\n",
        "    choice = (input(\"Escolha [1/2/3]: \").strip() or \"1\")\n",
        "\n",
        "    dataset_override = None\n",
        "    default_dir = None\n",
        "\n",
        "    if choice == \"2\":\n",
        "        default_dir = DEFAULT_DATASET_DIR\n",
        "        print(f\"‚Ñπ Usando dataset DEFAULT: {default_dir}\")\n",
        "\n",
        "    elif choice == \"3\":\n",
        "        ds_input = input(\n",
        "            \"Informe URL (.zip/.tar/.tar.gz/.tgz) OU caminho local com positives/ e negatives/ \"\n",
        "            f\"(Enter = usar default {REMOTE_DATASET_URL_DEFAULT}): \"\n",
        "        ).strip()\n",
        "        if not ds_input:\n",
        "            ds_input = REMOTE_DATASET_URL_DEFAULT\n",
        "\n",
        "        if ds_input.startswith((\"http://\",\"https://\")):\n",
        "            dataset_override = _download_and_unpack_archive(ds_input, DATA_DIR)\n",
        "        else:\n",
        "            p = Path(ds_input)\n",
        "            if not p.exists():\n",
        "                raise ValueError(f\"Caminho n√£o existe: {p}\")\n",
        "            if not ((p / \"positives\").is_dir() and (p / \"negatives\").is_dir()):\n",
        "                raise ValueError(f\"Caminho inv√°lido (esperado positives/ e negatives/): {p}\")\n",
        "            dataset_override = p\n",
        "\n",
        "    # Perguntas opcionais (as mesmas que voc√™ j√° tinha)\n",
        "    try:\n",
        "        use_camera = input(\"Usar c√¢mera para completar negativos (tamb√©m em Remoto/Default)? [S/n]: \").strip().lower()\n",
        "        use_camera = not (use_camera in (\"n\",\"nao\",\"n√£o\"))\n",
        "    except Exception:\n",
        "        use_camera = True\n",
        "\n",
        "    try:\n",
        "        neg_src = input(\"Pasta extra com negativos (vazio = nenhuma): \").strip() or None\n",
        "    except Exception:\n",
        "        neg_src = None\n",
        "\n",
        "    try:\n",
        "        max_imgs = input(\"Limitar n¬∫ de imagens por classe? (vazio = sem limite): \").strip()\n",
        "        max_imgs = int(max_imgs) if max_imgs else None\n",
        "    except Exception:\n",
        "        max_imgs = None\n",
        "\n",
        "    try:\n",
        "        th = input(\"Conf threshold (vazio = 0.5): \").strip()\n",
        "        th = float(th) if th else 0.5\n",
        "    except Exception:\n",
        "        th = 0.5\n",
        "\n",
        "    # ---------- chamadas finais ----------\n",
        "    if dataset_override is not None:\n",
        "        # REMOTO/LOCAL (pronto/baixado)\n",
        "        # se negatives/ estiver vazio e o usu√°rio permitir, completa com c√¢mera\n",
        "        neg_dir_check = dataset_override / \"negatives\"\n",
        "        if use_camera and (not any(neg_dir_check.iterdir())):\n",
        "            _ensure_negatives_via_camera(neg_dir_check, frames=40, sleep_sec=0.15)\n",
        "\n",
        "        _ = run_offline_eval_from_enrollment(\n",
        "            eval_max_images=max_imgs,\n",
        "            conf_threshold=th,\n",
        "            use_camera=False,                 # j√° tratamos c√¢mera acima\n",
        "            negatives_src_dir=None,\n",
        "            dataset_dir_override=dataset_override,\n",
        "            default_dataset_dir=None,\n",
        "        )\n",
        "\n",
        "    elif default_dir is not None:\n",
        "        # DEFAULT\n",
        "        neg_dir_check = default_dir / \"negatives\"\n",
        "        if use_camera and (not any(neg_dir_check.iterdir())):\n",
        "            _ensure_negatives_via_camera(neg_dir_check, frames=40, sleep_sec=0.15)\n",
        "\n",
        "        _ = run_offline_eval_from_enrollment(\n",
        "            eval_max_images=max_imgs,\n",
        "            conf_threshold=th,\n",
        "            use_camera=False,                 # j√° tratamos c√¢mera acima\n",
        "            negatives_src_dir=None,\n",
        "            dataset_dir_override=None,\n",
        "            default_dataset_dir=default_dir,\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        # AUTOM√ÅTICO (enrollment + negativos persistentes/c√¢mera/pasta extra)\n",
        "        _ = run_offline_eval_from_enrollment(\n",
        "            neg_target_min=30,\n",
        "            capture_batch=30,\n",
        "            capture_sleep_sec=0.15,\n",
        "            eval_max_images=max_imgs,\n",
        "            conf_threshold=th,\n",
        "            use_camera=use_camera,            # aqui a c√¢mera ainda vale dentro da fun√ß√£o\n",
        "            negatives_src_dir=neg_src,\n",
        "            dataset_dir_override=None,\n",
        "            default_dataset_dir=None,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c1ab4dd",
      "metadata": {
        "id": "9c1ab4dd"
      },
      "source": [
        "\n",
        "## 15) FaceNet (Embeddings) ‚Äî Reconhecedor Opcional\n",
        "\n",
        "Esta se√ß√£o adiciona **FaceNet** (via `facenet-pytorch`) como alternativa ao LBPH.  \n",
        "A autentica√ß√£o passa a comparar **vetores de embeddings** por **cosine similarity** (ou L2).\n",
        "\n",
        "> **Pr√©-requisitos**: funcionar melhor em GPU (Colab). Em CPU roda, mas fica mais lento.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "596d4860",
      "metadata": {
        "id": "596d4860"
      },
      "source": [
        "\n",
        "### 15.1) Instala√ß√£o (Colab/Local)\n",
        "Se estiver no **Colab**, instale as depend√™ncias abaixo. Em ambiente local, use seu gerenciador de pacotes preferido.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EfHO1mkn_wHq",
      "metadata": {
        "id": "EfHO1mkn_wHq"
      },
      "source": [
        "Colab (CUDA 12.x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E6Iotnr70qEi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E6Iotnr70qEi",
        "outputId": "e6bdf82f-6ac5-4453-9bb6-7a61c68126d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 2.8.0\n",
            "Uninstalling torch-2.8.0:\n",
            "  Successfully uninstalled torch-2.8.0\n",
            "Found existing installation: torchvision 0.17.0\n",
            "Uninstalling torchvision-0.17.0:\n",
            "  Successfully uninstalled torchvision-0.17.0\n",
            "Found existing installation: torchaudio 2.8.0\n",
            "Uninstalling torchaudio-2.8.0:\n",
            "  Successfully uninstalled torchaudio-2.8.0\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (780.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m780.4/780.4 MB\u001b[0m \u001b[31m663.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m690.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m599.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.1.0 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m461.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Collecting sympy==1.13.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.8.93)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (10.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Installing collected packages: triton, sympy, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.4.0\n",
            "    Uninstalling triton-3.4.0:\n",
            "      Successfully uninstalled triton-3.4.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.3\n",
            "    Uninstalling sympy-1.13.3:\n",
            "      Successfully uninstalled sympy-1.13.3\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.8.90\n",
            "    Uninstalling nvidia-nvtx-cu12-12.8.90:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.8.90\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
            "    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.8.90\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.8.90:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.8.90\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.8.93\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.8.93:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.8.93\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.8.90\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.8.90:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.8.90\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "facenet-pytorch 2.6.0 requires torch<2.3.0,>=2.2.0, but you have torch 2.5.1+cu121 which is incompatible.\n",
            "facenet-pytorch 2.6.0 requires torchvision<0.18.0,>=0.17.0, but you have torchvision 0.20.1+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.21.5 nvidia-nvtx-cu12-12.1.105 sympy-1.13.1 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121 triton-3.1.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "fee2c43d90b44f438a01c8c919b6d1b8",
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: facenet-pytorch in /usr/local/lib/python3.12/dist-packages (2.6.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (1.26.4)\n",
            "Requirement already satisfied: Pillow<10.3.0,>=10.2.0 in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (10.2.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (2.32.4)\n",
            "Collecting torch<2.3.0,>=2.2.0 (from facenet-pytorch)\n",
            "  Using cached torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting torchvision<0.18.0,>=0.17.0 (from facenet-pytorch)\n",
            "  Using cached torchvision-0.17.2-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (4.67.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.0.106)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3.0,>=2.2.0->facenet-pytorch) (12.8.93)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet-pytorch) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch<2.3.0,>=2.2.0->facenet-pytorch) (1.3.0)\n",
            "Using cached torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl (755.5 MB)\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# 1) Limpe vers√µes conflitantes (opcional, mas recomendado)\n",
        "!pip uninstall -y torch torchvision torchaudio\n",
        "# 2) Instale PyTorch compat√≠vel com a CUDA do Colab (12.1 hoje)\n",
        "!pip install -U --index-url https://download.pytorch.org/whl/cu121 torch torchvision torchaudio\n",
        "# 3) Instale o facenet-pytorch\n",
        "!pip install -U facenet-pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xpU7YuaF_poJ",
      "metadata": {
        "id": "xpU7YuaF_poJ"
      },
      "source": [
        "CPU only (sem GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ztqCzXq-1ItH",
      "metadata": {
        "id": "ztqCzXq-1ItH"
      },
      "outputs": [],
      "source": [
        "#!pip uninstall -y torch torchvision torchaudio\n",
        "#!pip install -U --index-url https://download.pytorch.org/whl/cpu torch torchvision torchaudio\n",
        "#!pip install -U facenet-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ESO5PNCj0_gk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "08d5e8b536974168b23a469e3d749cd6",
            "55ad963f2b8a4ca28265d8a8aebdbe20",
            "4755416cf99e4fa396e0f6ea355845ab",
            "7f817e7ff41e4f9bab61540507b30cf3",
            "915fb19be01c4dcc8110992535ecdddb",
            "be4df0928ab247ee8573d6f75cd3ae07",
            "96a521bea3524fc8b3503710fb0f449a",
            "f5748b2fb6494e4d9abcd7067307649e",
            "3d40a722f79a46ba8a63635d626b81b7",
            "2ee413e79deb4989aba8db560234d590",
            "49f123251241473c98e17a740c29e0e8"
          ]
        },
        "id": "ESO5PNCj0_gk",
        "outputId": "896501a7-29a4-48b6-d80a-9f10826c2543"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch: 2.5.1+cu121 cuda? False\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08d5e8b536974168b23a469e3d749cd6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/107M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/facenet_pytorch/models/inception_resnet_v1.py:329: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(cached_file)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "facenet-pytorch OK\n"
          ]
        }
      ],
      "source": [
        "import torch, torchvision\n",
        "print(\"torch:\", torch.__version__, \"cuda?\", torch.cuda.is_available())\n",
        "import facenet_pytorch\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "_ = InceptionResnetV1(pretrained='vggface2').eval()\n",
        "print(\"facenet-pytorch OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44b21cff",
      "metadata": {
        "id": "44b21cff"
      },
      "source": [
        "\n",
        "### 15.2) Carregamento do Modelo & Utilit√°rios\n",
        "\n",
        "Usamos **InceptionResnetV1 (VGGFace2)** como extrator de embeddings e o **detector j√° existente** (`detect_faces`) deste notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0442aaf9",
      "metadata": {
        "id": "0442aaf9"
      },
      "outputs": [],
      "source": [
        "\n",
        "from typing import Dict, Tuple, Optional\n",
        "import numpy as np\n",
        "\n",
        "# Tentamos importar aqui; se falhar, o usu√°rio instala na c√©lula anterior\n",
        "try:\n",
        "    import torch\n",
        "    from facenet_pytorch import InceptionResnetV1\n",
        "except Exception as e:\n",
        "    print(\"N√£o foi poss√≠vel importar FaceNet. Execute a c√©lula de instala√ß√£o (15.1) e tente novamente. Erro:\", e)\n",
        "    torch = None\n",
        "    InceptionResnetV1 = None\n",
        "\n",
        "FACENET_DEVICE = \"cuda\" if (hasattr(torch, \"cuda\") and torch.cuda.is_available()) else \"cpu\"\n",
        "\n",
        "_FACENET_MODEL = None\n",
        "\n",
        "def get_facenet_model():\n",
        "    global _FACENET_MODEL\n",
        "    if _FACENET_MODEL is None:\n",
        "        if InceptionResnetV1 is None:\n",
        "            raise RuntimeError(\"Facenet n√£o est√° dispon√≠vel. Instale facenet-pytorch/torch e reexecute.\")\n",
        "        _FACENET_MODEL = InceptionResnetV1(pretrained='vggface2').eval().to(FACENET_DEVICE)\n",
        "    return _FACENET_MODEL\n",
        "\n",
        "def bgr_to_rgb(img_bgr):\n",
        "    return img_bgr[:, :, ::-1]\n",
        "\n",
        "def crop_and_resize_face(img_bgr, x, y, w, h, size=160):\n",
        "    # recorte com margem opcional\n",
        "    x0 = max(0, x); y0 = max(0, y)\n",
        "    x1 = min(img_bgr.shape[1], x + w)\n",
        "    y1 = min(img_bgr.shape[0], y + h)\n",
        "    face = img_bgr[y0:y1, x0:x1]\n",
        "    face = cv2.resize(face, (size, size), interpolation=cv2.INTER_LINEAR)\n",
        "    face_rgb = bgr_to_rgb(face).astype(np.float32) / 255.0\n",
        "    # normaliza como tensor [C,H,W]\n",
        "    face_rgb = np.transpose(face_rgb, (2, 0, 1))\n",
        "    return face_rgb\n",
        "\n",
        "def facenet_embedding_from_bgr_face(img_bgr, x, y, w, h):\n",
        "    model = get_facenet_model()\n",
        "    face_rgb_chw = crop_and_resize_face(img_bgr, x, y, w, h, size=160)\n",
        "    with torch.no_grad():\n",
        "        tens = torch.from_numpy(face_rgb_chw).unsqueeze(0).to(FACENET_DEVICE)  # [1,3,160,160]\n",
        "        emb = model(tens).cpu().numpy().reshape(-1)  # 512-dim\n",
        "    return emb\n",
        "\n",
        "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    if a.ndim > 1: a = a.ravel()\n",
        "    if b.ndim > 1: b = b.ravel()\n",
        "    denom = (np.linalg.norm(a) * np.linalg.norm(b)) + 1e-9\n",
        "    return float(np.dot(a, b) / denom)\n",
        "\n",
        "def l2_distance(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    return float(np.linalg.norm(a - b))\n",
        "\n",
        "def build_facenet_db(enroll_dir: str) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"Cria um banco simples com 1 embedding m√©dio por usu√°rio a partir das imagens de ENROLL_DIR.\n",
        "    Reaproveita a fun√ß√£o detect_faces() j√° definida no notebook.\n",
        "    \"\"\"\n",
        "    import os, glob\n",
        "    db = {}\n",
        "    for user in sorted(os.listdir(enroll_dir)):\n",
        "        user_dir = os.path.join(enroll_dir, user)\n",
        "        if not os.path.isdir(user_dir):\n",
        "            continue\n",
        "        embs = []\n",
        "        for p in glob.glob(os.path.join(user_dir, \"*\")):\n",
        "            try:\n",
        "                img = cv2.imread(p)\n",
        "                if img is None:\n",
        "                    continue\n",
        "                faces = detect_faces(img)  # usa o detector configurado (Haar/DNN)\n",
        "                if len(faces) == 0:\n",
        "                    continue\n",
        "                # pega a face com maior score (ou a primeira)\n",
        "                faces_sorted = sorted(faces, key=lambda f: f[-1] if len(f) == 5 else 0.0, reverse=True)\n",
        "                x, y, w, h = faces_sorted[0][:4]\n",
        "                emb = facenet_embedding_from_bgr_face(img, x, y, w, h)\n",
        "                embs.append(emb)\n",
        "            except Exception as _:\n",
        "                pass\n",
        "        if embs:\n",
        "            db[user] = np.mean(np.stack(embs, axis=0), axis=0)\n",
        "    return db\n",
        "\n",
        "def facenet_predict_user(embedding: np.ndarray, db: Dict[str, np.ndarray],\n",
        "                         metric: str = \"cosine\") -> Tuple[Optional[str], float]:\n",
        "    \"\"\"Retorna (user, score).\n",
        "    Para cosine: quanto MAIOR melhor (1.0 = id√™ntico).\n",
        "    Para L2: quanto MENOR melhor (0.0 = id√™ntico).\n",
        "    \"\"\"\n",
        "    if not db:\n",
        "        return None, 0.0\n",
        "    best_user = None\n",
        "    best_score = -1.0 if metric == \"cosine\" else 1e9\n",
        "    for user, ref in db.items():\n",
        "        if metric == \"cosine\":\n",
        "            s = cosine_similarity(embedding, ref)\n",
        "            if s > best_score:\n",
        "                best_score = s\n",
        "                best_user = user\n",
        "        else:\n",
        "            s = l2_distance(embedding, ref)\n",
        "            if s < best_score:\n",
        "                best_score = s\n",
        "                best_user = user\n",
        "    return best_user, float(best_score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01d9efbc",
      "metadata": {
        "id": "01d9efbc"
      },
      "source": [
        "\n",
        "### 15.3) Autentica√ß√£o com FaceNet (1:1 e 1:N)\n",
        "\n",
        "- **1:1**: confere se a face capturada corresponde ao `expected_user` usando similaridade **cosine** (padr√£o).  \n",
        "- **1:N**: identifica o usu√°rio mais pr√≥ximo no banco de embeddings.\n",
        "- Integra√ß√£o com **liveness** opcional, reutilizando a fun√ß√£o j√° existente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "04b3ab6a",
      "metadata": {
        "id": "04b3ab6a"
      },
      "outputs": [],
      "source": [
        "import time, os, statistics, collections\n",
        "import numpy as np, cv2\n",
        "\n",
        "def _check_prereqs():\n",
        "    missing = []\n",
        "    for name in [\"detect_faces\", \"facenet_embedding_from_bgr_face\", \"build_facenet_db\", \"ENROLL_DIR\", \"update_display_img\"]:\n",
        "        if name not in globals():\n",
        "            missing.append(name)\n",
        "    if missing:\n",
        "        raise RuntimeError(f\"As seguintes fun√ß√µes/vari√°veis precisam estar definidas antes: {missing}\")\n",
        "    _ = get_facenet_model()  # garante FaceNet carregado (gera mensagem clara caso falhe)\n",
        "\n",
        "def _draw_label(img, text, y=24, color=(0,255,0)):\n",
        "    cv2.putText(img, text, (10, y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2, cv2.LINE_AA)\n",
        "\n",
        "def authenticate_facenet_preview_runner(\n",
        "    mode=\"1vN\",                 # \"1v1\" ou \"1vN\"\n",
        "    expected_user=None,         # obrigat√≥rio em 1v1\n",
        "    metric=\"cosine\",            # \"cosine\" ou \"l2\"\n",
        "    cosine_accept=0.7,          # limiar p/ cosine\n",
        "    l2_accept=1.0,              # limiar p/ L2 (ajuste conforme dataset)\n",
        "    require_liveness=False,\n",
        "    max_frames=200,\n",
        "    width=640,\n",
        "    height=480,\n",
        "    show_raw=True,              # mostra o <video> ao vivo no Colab\n",
        "    log_every=5,                # log a cada N frames com face\n",
        "    max_evidences=3             # n¬∫ m√°x de imagens salvas\n",
        "):\n",
        "    _check_prereqs()\n",
        "\n",
        "    # Monta DB (1 embedding m√©dio por usu√°rio)\n",
        "    db = build_facenet_db(ENROLL_DIR)\n",
        "    if not db:\n",
        "        print(\"‚ö†Ô∏è Nenhum usu√°rio no ENROLL_DIR; fa√ßa 'novo' (enrollment) primeiro.\")\n",
        "        return {\"status\": \"fail\", \"reason\": \"empty_db\"}\n",
        "\n",
        "    print(f\"[FaceNet Runner] mode={mode} metric={metric} cos_accept={cosine_accept} l2_accept={l2_accept}\")\n",
        "    if mode == \"1v1\" and not expected_user:\n",
        "        print(\"‚ö†Ô∏è expected_user n√£o informado para 1v1\")\n",
        "        return {\"status\": \"fail\", \"reason\": \"missing_expected_user\"}\n",
        "\n",
        "    # Estado de liveness\n",
        "    prev_gray = None\n",
        "    energy_acc, frames_count = 0.0, 0\n",
        "\n",
        "    # Logs e evid√™ncias\n",
        "    pred_logs = []              # lista de (user, score)\n",
        "    evid_paths = []\n",
        "    users_counter = collections.Counter()\n",
        "\n",
        "    os.makedirs(EVIDENCE_DIR, exist_ok=True)\n",
        "\n",
        "    t0 = time.time()\n",
        "    ok_faces = 0\n",
        "    try:\n",
        "        for i in range(max_frames):\n",
        "            frame = capture_one(width=width, height=height, show_raw=show_raw)\n",
        "            if frame is None:\n",
        "                time.sleep(0.05)\n",
        "                continue\n",
        "\n",
        "            disp = frame.copy()\n",
        "\n",
        "            # Liveness (opcional)\n",
        "            if require_liveness:\n",
        "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "                if prev_gray is not None:\n",
        "                    diff = cv2.absdiff(gray, prev_gray)\n",
        "                    energy_acc += float(np.mean(diff))\n",
        "                prev_gray = gray\n",
        "                frames_count += 1\n",
        "\n",
        "            faces = detect_faces(frame)\n",
        "            if len(faces) > 0:\n",
        "                ok_faces += 1\n",
        "                faces_sorted = sorted(faces, key=lambda f: f[-1] if len(f)==5 else 0.0, reverse=True)\n",
        "                x, y, w, h = faces_sorted[0][:4]\n",
        "                cv2.rectangle(disp, (x,y), (x+w, y+h), (0,255,0), 2)\n",
        "\n",
        "                emb = facenet_embedding_from_bgr_face(frame, x, y, w, h)\n",
        "\n",
        "                if mode == \"1v1\":\n",
        "                    ref = db.get(expected_user)\n",
        "                    if ref is None:\n",
        "                        _draw_label(disp, f\"{expected_user} n√£o encontrado\", 24, (0,0,255))\n",
        "                    else:\n",
        "                        if metric == \"cosine\":\n",
        "                            score = cosine_similarity(emb, ref)\n",
        "                            verdict = (score >= cosine_accept)\n",
        "                            label = f\"{expected_user} | cos={score:.3f} | {'OK' if verdict else 'NEG'}\"\n",
        "                        else:\n",
        "                            score = l2_distance(emb, ref)\n",
        "                            verdict = (score <= l2_accept)\n",
        "                            label = f\"{expected_user} | l2={score:.3f} | {'OK' if verdict else 'NEG'}\"\n",
        "                        _draw_label(disp, label, 24, (0,255,0) if verdict else (0,0,255))\n",
        "                        pred_logs.append((expected_user, float(score)))\n",
        "                        users_counter[expected_user] += 1\n",
        "\n",
        "                        # salva evid√™ncias (1v1) ‚Äî primeiras N imagens\n",
        "                        if len(evid_paths) < max_evidences:\n",
        "                            ts = int(time.time()*1000)\n",
        "                            out_path = os.path.join(EVIDENCE_DIR, f\"facenet_1v1_{expected_user}_{score:.3f}_{ts}.jpg\")\n",
        "                            cv2.imwrite(out_path, disp)\n",
        "                            evid_paths.append(out_path)\n",
        "\n",
        "                else:\n",
        "                    # 1:N ‚Äî encontra o mais pr√≥ximo e loga\n",
        "                    if metric == \"cosine\":\n",
        "                        best_user = max(db.keys(), key=lambda u: cosine_similarity(emb, db[u]))\n",
        "                        best_score = cosine_similarity(emb, db[best_user])\n",
        "                        label = f\"{best_user} | cos={best_score:.3f}\"\n",
        "                    else:\n",
        "                        best_user = min(db.keys(), key=lambda u: l2_distance(emb, db[u]))\n",
        "                        best_score = l2_distance(emb, db[best_user])\n",
        "                        label = f\"{best_user} | l2={best_score:.3f}\"\n",
        "\n",
        "                    _draw_label(disp, label, 24, (255,255,255))\n",
        "\n",
        "                    pred_logs.append((best_user, float(best_score)))\n",
        "                    users_counter[best_user] += 1\n",
        "\n",
        "                    # salva evid√™ncias (1vN) ‚Äî primeiras N imagens\n",
        "                    if len(evid_paths) < max_evidences:\n",
        "                        ts = int(time.time()*1000)\n",
        "                        out_path = os.path.join(EVIDENCE_DIR, f\"facenet_1vN_{best_user}_{best_score:.3f}_{ts}.jpg\")\n",
        "                        cv2.imwrite(out_path, disp)\n",
        "                        evid_paths.append(out_path)\n",
        "\n",
        "                    # log a cada N frames com face\n",
        "                    if ok_faces % log_every == 0:\n",
        "                        print(f\"[pred] frame#{i:03d} -> user={best_user} score={best_score:.3f}\")\n",
        "\n",
        "            # Liveness status on-screen\n",
        "            if require_liveness and frames_count > 0:\n",
        "                avg_energy = energy_acc / max(1, frames_count)\n",
        "                live_ok = (avg_energy >= LIVENESS_MIN_ENERGY)\n",
        "                _draw_label(disp, f\"liveness={'OK' if live_ok else 'LOW'} ({avg_energy:.1f})\", 50, (255,255,0))\n",
        "\n",
        "            # atualiza a <img id=\"output\"> no Colab\n",
        "            update_display_img(disp)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"‚èπÔ∏è Interrompido pelo usu√°rio.\")\n",
        "    finally:\n",
        "        try:\n",
        "            stop_camera_ui()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    dt = time.time() - t0\n",
        "\n",
        "    # ===== Resumo final =====\n",
        "    mean_score = statistics.mean([s for (_, s) in pred_logs]) if pred_logs else 0.0\n",
        "    top_user = users_counter.most_common(1)[0][0] if users_counter else None\n",
        "\n",
        "    print(f\"[FaceNet Runner] terminado em {dt:.1f}s | frames_com_face={ok_faces}/{max_frames}\")\n",
        "    if mode == \"1vN\":\n",
        "        print(f\"[Resumo 1vN] top_user={top_user} | mean_score={mean_score:.3f} | n_preds={len(pred_logs)}\")\n",
        "    else:\n",
        "        print(f\"[Resumo 1v1] user={expected_user} | mean_score={mean_score:.3f} | n_preds={len(pred_logs)}\")\n",
        "    if evid_paths:\n",
        "        print(\"Evid√™ncias salvas:\")\n",
        "        for p in evid_paths:\n",
        "            print(\" -\", p)\n",
        "\n",
        "    return {\n",
        "        \"status\": \"ok\",\n",
        "        \"mode\": mode,\n",
        "        \"metric\": metric,\n",
        "        \"faces_seen\": ok_faces,\n",
        "        \"elapsed_sec\": dt,\n",
        "        \"mean_score\": mean_score,\n",
        "        \"top_user\": top_user,\n",
        "        \"pred_counts\": dict(users_counter),\n",
        "        \"evidences\": evid_paths,\n",
        "        \"n_preds\": len(pred_logs),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "974cf8bb",
      "metadata": {
        "id": "974cf8bb"
      },
      "source": [
        "\n",
        "### 15.4) Runner (FaceNet) ‚Äî Exemplo de Uso\n",
        "\n",
        "Use este runner independente para testar **FaceNet** sem alterar o runner original do notebook.  \n",
        "- Para **1:1**: fa√ßa `novo` (enrollment) pelo runner original, depois execute abaixo com `expected_user`.\n",
        "- Para **1:N**: basta ter **dois ou mais** diret√≥rios em `ENROLL_DIR`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "00f86858",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "00f86858",
        "outputId": "3857f8f5-753e-4bf6-a0f8-a046621ab85d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[FaceNet Runner] mode=1v1 metric=cosine cos_accept=0.7 l2_accept=1.0\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pr√©-visualiza√ß√£o (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚èπÔ∏è Interrompido pelo usu√°rio.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "if (window.stopColabCamera) window.stopColabCamera();",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[FaceNet Runner] terminado em 19.6s | frames_com_face=20/200\n",
            "[Resumo 1v1] user=teste | mean_score=0.762 | n_preds=20\n",
            "Evid√™ncias salvas:\n",
            " - cv_colab_data/evidence/facenet_1v1_teste_0.774_1759493393125.jpg\n",
            " - cv_colab_data/evidence/facenet_1v1_teste_0.784_1759493393941.jpg\n",
            " - cv_colab_data/evidence/facenet_1v1_teste_0.757_1759493394752.jpg\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'status': 'ok',\n",
              " 'mode': '1v1',\n",
              " 'metric': 'cosine',\n",
              " 'faces_seen': 20,\n",
              " 'elapsed_sec': 19.62465262413025,\n",
              " 'mean_score': 0.7624066815190306,\n",
              " 'top_user': 'teste',\n",
              " 'pred_counts': {'teste': 20},\n",
              " 'evidences': ['cv_colab_data/evidence/facenet_1v1_teste_0.774_1759493393125.jpg',\n",
              "  'cv_colab_data/evidence/facenet_1v1_teste_0.784_1759493393941.jpg',\n",
              "  'cv_colab_data/evidence/facenet_1v1_teste_0.757_1759493394752.jpg'],\n",
              " 'n_preds': 20}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1:N ‚Äî identifica entre os usu√°rios cadastrados em ENROLL_DIR\n",
        "\"\"\"\n",
        "authenticate_facenet_preview_runner(\n",
        "    mode=\"1vN\",\n",
        "    metric=\"cosine\",\n",
        "    cosine_accept=0.7,     # ajuste 0.6~0.8 conforme seu caso\n",
        "    require_liveness=False,\n",
        "    max_frames=200,\n",
        "    width=640, height=480,\n",
        "    show_raw=True          # mostra o <video> \"ao vivo\" no Colab\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "# 1:1 ‚Äî compara contra um usu√°rio espec√≠fico\n",
        "authenticate_facenet_preview_runner(\n",
        "    mode=\"1v1\",\n",
        "    expected_user=\"teste\",\n",
        "    metric=\"cosine\",\n",
        "    cosine_accept=0.7,\n",
        "    require_liveness=True,\n",
        "    max_frames=200\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26e1cf2f",
      "metadata": {
        "id": "26e1cf2f"
      },
      "source": [
        "\n",
        "> **Notas**  \n",
        "> ‚Ä¢ Limiar t√≠pico para **cosine**: `0.6‚Äì0.8` (quanto maior, mais estrito). Ajuste conforme seu dataset.  \n",
        "> ‚Ä¢ O banco usa o **m√©dio dos embeddings** por usu√°rio (robusto a varia√ß√µes).  \n",
        "> ‚Ä¢ A detec√ß√£o de faces vem da **c√©lula 5**, ent√£o o desempenho depende de `DETECTION_MODEL` (Haar/DNN).  \n",
        "> ‚Ä¢ Em cen√°rios cr√≠ticos, considere normalizar ilumina√ß√£o e alinhar o rosto antes do embedding.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "08d5e8b536974168b23a469e3d749cd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_55ad963f2b8a4ca28265d8a8aebdbe20",
              "IPY_MODEL_4755416cf99e4fa396e0f6ea355845ab",
              "IPY_MODEL_7f817e7ff41e4f9bab61540507b30cf3"
            ],
            "layout": "IPY_MODEL_915fb19be01c4dcc8110992535ecdddb"
          }
        },
        "2ee413e79deb4989aba8db560234d590": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d40a722f79a46ba8a63635d626b81b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4755416cf99e4fa396e0f6ea355845ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5748b2fb6494e4d9abcd7067307649e",
            "max": 111898327,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3d40a722f79a46ba8a63635d626b81b7",
            "value": 111898327
          }
        },
        "49f123251241473c98e17a740c29e0e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55ad963f2b8a4ca28265d8a8aebdbe20": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be4df0928ab247ee8573d6f75cd3ae07",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_96a521bea3524fc8b3503710fb0f449a",
            "value": "100%"
          }
        },
        "7f817e7ff41e4f9bab61540507b30cf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ee413e79deb4989aba8db560234d590",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_49f123251241473c98e17a740c29e0e8",
            "value": "‚Äá107M/107M‚Äá[00:00&lt;00:00,‚Äá174MB/s]"
          }
        },
        "915fb19be01c4dcc8110992535ecdddb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96a521bea3524fc8b3503710fb0f449a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be4df0928ab247ee8573d6f75cd3ae07": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5748b2fb6494e4d9abcd7067307649e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
