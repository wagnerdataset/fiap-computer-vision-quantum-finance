{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4c4ae182",
      "metadata": {
        "id": "4c4ae182"
      },
      "source": [
        "\n",
        "# Verificação Facial — Colab/Local (Webcam + Preview)  \n",
        "- **Detecta ambiente Colab/Local**:  \n",
        "  - `IN_COLAB` (True/False)\n",
        "- **Detectores de face**:  \n",
        "  - `haar` (OpenCV CascadeClassifier)  \n",
        "  - `dnn_ssd_resnet10` (OpenCV DNN com Caffe)\n",
        "- **Runner interativo**:\n",
        "  - **Inclusão (novo)**: coleta amostras de um novo usuário e treina LBPH  \n",
        "  - **Autenticação (auth)**: modos **1:1** (usuário esperado) e **1:N** (identificação)\n",
        "  - **Liveness**: checagem simples de energia do sinal\n",
        "- **Avaliação offline** renovada:\n",
        "  - Suporta **.zip / .tar / .tar.gz / .tgz** e **extração recursiva** (ex.: ZIP contendo TAR.GZ).\n",
        "  - **Remoto default** (POSITIVES): **Caltech Face 1999** (`faces.tar`, CaltechDATA).\n",
        "  - **Fallback NEGATIVES**: **Caltech-101** (extrai `BACKGROUND_Google`; se ausente, usa outras categorias ≠ “face”).\n",
        "  - **Câmera também em Remoto/Default** quando `negatives/` estiver vazio (opção interativa).\n",
        "  - Funciona com **single-class**: converte todas as imagens extraídas em `positives/` e completa `negatives/` automaticamente.\n",
        "- **Métricas e gráficos**: matriz de confusão, accuracy, precision, recall, F1, tempo médio; tabela comparativa (se `pandas` disponível).\n",
        "- **Dicas de tuning**: `conf_threshold` para DNN, `minNeighbors/scaleFactor/minSize` para Haar, filtros pós-detecção (razão w/h e tamanho).\n",
        "- **Downloads automáticos**:\n",
        "  - `deploy.prototxt` e `res10_300x300_ssd_iter_140000.caffemodel` (DNN SSD-ResNet10)\n",
        "  - Dataset remoto padrão (CBCL/MIT) reorganizado em `positives/` e `negatives/`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c0088b9",
      "metadata": {
        "id": "5c0088b9"
      },
      "source": [
        "## 1) Instalação (reinicie o runtime após rodar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d0a9722c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0a9722c",
        "outputId": "2e8fb52d-3778-4448-de00-2cb35b52cb66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python==4.8.1.78 in /usr/local/lib/python3.12/dist-packages (4.8.1.78)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "OpenCV: 4.8.1\n",
            "NumPy: 1.26.4\n",
            "cv2.face OK\n"
          ]
        }
      ],
      "source": [
        "# Recomendado: após executar, vá em Runtime > Restart runtime\n",
        "#!pip uninstall -y opencv-python opencv-contrib-python numpy\n",
        "!pip install --no-cache-dir numpy==1.26.4 opencv-contrib-python==4.8.1.78 matplotlib\n",
        "\n",
        "import cv2, numpy as np\n",
        "print(\"OpenCV:\", cv2.__version__)\n",
        "print(\"NumPy:\", np.__version__)\n",
        "try:\n",
        "    _ = cv2.face.LBPHFaceRecognizer_create()\n",
        "    print(\"cv2.face OK\")\n",
        "except Exception as e:\n",
        "    print(\"Falha cv2.face:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c0cfa7d",
      "metadata": {
        "id": "0c0cfa7d"
      },
      "source": [
        "## 2) Imports, diretórios e parâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "207dadc2",
      "metadata": {
        "id": "207dadc2"
      },
      "outputs": [],
      "source": [
        "import base64, json, time, uuid, math\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Javascript, display\n",
        "\n",
        "# Estrutura de pastas\n",
        "DATA_DIR = Path(\"cv_colab_data\")\n",
        "ENROLL_DIR = DATA_DIR/\"enroll\"\n",
        "EVIDENCE_DIR = DATA_DIR/\"evidence\"\n",
        "for d in (DATA_DIR, ENROLL_DIR, EVIDENCE_DIR):\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Parâmetros globais (ajuste conforme sua calibração/ambiente)\n",
        "SERVICE_THRESHOLD = 55.0       # será ajustado automaticamente na calibração\n",
        "LIVENESS_MIN_ENERGY = 8.0      # energia mínima média para ser \"live\"\n",
        "\n",
        "def show_bgr(img, title=\"preview\"):\n",
        "    rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    plt.figure(figsize=(5,4))\n",
        "    plt.imshow(rgb)\n",
        "    plt.title(title)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1196b8d6",
      "metadata": {
        "id": "1196b8d6"
      },
      "source": [
        "## 3) Pré-processamento (CLAHE + blur)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "799dc6d4",
      "metadata": {
        "id": "799dc6d4"
      },
      "outputs": [],
      "source": [
        "def preprocess_face_gray(gray_200x200):\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    g = clahe.apply(gray_200x200)\n",
        "    g = cv2.GaussianBlur(g, (3,3), 0)\n",
        "    return g"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "babfdb10",
      "metadata": {
        "id": "babfdb10"
      },
      "source": [
        "## 4) Webcam persistente + Preview - Verificação Facial Dual (Colab ↔ Local)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "21d7e319",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21d7e319",
        "outputId": "6c4ff5bd-8e88-4989-b9af-dbe6b631f9c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IN_COLAB = True\n"
          ]
        }
      ],
      "source": [
        "import time, base64, uuid\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Detecta ambiente\n",
        "IN_COLAB = True\n",
        "try:\n",
        "    from google.colab import output  # só existe no Colab\n",
        "    from IPython.display import Javascript, display\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "print(\"IN_COLAB =\", IN_COLAB)\n",
        "\n",
        "# ---------- (opcional) janela processada no Colab ----------\n",
        "def update_display_img(frame_bgr):\n",
        "    \"\"\"\n",
        "    Atualiza a <img id='output'> no Colab. Em ambiente local, essa função não faz nada\n",
        "    (use cv2.imshow no seu fluxo local).\n",
        "    \"\"\"\n",
        "    if not IN_COLAB:\n",
        "        return\n",
        "    ok, buffer = cv2.imencode('.jpg', frame_bgr)\n",
        "    if not ok:\n",
        "        return\n",
        "    b64 = base64.b64encode(buffer).decode('utf-8')\n",
        "    output.eval_js(f\"window.updateProcessed && window.updateProcessed('{b64}')\")\n",
        "\n",
        "# ---------- Colab: stream persistente via JS ----------\n",
        "if IN_COLAB:\n",
        "    def _ensure_camera_ready(show_raw=False, width=640, height=480):\n",
        "        js = f\"\"\"\n",
        "        (async () => {{\n",
        "          let container = document.getElementById('camera-container');\n",
        "          if (!container) {{\n",
        "            container = document.createElement('div');\n",
        "            container.id = 'camera-container';\n",
        "            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n",
        "            document.body.appendChild(container);\n",
        "            container.innerHTML = `\n",
        "              <div id=\"cam-left\" style=\"display:{'flex' if show_raw else 'none'}; flex-direction:column; align-items:center; gap:6px;\">\n",
        "                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:{width}px; border:1px solid #ddd; border-radius:8px;\"></video>\n",
        "                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n",
        "              </div>\n",
        "              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n",
        "                <img id=\"output\" style=\"max-width:{width}px; border:1px solid #ddd; border-radius:8px;\">\n",
        "                <small style=\"color:#555\">Frame processado</small>\n",
        "              </div>\n",
        "              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n",
        "            `;\n",
        "          }} else {{\n",
        "            const left = document.getElementById('cam-left');\n",
        "            if (left) left.style.display = { '\"flex\"' if show_raw else '\"none\"' };\n",
        "          }}\n",
        "\n",
        "          if (!window._colabStream || window._colabStreamInactive) {{\n",
        "            try {{\n",
        "              window._colabStream = await navigator.mediaDevices.getUserMedia({{ video: {{width:{width}, height:{height}}}, audio:false }});\n",
        "              window._colabStreamInactive = false;\n",
        "            }} catch (e) {{\n",
        "              console.error('getUserMedia failed', e);\n",
        "              return false;\n",
        "            }}\n",
        "          }}\n",
        "          const video = document.getElementById('webcam');\n",
        "          if (video && video.srcObject !== window._colabStream) {{\n",
        "            video.srcObject = window._colabStream;\n",
        "          }}\n",
        "\n",
        "          const canvas = document.getElementById('canvas');\n",
        "          window.captureFrame = () => {{\n",
        "            const ctx = canvas.getContext('2d');\n",
        "            const vw = (video && video.videoWidth) ? video.videoWidth : {width};\n",
        "            const vh = (video && video.videoHeight) ? video.videoHeight : {height};\n",
        "            canvas.width = vw; canvas.height = vh;\n",
        "            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n",
        "            return canvas.toDataURL('image/jpeg', 0.9);\n",
        "          }};\n",
        "          window.updateProcessed = (b64) => {{\n",
        "            const img = document.getElementById('output');\n",
        "            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n",
        "          }};\n",
        "          window.stopColabCamera = () => {{\n",
        "            try {{\n",
        "              if (window._colabStream && !window._colabStreamInactive) {{\n",
        "                window._colabStream.getTracks().forEach(t => t.stop());\n",
        "                window._colabStreamInactive = true;\n",
        "              }}\n",
        "            }} catch (e) {{ console.warn(e); }}\n",
        "            const c = document.getElementById('camera-container');\n",
        "            if (c) c.remove();\n",
        "          }};\n",
        "          return true;\n",
        "        }} )();\n",
        "        \"\"\"\n",
        "        display(Javascript(js))\n",
        "        time.sleep(0.2)\n",
        "\n",
        "    def _b64_to_image(data_url_or_b64):\n",
        "        if data_url_or_b64 is None:\n",
        "            return None\n",
        "        s = data_url_or_b64\n",
        "        if isinstance(s, bytes):\n",
        "            s = s.decode(\"utf-8\")\n",
        "        if s.startswith(\"data:image\"):\n",
        "            s = s.split(\",\")[1]\n",
        "        arr = np.frombuffer(base64.b64decode(s), dtype=np.uint8)\n",
        "        return cv2.imdecode(arr, cv2.IMREAD_COLOR)\n",
        "\n",
        "    def capture_one(width=640, height=480, show_raw=False):\n",
        "        _ensure_camera_ready(show_raw=show_raw, width=width, height=height)\n",
        "        data_url = output.eval_js(\"window.captureFrame ? window.captureFrame() : null\")\n",
        "        return _b64_to_image(data_url)\n",
        "\n",
        "    def capture_sequence(n=24, delay_ms=80, width=640, height=480, show_raw=False, preview_callback=None):\n",
        "        _ensure_camera_ready(show_raw=show_raw, width=width, height=height)\n",
        "        frames = []\n",
        "        for i in range(n):\n",
        "            data_url = output.eval_js(\"window.captureFrame ? window.captureFrame() : null\")\n",
        "            img = _b64_to_image(data_url)\n",
        "            if img is not None:\n",
        "                frames.append(img)\n",
        "                if callable(preview_callback):\n",
        "                    preview_callback(img, i)\n",
        "            time.sleep(max(0, delay_ms/1000.0))\n",
        "        return frames\n",
        "\n",
        "    def stop_camera_ui():\n",
        "        display(Javascript(\"if (window.stopColabCamera) window.stopColabCamera();\"))\n",
        "        time.sleep(0.2)\n",
        "\n",
        "# ---------- Local: OpenCV VideoCapture ----------\n",
        "else:\n",
        "    def capture_one(cam_index=0, width=640, height=480, show_raw=False):\n",
        "        cap = cv2.VideoCapture(cam_index)\n",
        "        cap.set(cv2.CAP_PROP_FRAME_WIDTH,  width)\n",
        "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
        "        ret, frame = cap.read()\n",
        "        cap.release()\n",
        "        if not ret:\n",
        "            raise RuntimeError(\"Não conseguiu capturar frame da webcam local\")\n",
        "        return frame\n",
        "\n",
        "    def capture_sequence(n=24, delay_ms=80, cam_index=0, width=640, height=480, show_raw=False, preview_callback=None):\n",
        "        cap = cv2.VideoCapture(cam_index)\n",
        "        cap.set(cv2.CAP_PROP_FRAME_WIDTH,  width)\n",
        "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
        "        frames = []\n",
        "        for i in range(n):\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                continue\n",
        "            frames.append(frame)\n",
        "            if callable(preview_callback):\n",
        "                preview_callback(frame, i)\n",
        "            # preview simples local\n",
        "            cv2.imshow(\"preview\", frame)\n",
        "            if cv2.waitKey(delay_ms) & 0xFF == ord(\"q\"):\n",
        "                break\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()\n",
        "        return frames\n",
        "\n",
        "    def stop_camera_ui():\n",
        "        # no local, apenas fecha janelas\n",
        "        cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a56dd9cb",
      "metadata": {
        "id": "a56dd9cb"
      },
      "source": [
        "## 5) Detecção de faces (Haar/DNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "23539590",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23539590",
        "outputId": "3a80ea9a-cf51-411f-9a94-9d2738753c63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Detecção] Modelo atual: haar | MODELS_DIR=cv_colab_data/models\n"
          ]
        }
      ],
      "source": [
        "# ============================\n",
        "# 5) Detecção de faces (Haar/DNN)\n",
        "# ============================\n",
        "#\n",
        "# Suporta dois detectores:\n",
        "#   - \"haar\": Haar Cascade (cv2.CascadeClassifier)\n",
        "#   - \"dnn_ssd_resnet10\": DNN (SSD ResNet10) via OpenCV DNN (Caffe)\n",
        "#\n",
        "# Escolha temporária (até o Runner setar automaticamente):\n",
        "#   - Ajuste DETECTION_MODEL = \"haar\" | \"dnn_ssd_resnet10\"\n",
        "#   - ou exporte a env: DETECTION_MODEL=haar | dnn_ssd_resnet10\n",
        "#\n",
        "# Uso: faces = detect_faces(img_bgr, conf_threshold=0.5)\n",
        "# Retorno: lista de (x, y, w, h, score)\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# ======================\n",
        "# Configuração do modelo\n",
        "# ======================\n",
        "DETECTION_MODEL = os.environ.get(\"DETECTION_MODEL\", \"haar\").strip().lower()\n",
        "\n",
        "# Honra o DATA_DIR definido em outra célula; se não existir, cria um default\n",
        "try:\n",
        "    DATA_DIR  # definido na sua célula de diretórios\n",
        "except NameError:\n",
        "    DATA_DIR = Path(\"cv_colab_data\")\n",
        "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "MODELS_DIR = DATA_DIR / \"models\"\n",
        "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Haar: caminho padrão do OpenCV (com fallback)\n",
        "try:\n",
        "    import cv2.data as cvd\n",
        "    HAAR_PATH = str(Path(cvd.haarcascades) / \"haarcascade_frontalface_default.xml\")\n",
        "except Exception:\n",
        "    # fallback: se quiser manter tudo em DATA_DIR/models, pode copiar o xml pra lá\n",
        "    HAAR_PATH = str(MODELS_DIR / \"haarcascade_frontalface_default.xml\")\n",
        "\n",
        "# DNN (SSD ResNet10): caminhos dentro de DATA_DIR/models\n",
        "DNN_PROTO_PATH   = MODELS_DIR / \"deploy.prototxt\"\n",
        "# usamos o modelo Caffe \"não-fp16\", disponível publicamente:\n",
        "DNN_WEIGHTS_PATH = MODELS_DIR / \"res10_300x300_ssd_iter_140000.caffemodel\"\n",
        "\n",
        "# URLs oficiais/alternativas\n",
        "DNN_PROTO_URL   = \"https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt\"\n",
        "DNN_WEIGHTS_URL = \"https://github.com/opencv/opencv_3rdparty/raw/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel\"\n",
        "\n",
        "# --------------\n",
        "# Inicialização\n",
        "# --------------\n",
        "_haar_cascade = None\n",
        "_dnn_net = None\n",
        "\n",
        "def _download_file(url: str, dest: Path) -> bool:\n",
        "    \"\"\"\n",
        "    Baixa com urllib; se falhar e houver 'wget', tenta wget.\n",
        "    Retorna True se o arquivo existir ao final.\n",
        "    \"\"\"\n",
        "    dest.parent.mkdir(parents=True, exist_ok=True)\n",
        "    try:\n",
        "        import urllib.request\n",
        "        print(f\"Baixando {dest.name} …\")\n",
        "        urllib.request.urlretrieve(url, str(dest))\n",
        "        return dest.exists()\n",
        "    except Exception as e:\n",
        "        print(f\"Aviso: urllib falhou ({e}). Tentando wget (se disponível)…\")\n",
        "        try:\n",
        "            code = os.system(f\"wget -q -O {dest} {url}\")\n",
        "            return dest.exists() and code == 0\n",
        "        except Exception as e2:\n",
        "            print(f\"Aviso: wget também falhou ({e2}).\")\n",
        "            return dest.exists()\n",
        "\n",
        "def _download_if_missing():\n",
        "    ok = True\n",
        "    if not Path(DNN_PROTO_PATH).exists():\n",
        "        ok = _download_file(DNN_PROTO_URL, DNN_PROTO_PATH) and ok\n",
        "    if not Path(DNN_WEIGHTS_PATH).exists():\n",
        "        ok = _download_file(DNN_WEIGHTS_URL, DNN_WEIGHTS_PATH) and ok\n",
        "    if not ok:\n",
        "        print(\n",
        "            \"[DNN] Não foi possível garantir todos os arquivos.\\n\"\n",
        "            f\"  Prototxt: {DNN_PROTO_PATH}\\n\"\n",
        "            f\"  Pesos   : {DNN_WEIGHTS_PATH}\\n\"\n",
        "            \"  → Baixe manualmente ou defina DNN_PROTO_PATH / DNN_WEIGHTS_PATH.\"\n",
        "        )\n",
        "\n",
        "def _init_haar():\n",
        "    global _haar_cascade\n",
        "    if _haar_cascade is None:\n",
        "        if not os.path.exists(HAAR_PATH):\n",
        "            raise FileNotFoundError(f\"Haar cascade não encontrado em: {HAAR_PATH}\")\n",
        "        _haar_cascade = cv2.CascadeClassifier(HAAR_PATH)\n",
        "\n",
        "def _init_dnn():\n",
        "    _download_if_missing()\n",
        "    global _dnn_net\n",
        "    if _dnn_net is None:\n",
        "        if not (Path(DNN_PROTO_PATH).exists() and Path(DNN_WEIGHTS_PATH).exists()):\n",
        "            raise FileNotFoundError(\n",
        "                \"Arquivos do DNN não encontrados.\\n\"\n",
        "                f\"  Prototxt: {DNN_PROTO_PATH}\\n\"\n",
        "                f\"  Pesos   : {DNN_WEIGHTS_PATH}\\n\"\n",
        "                \"Ajuste DNN_PROTO_PATH / DNN_WEIGHTS_PATH ou salve os arquivos em DATA_DIR/'models'.\"\n",
        "            )\n",
        "        _dnn_net = cv2.dnn.readNetFromCaffe(str(DNN_PROTO_PATH), str(DNN_WEIGHTS_PATH))\n",
        "\n",
        "# --------------------\n",
        "# Função de detecção\n",
        "# --------------------\n",
        "def detect_faces(image_bgr, conf_threshold: float = 0.5):\n",
        "    \"\"\"\n",
        "    Retorna lista de detecções: (x, y, w, h, score)\n",
        "    \"\"\"\n",
        "    model = DETECTION_MODEL\n",
        "    if model == \"haar\":\n",
        "        _init_haar()\n",
        "        gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)\n",
        "        rects = _haar_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "        return [(int(x), int(y), int(w), int(h), 1.0) for (x, y, w, h) in rects]\n",
        "\n",
        "    elif model in (\"dnn\", \"dnn_ssd_resnet10\", \"ssd\", \"resnet10\"):\n",
        "        _init_dnn()\n",
        "        (h, w) = image_bgr.shape[:2]\n",
        "        blob = cv2.dnn.blobFromImage(\n",
        "            cv2.resize(image_bgr, (300, 300)), 1.0, (300, 300),\n",
        "            (104.0, 177.0, 123.0)\n",
        "        )\n",
        "        _dnn_net.setInput(blob)\n",
        "        detections = _dnn_net.forward()\n",
        "        boxes = []\n",
        "        for i in range(detections.shape[2]):\n",
        "            confidence = float(detections[0, 0, i, 2])\n",
        "            if confidence >= conf_threshold:\n",
        "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "                x, y = max(0, startX), max(0, startY)\n",
        "                ww, hh = max(0, endX - startX), max(0, endY - startY)\n",
        "                boxes.append((x, y, ww, hh, confidence))\n",
        "        return boxes\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Modelo '{model}' não suportado. Use 'haar' ou 'dnn_ssd_resnet10'.\")\n",
        "\n",
        "# --------------------\n",
        "# Helper de visualização\n",
        "# --------------------\n",
        "def draw_faces(image_bgr, faces, color=(0,255,0), thickness=2):\n",
        "    out = image_bgr.copy()\n",
        "    for (x,y,w,h,score) in faces:\n",
        "        cv2.rectangle(out, (x,y), (x+w, y+h), color, thickness)\n",
        "        cv2.putText(out, f\"{score:.2f}\", (x, max(0,y-5)), cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 1, cv2.LINE_AA)\n",
        "    return out\n",
        "\n",
        "print(f\"[Detecção] Modelo atual: {DETECTION_MODEL} | MODELS_DIR={MODELS_DIR}\")\n",
        "\n",
        "# Teste rápido (opcional)\n",
        "# img = capture_one(show_raw=True)\n",
        "# faces = detect_faces(img)\n",
        "# vis = draw_faces(img, faces)\n",
        "# show_bgr(vis, \"faces detectadas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5b79479",
      "metadata": {
        "id": "c5b79479"
      },
      "source": [
        "## 6) Config de preview e evidências"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "af35c8bc",
      "metadata": {
        "id": "af35c8bc"
      },
      "outputs": [],
      "source": [
        "# Cores das caixas (BGR)\n",
        "PREVIEW_BOX_COLOR = (0, 255, 0)     # verde\n",
        "CALIB_BOX_COLOR   = (255, 165, 0)   # laranja\n",
        "FINAL_BOX_COLOR   = (0, 255, 255)   # amarelo\n",
        "SAVE_EVIDENCE     = True\n",
        "\n",
        "def draw_box(img_bgr, bbox, color, thickness=2, label=None):\n",
        "    x,y,w,h = bbox\n",
        "    cv2.rectangle(img_bgr, (x,y), (x+w, y+h), color, thickness)\n",
        "    if label:\n",
        "        cv2.putText(img_bgr, label, (x, max(0, y-8)), cv2.FONT_HERSHEY_SIMPLEX, 0.55, color, 2, cv2.LINE_AA)\n",
        "\n",
        "def maybe_save_evidence(result: dict, vis_bgr, prefix=\"auth\"):\n",
        "    if not SAVE_EVIDENCE:\n",
        "        return\n",
        "    status = result.get(\"status\", \"\")\n",
        "    if status in (\"error\", \"route_review\"):\n",
        "        ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "        name = f\"{prefix}_{status}_{ts}_{uuid.uuid4().hex[:6]}.jpg\"\n",
        "        cv2.imwrite(str(EVIDENCE_DIR / name), vis_bgr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5ae218f",
      "metadata": {
        "id": "d5ae218f"
      },
      "source": [
        "## 7) Enrollment com preview (função)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "bf25ffcf",
      "metadata": {
        "id": "bf25ffcf"
      },
      "outputs": [],
      "source": [
        "def enroll_user_with_preview(user_id=\"novo_usuario_preview\", n_samples=30, interval_ms=100):\n",
        "    user_dir = ENROLL_DIR / user_id\n",
        "    user_dir.mkdir(parents=True, exist_ok=True)\n",
        "    setup = capture_one(show_raw=True)  # inicializa UI (mostra preview bruto)\n",
        "\n",
        "    saved = 0\n",
        "    attempts = 0\n",
        "    print(f\"Coletando {n_samples} amostras para '{user_id}'… Olhe para a câmera.\")\n",
        "    while saved < n_samples and attempts < n_samples*3:\n",
        "        attempts += 1\n",
        "        fr = capture_one(show_raw=True)\n",
        "        if fr is None:\n",
        "            time.sleep(0.05);\n",
        "            continue\n",
        "        faces = detect_faces(fr)\n",
        "        vis = fr.copy()\n",
        "        if len(faces):\n",
        "            (x,y,w,h,score) = max(faces, key=lambda f: f[2]*f[3]) # Unpack 5 values\n",
        "            draw_box(vis, (x,y,w,h), PREVIEW_BOX_COLOR, label=\"enroll\")\n",
        "            face = fr[y:y+h, x:x+w]\n",
        "            g = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
        "            g = cv2.resize(g, (200,200))\n",
        "            g = preprocess_face_gray(g)\n",
        "            cv2.imwrite(str(user_dir/f\"{user_id}_{uuid.uuid4().hex[:6]}.jpg\"), g)\n",
        "            saved += 1\n",
        "        update_display_img(vis)\n",
        "        time.sleep(max(0, interval_ms/1000.0))\n",
        "    print(f\"✅ Enrollment concluído: {saved}/{n_samples} amostras salvas.\")\n",
        "    if saved < max(10, int(0.5*n_samples)):\n",
        "        print(\"⚠️ Poucas amostras úteis. Considere refazer com melhor enquadramento/iluminação.\")\n",
        "    # não fecha a UI aqui para reaproveitar no próximo passo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0f00c3b",
      "metadata": {
        "id": "a0f00c3b"
      },
      "source": [
        "## 8) Modelo LBPH em memória (cache global)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "be1e62e6",
      "metadata": {
        "id": "be1e62e6"
      },
      "outputs": [],
      "source": [
        "REC_CACHE = None\n",
        "LABEL_MAP_CACHE = None\n",
        "INV_LABEL_CACHE = None\n",
        "\n",
        "def _load_images_and_labels():\n",
        "    images, labels = [], []\n",
        "    label_map = {}\n",
        "    next_label = 0\n",
        "    for ud in sorted(ENROLL_DIR.glob(\"*\")):\n",
        "        if not ud.is_dir():\n",
        "            continue\n",
        "        uid = ud.name\n",
        "        label_map[uid] = next_label\n",
        "        for p in ud.glob(\"*.jpg\"):\n",
        "            g = cv2.imread(str(p), cv2.IMREAD_GRAYSCALE)\n",
        "            if g is None:\n",
        "                continue\n",
        "            g = cv2.resize(g, (200,200))\n",
        "            g = preprocess_face_gray(g)\n",
        "            images.append(g); labels.append(next_label)\n",
        "        next_label += 1\n",
        "    return images, np.array(labels), label_map\n",
        "\n",
        "def train_lbph_in_memory(neighbors=16):\n",
        "    global REC_CACHE, LABEL_MAP_CACHE, INV_LABEL_CACHE\n",
        "    images, labels, label_map = _load_images_and_labels()\n",
        "    if len(images) == 0:\n",
        "        raise RuntimeError(\"Sem amostras. Faça o enrollment antes.\")\n",
        "    rec = cv2.face.LBPHFaceRecognizer_create(radius=2, neighbors=neighbors, grid_x=8, grid_y=8)\n",
        "    rec.train(images, labels)\n",
        "    REC_CACHE = rec\n",
        "    LABEL_MAP_CACHE = label_map\n",
        "    INV_LABEL_CACHE = {v:k for k,v in label_map.items()}\n",
        "    print(f\"Modelo LBPH treinado em memória. Usuários: {list(label_map.keys())}\")\n",
        "    return rec\n",
        "\n",
        "def get_recognizer(neighbors=16, force_retrain=False):\n",
        "    global REC_CACHE, LABEL_MAP_CACHE, INV_LABEL_CACHE\n",
        "    if force_retrain or REC_CACHE is None or LABEL_MAP_CACHE is None or INV_LABEL_CACHE is None:\n",
        "        print(\"↻ Treinando LBPH (memória)…\")\n",
        "        return train_lbph_in_memory(neighbors=neighbors)\n",
        "    return REC_CACHE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de3a1cce",
      "metadata": {
        "id": "de3a1cce"
      },
      "source": [
        "## 9) Calibração automática do limiar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6a2c5e8c",
      "metadata": {
        "id": "6a2c5e8c"
      },
      "outputs": [],
      "source": [
        "def _detect_face_gray200(img_bgr):\n",
        "    faces = detect_faces(img_bgr)\n",
        "    if len(faces)==0:\n",
        "        return None, None\n",
        "    (x,y,w,h,score) = max(faces, key=lambda f: f[2]*f[3]) # Unpack 5 values including score\n",
        "    face = img_bgr[y:y+h, x:x+w]\n",
        "    g = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
        "    g = cv2.resize(g, (200,200))\n",
        "    g = preprocess_face_gray(g)\n",
        "    return g, (x,y,w,h)\n",
        "\n",
        "def calibrate_threshold(samples=15, neighbors=16):\n",
        "    global SERVICE_THRESHOLD\n",
        "    rec = get_recognizer(neighbors=neighbors)\n",
        "    dists = []\n",
        "    print(f\"📏 Calibrando limiar (coletando {samples} distâncias)…\")\n",
        "    for i in range(samples):\n",
        "        fr = capture_one(show_raw=True)\n",
        "        if fr is None:\n",
        "            time.sleep(0.05);\n",
        "            continue\n",
        "        g200, bbox = _detect_face_gray200(fr)\n",
        "        vis = fr.copy()\n",
        "        if bbox is not None:\n",
        "            (x,y,w,h) = bbox\n",
        "            draw_box(vis, (x,y,w,h), CALIB_BOX_COLOR, label=\"calib\")\n",
        "            _, dist = rec.predict(g200)\n",
        "            dists.append(dist)\n",
        "            print(f\"[{i+1}/{samples}] dist={dist:.1f}\")\n",
        "        update_display_img(vis)\n",
        "        time.sleep(0.08)\n",
        "    if dists:\n",
        "        p95 = float(np.percentile(dists, 95))\n",
        "        SERVICE_THRESHOLD = round(p95 + 5.0, 1)\n",
        "        print(f\"🎯 Novo SERVICE_THRESHOLD = {SERVICE_THRESHOLD} (p95={p95:.1f} + margem)\")\n",
        "    else:\n",
        "        print(\"⚠️ Calibração insuficiente; threshold mantido.\")\n",
        "    return SERVICE_THRESHOLD"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5468bd2f",
      "metadata": {
        "id": "5468bd2f"
      },
      "source": [
        "## 10) Liveness passivo com preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4ff8dc03",
      "metadata": {
        "id": "4ff8dc03"
      },
      "outputs": [],
      "source": [
        "def liveness_passive_preview(n=24, delay_ms=80, min_energy=LIVENESS_MIN_ENERGY):\n",
        "    prev = None\n",
        "    energy = 0.0\n",
        "    used = 0\n",
        "    for i in range(n):\n",
        "        fr = capture_one(show_raw=True)\n",
        "        if fr is None:\n",
        "            time.sleep(0.05)\n",
        "            continue\n",
        "        faces = detect_faces(fr)\n",
        "        vis = fr.copy()\n",
        "        if len(faces):\n",
        "            (x,y,w,h,score) = max(faces, key=lambda f: f[2]*f[3]) # Unpack 5 values\n",
        "            draw_box(vis, (x,y,w,h), PREVIEW_BOX_COLOR, label=\"live\")\n",
        "            face = fr[y:y+h, x:x+w]\n",
        "            g = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
        "            g = cv2.resize(g, (160,160))\n",
        "            g = preprocess_face_gray(g)\n",
        "            if prev is not None:\n",
        "                diff = cv2.absdiff(g, prev)\n",
        "                energy += float(np.mean(diff))\n",
        "            prev = g; used += 1\n",
        "        update_display_img(vis)\n",
        "        time.sleep(max(0, delay_ms/1000.0))\n",
        "    avg = energy / max(1, used)\n",
        "    verdict = \"live\" if avg >= min_energy else \"spoof\"\n",
        "    print(f\"[Liveness] energia média: {avg:.2f} -> {verdict}\")\n",
        "    return verdict, avg"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3eff28f",
      "metadata": {
        "id": "b3eff28f"
      },
      "source": [
        "## 11) Autenticação 1:1 (com liveness e preview)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6ea82543",
      "metadata": {
        "id": "6ea82543"
      },
      "outputs": [],
      "source": [
        "def authenticate_1v1_preview(expected_user=\"novo_usuario_preview\", neighbors=16, require_liveness=True):\n",
        "    thr = SERVICE_THRESHOLD\n",
        "    if require_liveness:\n",
        "        verdict, energy = liveness_passive_preview(n=24, delay_ms=80, min_energy=LIVENESS_MIN_ENERGY)\n",
        "        if verdict != \"live\":\n",
        "            print(f\"🛑 Liveness falhou (energia={energy:.2f}).\")\n",
        "            result = {\"status\":\"route_review\", \"mode\":\"1v1\", \"reason\":\"liveness_fail\", \"energy\":energy}\n",
        "            maybe_save_evidence(result, np.zeros((10,10,3), dtype=np.uint8), prefix=\"auth_1v1\")\n",
        "            return result\n",
        "\n",
        "    rec = get_recognizer(neighbors=neighbors)\n",
        "    fr = capture_one(show_raw=True)\n",
        "    if fr is None:\n",
        "        result = {\"status\":\"error\", \"mode\":\"1v1\", \"reason\":\"capture_fail\"}\n",
        "        return result\n",
        "\n",
        "    g200, bbox = _detect_face_gray200(fr)\n",
        "    vis = fr.copy()\n",
        "    if bbox is None:\n",
        "        update_display_img(vis)\n",
        "        result = {\"status\":\"error\", \"mode\":\"1v1\", \"reason\":\"no_face\"}\n",
        "        maybe_save_evidence(result, vis, prefix=\"auth_1v1\")\n",
        "        return result\n",
        "\n",
        "    (x,y,w,h) = bbox\n",
        "    draw_box(vis, (x,y,w,h), FINAL_BOX_COLOR, label=\"auth 1:1\")\n",
        "    update_display_img(vis)\n",
        "\n",
        "    label, conf = rec.predict(g200)\n",
        "    pred_user = INV_LABEL_CACHE.get(label, \"desconhecido\")\n",
        "    print(f\"🔐 1:1 — esperado={expected_user} | predito={pred_user} | dist={conf:.1f} | thr={thr}\")\n",
        "\n",
        "    if pred_user == expected_user and conf <= thr:\n",
        "        result = {\"status\":\"approved\", \"mode\":\"1v1\", \"user\":pred_user, \"dist\":float(conf), \"threshold\":thr}\n",
        "    else:\n",
        "        result = {\"status\":\"route_review\", \"mode\":\"1v1\", \"reason\":\"no_match\", \"pred\":pred_user, \"dist\":float(conf), \"threshold\":thr}\n",
        "        maybe_save_evidence(result, vis, prefix=\"auth_1v1\")\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eff1f2a9",
      "metadata": {
        "id": "eff1f2a9"
      },
      "source": [
        "## 12) Autenticação 1:N (com liveness e preview)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "353349b8",
      "metadata": {
        "id": "353349b8"
      },
      "outputs": [],
      "source": [
        "def authenticate_1vN_preview(neighbors=16, require_liveness=True):\n",
        "    thr = SERVICE_THRESHOLD\n",
        "    if require_liveness:\n",
        "        verdict, energy = liveness_passive_preview(n=24, delay_ms=80, min_energy=LIVENESS_MIN_ENERGY)\n",
        "        if verdict != \"live\":\n",
        "            print(f\"🛑 Liveness falhou (energia={energy:.2f}).\")\n",
        "            result = {\"status\":\"route_review\", \"mode\":\"1vN\", \"reason\":\"liveness_fail\", \"energy\":energy}\n",
        "            maybe_save_evidence(result, np.zeros((10,10,3), dtype=np.uint8), prefix=\"auth_1vN\")\n",
        "            return result\n",
        "\n",
        "    rec = get_recognizer(neighbors=neighbors)\n",
        "    fr = capture_one(show_raw=True)\n",
        "    if fr is None:\n",
        "        result = {\"status\":\"error\", \"mode\":\"1vN\", \"reason\":\"capture_fail\"}\n",
        "        return result\n",
        "\n",
        "    g200, bbox = _detect_face_gray200(fr)\n",
        "    vis = fr.copy()\n",
        "    if bbox is None:\n",
        "        update_display_img(vis)\n",
        "        result = {\"status\":\"error\", \"mode\":\"1vN\", \"reason\":\"no_face\"}\n",
        "        maybe_save_evidence(result, vis, prefix=\"auth_1vN\")\n",
        "        return result\n",
        "\n",
        "    (x,y,w,h) = bbox\n",
        "    draw_box(vis, (x,y,w,h), FINAL_BOX_COLOR, label=\"auth 1:N\")\n",
        "    update_display_img(vis)\n",
        "\n",
        "    label, conf = rec.predict(g200)  # menor = melhor\n",
        "    user_pred = INV_LABEL_CACHE.get(label, \"desconhecido\")\n",
        "    print(f\"🧭 1:N — predito={user_pred} | dist={conf:.1f} | thr={thr}\")\n",
        "\n",
        "    if conf <= thr:\n",
        "        result = {\"status\":\"approved\", \"mode\":\"1vN\", \"user\":user_pred, \"dist\":float(conf), \"threshold\":thr}\n",
        "    else:\n",
        "        result = {\"status\":\"route_review\", \"mode\":\"1vN\", \"reason\":\"no_match\", \"pred\":user_pred, \"dist\":float(conf), \"threshold\":thr}\n",
        "        maybe_save_evidence(result, vis, prefix=\"auth_1vN\")\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vxUzE2bnhcOQ",
      "metadata": {
        "id": "vxUzE2bnhcOQ"
      },
      "source": [
        "## 13) Bloco de utilidades de avaliação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "PhKjkZ4ghdSl",
      "metadata": {
        "id": "PhKjkZ4ghdSl"
      },
      "outputs": [],
      "source": [
        "# =============================================\n",
        "# Função: avaliação OFFLINE com DATASET AUTOMÁTICO\n",
        "#  - positives: ENROLL_DIR/<user> (ou todos, se não houver alvo)\n",
        "#  - negatives: persistidos em DATA_DIR/\"negatives\" (captura da câmera se faltarem)\n",
        "#  - usa detect_faces() da célula 5\n",
        "# Retorna: dict com métricas e info (ou None se falhar)\n",
        "# =============================================\n",
        "import os, time, shutil\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "try:\n",
        "    import pandas as pd\n",
        "except Exception:\n",
        "    pd = None\n",
        "\n",
        "def run_offline_eval_from_enrollment(\n",
        "    neg_target_min: int = 30,\n",
        "    capture_batch: int = 30,\n",
        "    capture_sleep_sec: float = 0.15,\n",
        "    eval_max_images: int | None = None,\n",
        "    conf_threshold: float = 0.5,\n",
        "    use_camera: bool = True,\n",
        "    negatives_src_dir: str | Path | None = None,\n",
        "    dataset_dir_override: str | Path | None = None, # Adicionado\n",
        "    default_dataset_dir: str | Path | None = None,  # Adicionado\n",
        "):\n",
        "    \"\"\"\n",
        "    Executa avaliação offline após o fluxo principal do Runner.\n",
        "    - Garante arquivos do DNN (se DETECTION_MODEL = dnn_ssd_resnet10).\n",
        "    - Monta dataset em DATA_DIR/'dataset_auto' (positives de ENROLL_DIR, negatives persistentes).\n",
        "    - Roda matriz de confusão e métricas (robusto a apenas 1 classe).\n",
        "    \"\"\"\n",
        "    # ===== caminhos vindos de células anteriores =====\n",
        "    try:\n",
        "        DATA_DIR\n",
        "    except NameError:\n",
        "        # fallback seguro\n",
        "        globals()[\"DATA_DIR\"] = Path(\"cv_colab_data\")\n",
        "        DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    try:\n",
        "        ENROLL_DIR\n",
        "    except NameError:\n",
        "        globals()[\"ENROLL_DIR\"] = DATA_DIR / \"enroll\"\n",
        "        ENROLL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    NEG_STORE_DIR = DATA_DIR / \"negatives\"      # negativos persistentes\n",
        "    DATASET_AUTO  = DATA_DIR / \"dataset_auto\"   # dataset gerado automaticamente\n",
        "    POS_DIR_AUTO  = DATASET_AUTO / \"positives\"\n",
        "    NEG_DIR_AUTO  = DATASET_AUTO / \"negatives\"\n",
        "    NEG_STORE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    NAME_MAP = {True: \"face\", False: \"no_face\"}\n",
        "\n",
        "    # ===== helpers =====\n",
        "    def _list_images(dirpath: Path):\n",
        "        exts = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\"}\n",
        "        if not dirpath.is_dir():\n",
        "            return []\n",
        "        return sorted([str(p) for p in dirpath.iterdir() if p.suffix.lower() in exts])\n",
        "\n",
        "    def _copy_all_images(src: Path, dst: Path):\n",
        "        dst.mkdir(parents=True, exist_ok=True)\n",
        "        count = 0\n",
        "        for ext in (\"*.jpg\",\"*.jpeg\",\"*.png\",\"*.bmp\",\"*.tif\",\"*.tiff\"):\n",
        "            for p in src.rglob(ext):\n",
        "                out = dst / f\"{p.stem}_{count}{p.suffix.lower()}\"\n",
        "                try:\n",
        "                    shutil.copy2(p, out)\n",
        "                    count += 1\n",
        "                except Exception:\n",
        "                    pass\n",
        "        return count\n",
        "\n",
        "    def _capture_negatives_persistent(store_dir: Path, frames=30, sleep=0.15):\n",
        "        if not use_camera:\n",
        "            return 0\n",
        "        cap = cv2.VideoCapture(0)\n",
        "        if not cap.isOpened():\n",
        "            print(\"✘ Não foi possível abrir a câmera para capturar negativos.\")\n",
        "            return 0\n",
        "        print(f\"🎥 Capturando {frames} negativos (aponte para parede/quadro vazio ou saia do frame)…\")\n",
        "        count = 0\n",
        "        for i in range(frames):\n",
        "            ok, frame = cap.read()\n",
        "            if not ok:\n",
        "                break\n",
        "            out = store_dir / f\"neg_{int(time.time())}_{i:03d}.jpg\"\n",
        "            cv2.imwrite(str(out), frame)\n",
        "            time.sleep(sleep)\n",
        "            count += 1\n",
        "        cap.release()\n",
        "        print(f\"✔ Negativos capturados (persistentes): {count}\")\n",
        "        return count\n",
        "\n",
        "    def _ensure_dnn_files_if_needed():\n",
        "        det = os.environ.get(\"DETECTION_MODEL\", \"\").lower()\n",
        "        if det not in (\"dnn_ssd_resnet10\", \"dnn\", \"ssd\", \"resnet10\"):\n",
        "            return\n",
        "        MODELS_DIR = DATA_DIR / \"models\"\n",
        "        PROTO   = MODELS_DIR / \"deploy.prototxt\"\n",
        "        WEIGHTS = MODELS_DIR / \"res10_300x300_ssd_iter_140000.caffemodel\"\n",
        "        if PROTO.exists() and WEIGHTS.exists():\n",
        "            return\n",
        "        MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "        try:\n",
        "            import urllib.request\n",
        "            if not PROTO.exists():\n",
        "                print(\"Baixando deploy.prototxt …\")\n",
        "                urllib.request.urlretrieve(\n",
        "                    \"https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt\",\n",
        "                    str(PROTO)\n",
        "                )\n",
        "            if not WEIGHTS.exists():\n",
        "                print(\"Baixando res10_300x300_ssd_iter_140000.caffemodel …\")\n",
        "                urllib.request.urlretrieve(\n",
        "                    \"https://github.com/opencv/opencv_3rdparty/raw/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel\",\n",
        "                    str(WEIGHTS)\n",
        "                )\n",
        "            print(\"✔ DNN pronto em\", MODELS_DIR)\n",
        "        except Exception as e:\n",
        "            print(\"✘ Falha ao baixar arquivos do DNN automaticamente:\", e)\n",
        "            print(\"  → Baixe manualmente para:\", MODELS_DIR)\n",
        "\n",
        "    def _evaluate_current_detector(dataset_dir: Path, max_images=None, conf_threshold=0.5):\n",
        "        pos_imgs = _list_images(dataset_dir / \"positives\")\n",
        "        neg_imgs = _list_images(dataset_dir / \"negatives\")\n",
        "        if max_images:\n",
        "            pos_imgs = pos_imgs[:max_images]\n",
        "            neg_imgs = neg_imgs[:max_images]\n",
        "\n",
        "        y_true, y_pred, times = [], [], []\n",
        "        import time as _t\n",
        "        # usa detect_faces() da célula 5\n",
        "        for p in pos_imgs:\n",
        "            img = cv2.imread(p)\n",
        "            if img is None:\n",
        "                continue\n",
        "            t0 = _t.perf_counter()\n",
        "            boxes = detect_faces(img, conf_threshold=conf_threshold)\n",
        "            times.append(_t.perf_counter() - t0)\n",
        "            y_true.append(True)\n",
        "            y_pred.append(len(boxes) > 0)\n",
        "\n",
        "        for p in neg_imgs:\n",
        "            img = cv2.imread(p)\n",
        "            if img is None:\n",
        "                continue\n",
        "            t0 = _t.perf_counter()\n",
        "            boxes = detect_faces(img, conf_threshold=conf_threshold)\n",
        "            times.append(_t.perf_counter() - t0)\n",
        "            y_true.append(False)\n",
        "            y_pred.append(len(boxes) > 0)\n",
        "\n",
        "        if not y_true:\n",
        "            raise ValueError(\"Dataset vazio para avaliação.\")\n",
        "\n",
        "        # classes realmente presentes\n",
        "        present = sorted(set(y_true))\n",
        "        cm = confusion_matrix(y_true, y_pred, labels=present)\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "        def _safe(fn):\n",
        "            try:\n",
        "                return fn(y_true, y_pred, zero_division=0)\n",
        "            except Exception:\n",
        "                return float(\"nan\")\n",
        "\n",
        "        if set(present) == {True, False}:\n",
        "            prec = precision_score(y_true, y_pred, zero_division=0)\n",
        "            rec  = recall_score(y_true, y_pred, zero_division=0)\n",
        "            f1   = f1_score(y_true, y_pred, zero_division=0)\n",
        "            print(\"\\n=== Relatório (2 classes) ===\")\n",
        "            print(classification_report(\n",
        "                y_true, y_pred, labels=present,\n",
        "                target_names=[NAME_MAP[c] for c in present], zero_division=0\n",
        "            ))\n",
        "        else:\n",
        "            prec = _safe(precision_score)\n",
        "            rec  = _safe(recall_score)\n",
        "            f1   = _safe(f1_score)\n",
        "            faltante = \"no_face\" if present == [True] else \"face\"\n",
        "            print(f\"\\n[AVISO] Apenas 1 classe presente (faltando: {faltante}). Métricas completas não se aplicam.\")\n",
        "\n",
        "        avg_time = float(np.mean(times)) if times else float(\"nan\")\n",
        "        mdl = os.environ.get(\"DETECTION_MODEL\", \"(desconhecido)\")\n",
        "\n",
        "        print(f\"\\n=== Resultados: {mdl} ===\")\n",
        "        print(f\"Accuracy : {acc:.4f}\")\n",
        "        print(f\"Precision: {prec if np.isfinite(prec) else 'N/A'}\")\n",
        "        print(f\"Recall   : {rec if np.isfinite(rec) else 'N/A'}\")\n",
        "        print(f\"F1-score : {f1 if np.isfinite(f1) else 'N/A'}\")\n",
        "        print(f\"Tempo médio por imagem: {avg_time:.4f} s\")\n",
        "\n",
        "        # Plot\n",
        "        plt.figure(figsize=(4, 3))\n",
        "        plt.imshow(cm, interpolation='nearest')\n",
        "        plt.title(f\"Matriz de Confusão - {mdl}\")\n",
        "        plt.colorbar()\n",
        "        ticks = np.arange(len(present))\n",
        "        plt.xticks(ticks, [NAME_MAP[c] for c in present], rotation=45)\n",
        "        plt.yticks(ticks, [NAME_MAP[c] for c in present])\n",
        "        thresh = cm.max() / 2.0 if cm.size else 0\n",
        "        for i in range(cm.shape[0]):\n",
        "            for j in range(cm.shape[1]):\n",
        "                plt.text(j, i, f\"{cm[i, j]:d}\",\n",
        "                         ha=\"center\",\n",
        "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        plt.ylabel('Verdadeiro')\n",
        "        plt.xlabel('Predito')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        metrics = {\n",
        "            \"model\": mdl, \"accuracy\": float(acc),\n",
        "            \"precision\": (float(prec) if np.isfinite(prec) else None),\n",
        "            \"recall\": (float(rec) if np.isfinite(rec) else None),\n",
        "            \"f1\": (float(f1) if np.isfinite(f1) else None),\n",
        "            \"avg_time\": avg_time, \"n_samples\": len(y_true),\n",
        "            \"classes_presentes\": [NAME_MAP[c] for c in present],\n",
        "            \"confusion_matrix\": cm,\n",
        "        }\n",
        "\n",
        "        if pd is not None:\n",
        "            try:\n",
        "                df = pd.DataFrame([{\n",
        "                    \"modelo\": metrics[\"model\"], \"accuracy\": metrics[\"accuracy\"],\n",
        "                    \"precision\": metrics[\"precision\"], \"recall\": metrics[\"recall\"],\n",
        "                    \"f1\": metrics[\"f1\"], \"avg_time\": metrics[\"avg_time\"],\n",
        "                    \"n\": metrics[\"n_samples\"], \"classes_presentes\": metrics[\"classes_presentes\"]\n",
        "                }])\n",
        "                display(df)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    # ===== 1) DNN (se necessário) =====\n",
        "    _ensure_dnn_files_if_needed()\n",
        "\n",
        "    # ===== Use dataset_override if provided =====\n",
        "    if dataset_dir_override is not None:\n",
        "        print(f\"ℹ Usando dataset override: {dataset_dir_override}\")\n",
        "        try:\n",
        "            metrics = _evaluate_current_detector(Path(dataset_dir_override), max_images=eval_max_images, conf_threshold=conf_threshold)\n",
        "            return metrics\n",
        "        except Exception as e:\n",
        "            print(\"✘ Falha na avaliação offline com dataset override:\", e)\n",
        "            return None\n",
        "\n",
        "    # ===== Use default_dataset_dir if provided =====\n",
        "    if default_dataset_dir is not None:\n",
        "        print(f\"ℹ Usando dataset DEFAULT: {default_dataset_dir}\")\n",
        "        try:\n",
        "            metrics = _evaluate_current_detector(Path(default_dataset_dir), max_images=eval_max_images, conf_threshold=conf_threshold)\n",
        "            return metrics\n",
        "        except Exception as e:\n",
        "            print(\"✘ Falha na avaliação offline com dataset default:\", e)\n",
        "            return None\n",
        "\n",
        "    # ===== 2) usuário alvo para positives (para dataset_auto) =====\n",
        "    target_user = None\n",
        "    if 'NEW_USER_ID' in globals() and NEW_USER_ID:\n",
        "        target_user = NEW_USER_ID\n",
        "    elif 'AUTH_MODE_1V1' in globals() and AUTH_MODE_1V1 and 'EXPECTED_USER_1V1' in globals() and EXPECTED_USER_1V1:\n",
        "        target_user = EXPECTED_USER_1V1\n",
        "\n",
        "    # ===== 3) (re)criar dataset_auto =====\n",
        "    if DATASET_AUTO.exists():\n",
        "        shutil.rmtree(DATASET_AUTO)\n",
        "    POS_DIR_AUTO.mkdir(parents=True, exist_ok=True)\n",
        "    NEG_DIR_AUTO.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # ===== 4) positives (para dataset_auto) =====\n",
        "    npos = 0\n",
        "    if target_user:\n",
        "        src_pos = ENROLL_DIR / target_user\n",
        "        if src_pos.is_dir():\n",
        "            npos = _copy_all_images(src_pos, POS_DIR_AUTO)\n",
        "            print(f\"✔ Positives de '{target_user}': {npos}\")\n",
        "        else:\n",
        "            print(f\"✘ Não encontrei ENROLL_DIR para '{target_user}': {src_pos}\")\n",
        "    else:\n",
        "        if ENROLL_DIR.is_dir():\n",
        "            for sub in ENROLL_DIR.iterdir():\n",
        "                if sub.is_dir():\n",
        "                    npos += _copy_all_images(sub, POS_DIR_AUTO)\n",
        "        print(f\"✔ Positives (todos): {npos}\")\n",
        "\n",
        "    # ===== 5) negatives persistentes (para dataset_auto) =====\n",
        "    neg_existing = len(_list_images(NEG_STORE_DIR))\n",
        "    # opção: copiar de pasta externa, se fornecida\n",
        "    if negatives_src_dir is not None and Path(negatives_src_dir).is_dir():\n",
        "        added = _copy_all_images(Path(negatives_src_dir), NEG_STORE_DIR)\n",
        "        neg_existing += added\n",
        "        print(f\"✔ Negativos importados de '{negatives_src_dir}': +{added} (total={neg_existing})\")\n",
        "\n",
        "    if neg_existing < neg_target_min:\n",
        "        falta = neg_target_min - neg_existing\n",
        "        batch = max(capture_batch, falta)\n",
        "        print(f\"ℹ Negativos existentes: {neg_existing}. Capturando {batch} para atingir >= {neg_target_min} …\")\n",
        "        _capture_negatives_persistent(NEG_STORE_DIR, frames=batch, sleep=capture_sleep_sec)\n",
        "\n",
        "    # copiar negativos persistentes → dataset_auto\n",
        "    nneg = _copy_all_images(NEG_STORE_DIR, NEG_DIR_AUTO)\n",
        "    print(f\"✔ Negatives adicionados ao dataset_auto: {nneg}\")\n",
        "\n",
        "    # ===== 6) avaliação (dataset_auto) =====\n",
        "    try:\n",
        "        metrics = _evaluate_current_detector(DATASET_AUTO, max_images=eval_max_images, conf_threshold=conf_threshold)\n",
        "        return metrics\n",
        "    except Exception as e:\n",
        "        print(\"✘ Falha na avaliação offline:\", e)\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "250ccea5",
      "metadata": {
        "id": "250ccea5"
      },
      "source": [
        "## 14) Pipelines compactos (1:1 e 1:N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "51b5a590",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "51b5a590",
        "outputId": "136aedf3-420b-4709-8657-bea3819d0f95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selecione o detector de faces a ser usado na célula 5 (detect_faces).\n",
            "Opções: 'haar' (CascadeClassifier) ou 'dnn_ssd_resnet10' (OpenCV DNN SSD ResNet10)\n",
            "Detector [dnn_ssd_resnet10/haar]: haar\n",
            "[Runner] Detector selecionado: haar\n",
            "Deseja incluir um novo usuário no modelo ou apenas autenticar um já existente?\n",
            "Digite 'novo' para incluir, ou 'auth' para apenas autenticar.\n",
            "Modo [novo/auth]: novo\n",
            "Digite o identificador do novo usuário (ou deixe em branco para gerar automático): teste\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Coletando 30 amostras para 'teste'… Olhe para a câmera.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Enrollment concluído: 30/30 amostras salvas.\n",
            "↻ Treinando LBPH (memória)…\n",
            "Modelo LBPH treinado em memória. Usuários: ['teste']\n",
            "📏 Calibrando limiar (coletando 15 distâncias)…\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/15] dist=68.2\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2/15] dist=97.3\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3/15] dist=67.8\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4/15] dist=68.6\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[5/15] dist=66.3\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[6/15] dist=63.3\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[7/15] dist=62.8\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[8/15] dist=63.5\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[9/15] dist=62.4\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[10/15] dist=66.6\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[11/15] dist=71.7\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[12/15] dist=64.9\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[13/15] dist=68.5\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[14/15] dist=64.6\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[15/15] dist=64.4\n",
            "🎯 Novo SERVICE_THRESHOLD = 84.4 (p95=79.4 + margem)\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Liveness] energia média: 8.83 -> live\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔐 1:1 — esperado=teste | predito=teste | dist=63.7 | thr=84.4\n"
          ]
        },
        {
          "data": {
            "application/javascript": "if (window.stopColabCamera) window.stopColabCamera();",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'status': 'approved',\n",
              " 'mode': '1v1',\n",
              " 'user': 'teste',\n",
              " 'dist': 63.742280503879435,\n",
              " 'threshold': 84.4}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# =============================\n",
        "# Runner: inclusão (novo) ou autenticação (auth) com escolha 1:1 / 1:N\n",
        "# + escolha do detector de faces (haar | dnn_ssd_resnet10)\n",
        "# =============================\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Aqui apenas definimos qual detector será usado pelo pipeline já existente.\n",
        "print(\"Selecione o detector de faces a ser usado na célula 5 (detect_faces).\")\n",
        "print(\"Opções: 'haar' (CascadeClassifier) ou 'dnn_ssd_resnet10' (OpenCV DNN SSD ResNet10)\")\n",
        "_detector_choice = input(\"Detector [dnn_ssd_resnet10/haar]: \").strip().lower()\n",
        "if _detector_choice not in (\"\", \"haar\", \"dnn_ssd_resnet10\", \"dnn\", \"ssd\", \"resnet10\"):\n",
        "    print(\"Opção inválida; usando padrão: dnn_ssd_resnet10\")\n",
        "    _detector_choice = \"dnn_ssd_resnet10\"\n",
        "if _detector_choice in (\"\", \"dnn\", \"ssd\", \"resnet10\"):\n",
        "    _detector_choice = \"dnn_ssd_resnet10\"\n",
        "\n",
        "# Propaga para a célula 5: variável global e variável de ambiente (para debug/diagnóstico)\n",
        "try:\n",
        "    DETECTION_MODEL  # verifica se existe a global da célula 5\n",
        "    globals()[\"DETECTION_MODEL\"] = _detector_choice\n",
        "except NameError:\n",
        "    # Se a célula 5 ainda não foi executada, definimos aqui para não quebrar;\n",
        "    # quando a célula 5 for rodada, ela lerá este valor do ambiente.\n",
        "    pass\n",
        "os.environ[\"DETECTION_MODEL\"] = _detector_choice\n",
        "print(f\"[Runner] Detector selecionado: {os.environ['DETECTION_MODEL']}\")\n",
        "\n",
        "# (Opcional) Validação rápida de arquivos quando DNN é escolhido (evita erro tardio)\n",
        "if os.environ[\"DETECTION_MODEL\"] == \"dnn_ssd_resnet10\":\n",
        "    proto = os.environ.get(\"DNN_PROTO_PATH\", \"models/deploy.prototxt\")\n",
        "    weights = os.environ.get(\"DNN_WEIGHTS_PATH\", \"models/res10_300x300_ssd_iter_140000_fp16.caffemodel\")\n",
        "    if not (Path(proto).exists() and Path(weights).exists()):\n",
        "        print(\"[Aviso DNN] Arquivos do DNN não encontrados.\")\n",
        "        print(f\"  Prototxt: {proto}\")\n",
        "        print(f\"  Pesos   : {weights}\")\n",
        "        print(\"  → Ajuste DNN_PROTO_PATH / DNN_WEIGHTS_PATH ou coloque os arquivos em 'models/'.\")\n",
        "\n",
        "N_SAMPLES          = 30                      # amostras para enrollment (30–60 recomendado)\n",
        "FRAME_DELAY_MS     = 100                     # intervalo entre capturas (ms)\n",
        "LBPH_NEIGHBORS     = 16                      # 8 ou 16 costumam ir bem\n",
        "DO_LIVENESS_TEST   = True                    # liveness antes da autenticação\n",
        "\n",
        "# Pergunta inicial\n",
        "print(\"Deseja incluir um novo usuário no modelo ou apenas autenticar um já existente?\")\n",
        "print(\"Digite 'novo' para incluir, ou 'auth' para apenas autenticar.\")\n",
        "modo = input(\"Modo [novo/auth]: \").strip().lower()\n",
        "\n",
        "# Definições padrão\n",
        "NEW_USER_ID = None\n",
        "EXPECTED_USER_1V1 = None\n",
        "AUTH_MODE_1V1 = True  # default (será perguntado quando for 'auth')\n",
        "\n",
        "if modo == \"novo\":\n",
        "    typed_name = input(\"Digite o identificador do novo usuário (ou deixe em branco para gerar automático): \").strip()\n",
        "    if not typed_name:\n",
        "        # Gera nome automático com o próximo índice baseado nas pastas já existentes\n",
        "        try:\n",
        "            existing_dirs = sorted([d.name for d in ENROLL_DIR.iterdir() if d.is_dir()])\n",
        "        except Exception:\n",
        "            existing_dirs = []\n",
        "        idx = len(existing_dirs)\n",
        "        typed_name = f\"novo_usuario_preview_{idx+1}\"\n",
        "        print(f\"[auto] Nome atribuído: {typed_name}\")\n",
        "    NEW_USER_ID = typed_name\n",
        "    EXPECTED_USER_1V1 = NEW_USER_ID  # após incluir, autentica 1:1 (mesma pessoa)\n",
        "\n",
        "elif modo == \"auth\":\n",
        "    # Escolha do tipo de autenticação\n",
        "    print(\"Qual tipo de autenticação deseja usar?\")\n",
        "    print(\"Digite '1' para 1:1 (comparar com um usuário específico) ou 'N' para 1:N (identificar entre os cadastrados).\")\n",
        "    auth_choice = input(\"Tipo [1/N]: \").strip().lower()\n",
        "    if auth_choice == \"1\":\n",
        "        AUTH_MODE_1V1 = True\n",
        "        EXPECTED_USER_1V1 = input(\"Digite o identificador do usuário a ser autenticado (1:1): \").strip()\n",
        "        if not EXPECTED_USER_1V1:\n",
        "            raise ValueError(\"Para autenticação 1:1 é necessário informar o identificador esperado.\")\n",
        "    else:\n",
        "        AUTH_MODE_1V1 = False\n",
        "        EXPECTED_USER_1V1 = None  # 1:N não requer nome\n",
        "else:\n",
        "    raise ValueError(\"Opção inválida. Use 'novo' ou 'auth'.\")\n",
        "\n",
        "display_result = {\"status\":\"error\", \"reason\":\"not_executed\"}\n",
        "try:\n",
        "    if modo == \"novo\":\n",
        "        # 1) enrollment do novo usuário\n",
        "        enroll_user_with_preview(user_id=NEW_USER_ID, n_samples=N_SAMPLES, interval_ms=FRAME_DELAY_MS)\n",
        "        # 2) re-treino do modelo em memória\n",
        "        _ = get_recognizer(neighbors=LBPH_NEIGHBORS, force_retrain=True)\n",
        "        # 3) calibração de threshold\n",
        "        calibrate_threshold(samples=15, neighbors=LBPH_NEIGHBORS)\n",
        "    else:\n",
        "        # Apenas autenticação: garante que o modelo está carregado (re-treina se necessário)\n",
        "        try:\n",
        "            _ = get_recognizer(neighbors=LBPH_NEIGHBORS, force_retrain=False)\n",
        "        except Exception:\n",
        "            _ = get_recognizer(neighbors=LBPH_NEIGHBORS, force_retrain=True)\n",
        "\n",
        "    # 4) autenticação (sempre roda)\n",
        "    if AUTH_MODE_1V1:\n",
        "        display_result = authenticate_1v1_preview(expected_user=EXPECTED_USER_1V1,\n",
        "                                                  neighbors=LBPH_NEIGHBORS,\n",
        "                                                  require_liveness=DO_LIVENESS_TEST)\n",
        "    else:\n",
        "        display_result = authenticate_1vN_preview(neighbors=LBPH_NEIGHBORS,\n",
        "                                                  require_liveness=DO_LIVENESS_TEST)\n",
        "\n",
        "finally:\n",
        "    # sempre encerra a pré-visualização no Colab / janelas no local\n",
        "    try:\n",
        "        stop_camera_ui()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "display_result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C9h-DdbkOaqp",
      "metadata": {
        "id": "C9h-DdbkOaqp"
      },
      "source": [
        "## 15) Execução de AVALIAÇÃO offline (após o fluxo de câmera)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "RxMyTuepD2e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RxMyTuepD2e2",
        "outputId": "43fed9ef-4fb5-4496-baca-351b0eb9b806"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rodar avaliação offline (Automático/Default/Remoto)? [s/N]: s\n",
            "Fonte do dataset:\n",
            "  1) Automático (enrollment + negatives persistentes/câmera)\n",
            "  2) DEFAULT (cv_colab_data/dataset_default)\n",
            "  3) REMOTO/LOCAL existente (URL .zip/.tar/.tar.gz/.tgz ou caminho local)\n",
            "Escolha [1/2/3]: 3\n",
            "Informe URL (.zip/.tar/.tar.gz/.tgz) OU caminho local com positives/ e negatives/ (Enter = usar default https://data.caltech.edu/records/6rjah-hdv18/files/faces.tar): \n",
            "Baixando dataset remoto: https://data.caltech.edu/records/6rjah-hdv18/files/faces.tar\n",
            "✔ Arquivo salvo em: /tmp/tmp7mp31qeu/dataset_remote.tar\n",
            "[debug] Arquivos extraídos: 452 (mostrando até 12)\n",
            "   - cv_colab_data/dataset_remote/image_0354.jpg\n",
            "   - cv_colab_data/dataset_remote/image_0024.jpg\n",
            "   - cv_colab_data/dataset_remote/image_0350.jpg\n",
            "   - cv_colab_data/dataset_remote/image_0025.jpg\n",
            "   - cv_colab_data/dataset_remote/image_0406.jpg\n",
            "   - cv_colab_data/dataset_remote/image_0280.jpg\n",
            "   - cv_colab_data/dataset_remote/image_0032.jpg\n",
            "   - cv_colab_data/dataset_remote/image_0095.jpg\n",
            "   - cv_colab_data/dataset_remote/image_0059.jpg\n",
            "   - cv_colab_data/dataset_remote/image_0360.jpg\n",
            "   - cv_colab_data/dataset_remote/image_0033.jpg\n",
            "   - cv_colab_data/dataset_remote/image_0048.jpg\n",
            "[single-class] Convertendo todas as imagens extraídas em 'positives/' e completando 'negatives/'…\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2285713670.py:239: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tf.extractall(str(out_dir))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[single-class] Positives criados: 450\n",
            "[negatives] Nenhum negative encontrado. Baixando Caltech-101…\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2285713670.py:109: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  tf.extractall(str(out_dir)); return True\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[negatives] Encontrado BACKGROUND_Google — usando como negativos.\n",
            "[negatives] Copiados 467 negatives → cv_colab_data/dataset_remote/negatives\n",
            "✔ dataset 'single-class' preparado em: cv_colab_data/dataset_remote\n",
            "Usar câmera para completar negativos (também em Remoto/Default)? [S/n]: s\n",
            "Pasta extra com negativos (vazio = nenhuma): \n",
            "Limitar nº de imagens por classe? (vazio = sem limite): \n",
            "Conf threshold (vazio = 0.5): \n",
            "ℹ Usando dataset override: cv_colab_data/dataset_remote\n",
            "\n",
            "=== Relatório (2 classes) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     no_face       0.99      0.90      0.95       467\n",
            "        face       0.91      0.99      0.95       450\n",
            "\n",
            "    accuracy                           0.95       917\n",
            "   macro avg       0.95      0.95      0.95       917\n",
            "weighted avg       0.95      0.95      0.95       917\n",
            "\n",
            "\n",
            "=== Resultados: haar ===\n",
            "Accuracy : 0.9466\n",
            "Precision: 0.9066937119675457\n",
            "Recall   : 0.9933333333333333\n",
            "F1-score : 0.9480381760339343\n",
            "Tempo médio por imagem: 0.1237 s\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x300 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"        )\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"modelo\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"haar\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.9465648854961832,\n        \"max\": 0.9465648854961832,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9465648854961832\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.9066937119675457,\n        \"max\": 0.9066937119675457,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9066937119675457\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.9933333333333333,\n        \"max\": 0.9933333333333333,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9933333333333333\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.9480381760339343,\n        \"max\": 0.9480381760339343,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9480381760339343\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.12374462504253199,\n        \"max\": 0.12374462504253199,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.12374462504253199\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 917,\n        \"max\": 917,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          917\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"classes_presentes\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-df7f833e-80d0-4e6e-802a-965aa9d8ba1f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>modelo</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1</th>\n",
              "      <th>avg_time</th>\n",
              "      <th>n</th>\n",
              "      <th>classes_presentes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>haar</td>\n",
              "      <td>0.946565</td>\n",
              "      <td>0.906694</td>\n",
              "      <td>0.993333</td>\n",
              "      <td>0.948038</td>\n",
              "      <td>0.123745</td>\n",
              "      <td>917</td>\n",
              "      <td>[no_face, face]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df7f833e-80d0-4e6e-802a-965aa9d8ba1f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-df7f833e-80d0-4e6e-802a-965aa9d8ba1f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-df7f833e-80d0-4e6e-802a-965aa9d8ba1f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "  modelo  accuracy  precision    recall        f1  avg_time    n  \\\n",
              "0   haar  0.946565   0.906694  0.993333  0.948038  0.123745  917   \n",
              "\n",
              "  classes_presentes  \n",
              "0   [no_face, face]  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# =============================\n",
        "# Execução de AVALIAÇÃO offline (após o fluxo de câmera)\n",
        "# - Fonte: Automático | Default | Remoto/Local\n",
        "# - Remoto default (positivos): Caltech Face 1999 (faces.tar)\n",
        "# - Fallback de negativos: Caltech-101 (extrai camadas internas; usa BACKGROUND_Google ou outras classes não-face)\n",
        "# - Captura por câmera também em Remoto/Default quando negatives/ estiver vazio\n",
        "# =============================\n",
        "from pathlib import Path\n",
        "import os, re, shutil, tarfile, zipfile, urllib.request, tempfile, glob\n",
        "\n",
        "# ---------- diretórios base ----------\n",
        "try:\n",
        "    DATA_DIR\n",
        "except NameError:\n",
        "    DATA_DIR = Path(\"cv_colab_data\")\n",
        "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DEFAULT_DATASET_DIR = DATA_DIR / \"dataset_default\"\n",
        "(DEFAULT_DATASET_DIR / \"positives\").mkdir(parents=True, exist_ok=True)\n",
        "(DEFAULT_DATASET_DIR / \"negatives\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------- URLs defaults ----------\n",
        "# Positivos (rostos) — Caltech Face 1999 (CaltechDATA)\n",
        "REMOTE_DATASET_URL_DEFAULT = \"https://data.caltech.edu/records/6rjah-hdv18/files/faces.tar\"\n",
        "# Negativos fallback — Caltech-101 (CaltechDATA, zip que contém .tar.gz internamente)\n",
        "CALTECH101_URL = \"https://data.caltech.edu/records/mzrjq-6wc02/files/caltech-101.zip\"\n",
        "\n",
        "# ---------- helpers utilitários ----------\n",
        "def _debug_list_some_files(root: Path, n=12):\n",
        "    files = [p for p in root.rglob(\"*\") if p.is_file()]\n",
        "    print(f\"[debug] Arquivos extraídos: {len(files)} (mostrando até {n})\")\n",
        "    for p in files[:n]:\n",
        "        print(\"   -\", p.as_posix())\n",
        "\n",
        "def _find_dataset_root_with_pos_neg(root: Path) -> Path | None:\n",
        "    \"\"\"Procura um diretório que contenha positives/ e negatives/.\"\"\"\n",
        "    if (root / \"positives\").is_dir() and (root / \"negatives\").is_dir():\n",
        "        return root\n",
        "    for p in root.rglob(\"*\"):\n",
        "        if p.is_dir() and (p / \"positives\").is_dir() and (p / \"negatives\").is_dir():\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "def _collect_images(root: Path):\n",
        "    return [Path(x) for x in glob.glob(str(root / \"**\" / \"*\"), recursive=True)\n",
        "            if os.path.isfile(x) and Path(x).suffix.lower() in (\".jpg\",\".jpeg\",\".png\",\".bmp\",\".tif\",\".tiff\",\".pgm\")]\n",
        "\n",
        "def _reorganize_faces_nonfaces(src_root: Path) -> Path | None:\n",
        "    \"\"\"\n",
        "    Heurística para separar positivos/negativos:\n",
        "      - negativos: 'nonface', 'non-face', 'non_face', 'non', 'neg', 'background', 'bg'\n",
        "      - positivos: contém 'face' e NÃO contém termos de negativo (evita 'interface')\n",
        "    \"\"\"\n",
        "    neg_patterns = (r\"non[\\-_ ]?face\", r\"\\bnon\\b\", r\"\\bneg(ative)?s?\\b\", r\"background\", r\"\\bbg\\b\")\n",
        "    def is_neg(path_str: str):\n",
        "        low = path_str.lower()\n",
        "        return any(re.search(p, low) for p in neg_patterns)\n",
        "\n",
        "    pos_dir = src_root / \"positives\"; pos_dir.mkdir(exist_ok=True)\n",
        "    neg_dir = src_root / \"negatives\"; neg_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    copied_pos = copied_neg = 0\n",
        "    for f in _collect_images(src_root):\n",
        "        s = f.as_posix().lower()\n",
        "        if is_neg(s):\n",
        "            dst = neg_dir / f.name\n",
        "            try: shutil.copy2(f, dst); copied_neg += 1\n",
        "            except: pass\n",
        "        elif (\"face\" in s) and not is_neg(s) and (\"interface\" not in s):\n",
        "            dst = pos_dir / f.name\n",
        "            try: shutil.copy2(f, dst); copied_pos += 1\n",
        "            except: pass\n",
        "\n",
        "    if copied_pos + copied_neg == 0:\n",
        "        return None\n",
        "    print(f\"[reorganize] positives={copied_pos}, negatives={copied_neg}\")\n",
        "    return src_root\n",
        "\n",
        "# ---------- negativos: Caltech-101 com extração recursiva ----------\n",
        "def _ensure_negatives_from_caltech101(neg_dir: Path, max_to_copy: int | None = None):\n",
        "    \"\"\"\n",
        "    Se 'neg_dir' estiver vazio, baixa Caltech-101 (CaltechDATA, .zip) e preenche negativos:\n",
        "      1) Extrai recursivamente camadas internas (.zip/.tar/.tgz/.tar.gz).\n",
        "      2) Preferência: pasta BACKGROUND_Google.\n",
        "      3) Se não houver, usa outras categorias != 'face' como negativos.\n",
        "    \"\"\"\n",
        "    if any(neg_dir.iterdir()):\n",
        "        return\n",
        "\n",
        "    print(\"[negatives] Nenhum negative encontrado. Baixando Caltech-101…\")\n",
        "    import zipfile, tarfile\n",
        "    tmp = Path(tempfile.mkdtemp())\n",
        "    try:\n",
        "        # download\n",
        "        guessed_suffix = \"\".join(Path(CALTECH101_URL.split(\"?\")[0]).suffixes) or \".zip\"\n",
        "        arc = tmp / (\"caltech101\" + guessed_suffix)\n",
        "        urllib.request.urlretrieve(CALTECH101_URL, str(arc))\n",
        "\n",
        "        extract_root = tmp / \"caltech101_extracted\"\n",
        "        extract_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        def _extract_once(archive_path: Path, out_dir: Path):\n",
        "            low = archive_path.name.lower()\n",
        "            if low.endswith(\".zip\"):\n",
        "                with zipfile.ZipFile(str(archive_path), \"r\") as zf:\n",
        "                    zf.extractall(str(out_dir)); return True\n",
        "            if low.endswith((\".tar.gz\", \".tgz\", \".tar\")):\n",
        "                with tarfile.open(str(archive_path), \"r:*\") as tf:\n",
        "                    tf.extractall(str(out_dir)); return True\n",
        "            return False\n",
        "\n",
        "        # primeira camada\n",
        "        if not _extract_once(arc, extract_root):\n",
        "            print(\"[negatives] Formato não reconhecido no primeiro nível, abortando fallback.\")\n",
        "            return\n",
        "\n",
        "        # extrai camadas internas até 2 níveis\n",
        "        def _extract_nested_archives(root: Path, max_depth: int = 2):\n",
        "            for _ in range(max_depth):\n",
        "                inner = [p for p in root.rglob(\"*\") if p.is_file() and (\n",
        "                    p.name.lower().endswith(\".zip\") or\n",
        "                    p.name.lower().endswith(\".tar\") or\n",
        "                    p.name.lower().endswith(\".tar.gz\") or\n",
        "                    p.name.lower().endswith(\".tgz\") or\n",
        "                    p.name.lower().endswith(\".gz\")\n",
        "                )]\n",
        "                any_new = False\n",
        "                for a in inner:\n",
        "                    base = a.name\n",
        "                    for suf in (\".tar.gz\", \".tgz\", \".zip\", \".tar\", \".gz\"):\n",
        "                        if base.endswith(suf):\n",
        "                            base = base[: -len(suf)]\n",
        "                    out = a.parent / (base + \"_extracted\")\n",
        "                    out.mkdir(exist_ok=True)\n",
        "                    try:\n",
        "                        if _extract_once(a, out):\n",
        "                            any_new = True\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                if not any_new:\n",
        "                    break\n",
        "\n",
        "        _extract_nested_archives(extract_root, max_depth=2)\n",
        "\n",
        "        # localizar BACKGROUND_Google\n",
        "        bg = None\n",
        "        for p in extract_root.rglob(\"*\"):\n",
        "            if p.is_dir() and p.name.lower() == \"background_google\":\n",
        "                bg = p; break\n",
        "\n",
        "        exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
        "        copied = 0\n",
        "        def _copy_all_images(src_dir: Path):\n",
        "            nonlocal copied\n",
        "            for f in src_dir.rglob(\"*\"):\n",
        "                if f.is_file() and f.suffix.lower() in exts:\n",
        "                    if max_to_copy is not None and copied >= max_to_copy:\n",
        "                        return\n",
        "                    dst = neg_dir / f.name\n",
        "                    try: shutil.copy2(f, dst); copied += 1\n",
        "                    except: pass\n",
        "\n",
        "        if bg is not None:\n",
        "            print(\"[negatives] Encontrado BACKGROUND_Google — usando como negativos.\")\n",
        "            _copy_all_images(bg)\n",
        "        else:\n",
        "            print(\"[negatives] BACKGROUND_Google não encontrado. Usando outras categorias como negativos.\")\n",
        "            # escolher diretório com mais subpastas (normalmente '101_ObjectCategories')\n",
        "            candidate_roots = [p for p in extract_root.rglob(\"*\") if p.is_dir()]\n",
        "            root = max(candidate_roots, key=lambda d: sum(1 for _ in d.iterdir() if _.is_dir()), default=extract_root)\n",
        "            for cat in root.iterdir():\n",
        "                if not cat.is_dir(): continue\n",
        "                name = cat.name.lower()\n",
        "                if \"face\" in name:  # não usar categorias de rosto como negativos\n",
        "                    continue\n",
        "                _copy_all_images(cat)\n",
        "                if max_to_copy is not None and copied >= max_to_copy:\n",
        "                    break\n",
        "\n",
        "        print(f\"[negatives] Copiados {copied} negatives → {neg_dir}\")\n",
        "        if copied == 0:\n",
        "            print(\"[negatives] Aviso: não foi possível obter imagens negativas do Caltech-101 (verifique o pacote).\")\n",
        "\n",
        "    finally:\n",
        "        try: shutil.rmtree(tmp)\n",
        "        except: pass\n",
        "\n",
        "# ---------- negativos via câmera (agora também em Remoto/Default) ----------\n",
        "def _ensure_negatives_via_camera(neg_dir: Path, frames: int = 40, sleep_sec: float = 0.15):\n",
        "    import time, cv2\n",
        "    neg_dir.mkdir(parents=True, exist_ok=True)\n",
        "    cap = cv2.VideoCapture(0)\n",
        "    if not cap.isOpened():\n",
        "        print(\"✘ Câmera indisponível para capturar negativos.\")\n",
        "        return 0\n",
        "    print(f\"🎥 Capturando {frames} negativos… (aponte para parede/fundo vazio)\")\n",
        "    saved = 0\n",
        "    for i in range(frames):\n",
        "        ok, frame = cap.read()\n",
        "        if not ok: break\n",
        "        cv2.imwrite(str(neg_dir / f\"neg_cam_{int(time.time())}_{i:03d}.jpg\"), frame)\n",
        "        time.sleep(sleep_sec); saved += 1\n",
        "    cap.release()\n",
        "    print(f\"✔ Negativos capturados: {saved}\")\n",
        "    return saved\n",
        "\n",
        "# ---------- download/extração do dataset remoto (faces.tar, etc.) ----------\n",
        "def _download_and_unpack_archive(url: str, dest_root: Path, *, fallback_to_default=True) -> Path:\n",
        "    \"\"\"\n",
        "    Baixa .zip/.tar/.tar.gz/.tgz para dest_root/dataset_remote e tenta:\n",
        "      1) Encontrar positives/ e negatives/;\n",
        "      2) Reorganizar por heurística (face vs non/background);\n",
        "      3) Completar negatives com Caltech-101, se necessário;\n",
        "      4) Fallback POSITIVOS default (Caltech Face 1999);\n",
        "      5) Último recurso: 'single-class' (todas imagens -> positives) + completar negatives.\n",
        "    Retorna a pasta final contendo positives/ e negatives/.\n",
        "    \"\"\"\n",
        "    dest_root.mkdir(parents=True, exist_ok=True)\n",
        "    tmp_dir = Path(tempfile.mkdtemp())\n",
        "    try:\n",
        "        suffixes = \"\".join(Path(url.split(\"?\")[0]).suffixes) or \".bin\"\n",
        "        archive_path = tmp_dir / f\"dataset_remote{suffixes}\"\n",
        "        print(f\"Baixando dataset remoto: {url}\")\n",
        "        urllib.request.urlretrieve(url, str(archive_path))\n",
        "        print(f\"✔ Arquivo salvo em: {archive_path}\")\n",
        "\n",
        "        out_dir = dest_root / \"dataset_remote\"\n",
        "        if out_dir.exists(): shutil.rmtree(out_dir)\n",
        "        out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # extrair primeira camada\n",
        "        lower = archive_path.name.lower()\n",
        "        try:\n",
        "            if lower.endswith(\".zip\"):\n",
        "                with zipfile.ZipFile(str(archive_path), \"r\") as zf:\n",
        "                    zf.extractall(str(out_dir))\n",
        "            elif lower.endswith((\".tar.gz\", \".tgz\", \".tar\")):\n",
        "                with tarfile.open(str(archive_path), \"r:*\") as tf:\n",
        "                    tf.extractall(str(out_dir))\n",
        "            else:\n",
        "                try:\n",
        "                    with zipfile.ZipFile(str(archive_path), \"r\") as zf:\n",
        "                        zf.extractall(str(out_dir))\n",
        "                except zipfile.BadZipFile:\n",
        "                    with tarfile.open(str(archive_path), \"r:*\") as tf:\n",
        "                        tf.extractall(str(out_dir))\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Falha ao extrair: {e}\")\n",
        "\n",
        "        # extrair camadas internas até 2 níveis (se houver)\n",
        "        def _extract_nested_archives(root: Path, max_depth: int = 2):\n",
        "            for _ in range(max_depth):\n",
        "                inner = [p for p in root.rglob(\"*\") if p.is_file() and (\n",
        "                    p.name.lower().endswith(\".zip\") or\n",
        "                    p.name.lower().endswith(\".tar\") or\n",
        "                    p.name.lower().endswith(\".tar.gz\") or\n",
        "                    p.name.lower().endswith(\".tgz\") or\n",
        "                    p.name.lower().endswith(\".gz\")\n",
        "                )]\n",
        "                any_new = False\n",
        "                for a in inner:\n",
        "                    base = a.name\n",
        "                    for suf in (\".tar.gz\", \".tgz\", \".zip\", \".tar\", \".gz\"):\n",
        "                        if base.endswith(suf):\n",
        "                            base = base[: -len(suf)]\n",
        "                    out = a.parent / (base + \"_extracted\")\n",
        "                    out.mkdir(exist_ok=True)\n",
        "                    try:\n",
        "                        if a.name.lower().endswith(\".zip\"):\n",
        "                            with zipfile.ZipFile(str(a), \"r\") as zf: zf.extractall(str(out))\n",
        "                            any_new = True\n",
        "                        else:\n",
        "                            with tarfile.open(str(a), \"r:*\") as tf: tf.extractall(str(out))\n",
        "                            any_new = True\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                if not any_new:\n",
        "                    break\n",
        "\n",
        "        _extract_nested_archives(out_dir, max_depth=2)\n",
        "        _debug_list_some_files(out_dir)\n",
        "\n",
        "        # já existe positives/negatives?\n",
        "        ds = _find_dataset_root_with_pos_neg(out_dir)\n",
        "        if ds:\n",
        "            print(f\"✔ dataset com positives/negatives encontrado em: {ds}\")\n",
        "            _ensure_negatives_from_caltech101(ds / \"negatives\")\n",
        "            return ds\n",
        "\n",
        "        # tenta reorganizar por heurística\n",
        "        ds = _reorganize_faces_nonfaces(out_dir)\n",
        "        if ds:\n",
        "            _ensure_negatives_from_caltech101(ds / \"negatives\")\n",
        "            if _find_dataset_root_with_pos_neg(ds):\n",
        "                print(f\"✔ dataset reorganizado em: {ds}\")\n",
        "                return ds\n",
        "\n",
        "        # fallback: positivos default\n",
        "        if fallback_to_default and (url != REMOTE_DATASET_URL_DEFAULT):\n",
        "            print(\"[fallback] Estrutura não reconhecida. Baixando POSITIVOS default (Caltech Face 1999)…\")\n",
        "            return _download_and_unpack_archive(REMOTE_DATASET_URL_DEFAULT, dest_root, fallback_to_default=False)\n",
        "\n",
        "        # último recurso: single-class (tudo -> positives) + completar negatives\n",
        "        print(\"[single-class] Convertendo todas as imagens extraídas em 'positives/' e completando 'negatives/'…\")\n",
        "        pos_dir = out_dir / \"positives\"\n",
        "        neg_dir = out_dir / \"negatives\"\n",
        "        pos_dir.mkdir(exist_ok=True); neg_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        imgs = _collect_images(out_dir)\n",
        "        copied = 0\n",
        "        for f in imgs:\n",
        "            # evitar arquivos já nas pastas alvo\n",
        "            try:\n",
        "                if f.is_relative_to(pos_dir) or f.is_relative_to(neg_dir):\n",
        "                    continue\n",
        "            except AttributeError:\n",
        "                # Python < 3.9: fallback\n",
        "                if str(pos_dir) in str(f.parent) or str(neg_dir) in str(f.parent):\n",
        "                    continue\n",
        "            try:\n",
        "                shutil.copy2(f, pos_dir / f.name); copied += 1\n",
        "            except: pass\n",
        "        print(f\"[single-class] Positives criados: {copied}\")\n",
        "\n",
        "        _ensure_negatives_from_caltech101(neg_dir)\n",
        "\n",
        "        if _find_dataset_root_with_pos_neg(out_dir):\n",
        "            print(f\"✔ dataset 'single-class' preparado em: {out_dir}\")\n",
        "            return out_dir\n",
        "\n",
        "        raise ValueError(\"Não foi possível montar positives/ e negatives/ automaticamente.\")\n",
        "    finally:\n",
        "        try: shutil.rmtree(tmp_dir)\n",
        "        except: pass\n",
        "\n",
        "# compat com versões antigas do código\n",
        "_download_and_unpack_zip = _download_and_unpack_archive\n",
        "\n",
        "# ---------- fluxo interativo ----------\n",
        "try:\n",
        "    run_eval_auto = input(\n",
        "        \"Rodar avaliação offline (Automático/Default/Remoto)? [s/N]: \"\n",
        "    ).strip().lower() in (\"s\",\"sim\",\"y\",\"yes\")\n",
        "except Exception:\n",
        "    run_eval_auto = False\n",
        "\n",
        "if run_eval_auto:\n",
        "    print(\"Fonte do dataset:\")\n",
        "    print(\"  1) Automático (enrollment + negatives persistentes/câmera)\")\n",
        "    print(f\"  2) DEFAULT ({DEFAULT_DATASET_DIR})\")\n",
        "    print(\"  3) REMOTO/LOCAL existente (URL .zip/.tar/.tar.gz/.tgz ou caminho local)\")\n",
        "    choice = (input(\"Escolha [1/2/3]: \").strip() or \"1\")\n",
        "\n",
        "    dataset_override = None\n",
        "    default_dir = None\n",
        "\n",
        "    if choice == \"2\":\n",
        "        default_dir = DEFAULT_DATASET_DIR\n",
        "        print(f\"ℹ Usando dataset DEFAULT: {default_dir}\")\n",
        "\n",
        "    elif choice == \"3\":\n",
        "        ds_input = input(\n",
        "            \"Informe URL (.zip/.tar/.tar.gz/.tgz) OU caminho local com positives/ e negatives/ \"\n",
        "            f\"(Enter = usar default {REMOTE_DATASET_URL_DEFAULT}): \"\n",
        "        ).strip()\n",
        "        if not ds_input:\n",
        "            ds_input = REMOTE_DATASET_URL_DEFAULT\n",
        "\n",
        "        if ds_input.startswith((\"http://\",\"https://\")):\n",
        "            dataset_override = _download_and_unpack_archive(ds_input, DATA_DIR)\n",
        "        else:\n",
        "            p = Path(ds_input)\n",
        "            if not p.exists():\n",
        "                raise ValueError(f\"Caminho não existe: {p}\")\n",
        "            if not ((p / \"positives\").is_dir() and (p / \"negatives\").is_dir()):\n",
        "                raise ValueError(f\"Caminho inválido (esperado positives/ e negatives/): {p}\")\n",
        "            dataset_override = p\n",
        "\n",
        "    # Perguntas opcionais (as mesmas que você já tinha)\n",
        "    try:\n",
        "        use_camera = input(\"Usar câmera para completar negativos (também em Remoto/Default)? [S/n]: \").strip().lower()\n",
        "        use_camera = not (use_camera in (\"n\",\"nao\",\"não\"))\n",
        "    except Exception:\n",
        "        use_camera = True\n",
        "\n",
        "    try:\n",
        "        neg_src = input(\"Pasta extra com negativos (vazio = nenhuma): \").strip() or None\n",
        "    except Exception:\n",
        "        neg_src = None\n",
        "\n",
        "    try:\n",
        "        max_imgs = input(\"Limitar nº de imagens por classe? (vazio = sem limite): \").strip()\n",
        "        max_imgs = int(max_imgs) if max_imgs else None\n",
        "    except Exception:\n",
        "        max_imgs = None\n",
        "\n",
        "    try:\n",
        "        th = input(\"Conf threshold (vazio = 0.5): \").strip()\n",
        "        th = float(th) if th else 0.5\n",
        "    except Exception:\n",
        "        th = 0.5\n",
        "\n",
        "    # ---------- chamadas finais ----------\n",
        "    if dataset_override is not None:\n",
        "        # REMOTO/LOCAL (pronto/baixado)\n",
        "        # se negatives/ estiver vazio e o usuário permitir, completa com câmera\n",
        "        neg_dir_check = dataset_override / \"negatives\"\n",
        "        if use_camera and (not any(neg_dir_check.iterdir())):\n",
        "            _ensure_negatives_via_camera(neg_dir_check, frames=40, sleep_sec=0.15)\n",
        "\n",
        "        _ = run_offline_eval_from_enrollment(\n",
        "            eval_max_images=max_imgs,\n",
        "            conf_threshold=th,\n",
        "            use_camera=False,                 # já tratamos câmera acima\n",
        "            negatives_src_dir=None,\n",
        "            dataset_dir_override=dataset_override,\n",
        "            default_dataset_dir=None,\n",
        "        )\n",
        "\n",
        "    elif default_dir is not None:\n",
        "        # DEFAULT\n",
        "        neg_dir_check = default_dir / \"negatives\"\n",
        "        if use_camera and (not any(neg_dir_check.iterdir())):\n",
        "            _ensure_negatives_via_camera(neg_dir_check, frames=40, sleep_sec=0.15)\n",
        "\n",
        "        _ = run_offline_eval_from_enrollment(\n",
        "            eval_max_images=max_imgs,\n",
        "            conf_threshold=th,\n",
        "            use_camera=False,                 # já tratamos câmera acima\n",
        "            negatives_src_dir=None,\n",
        "            dataset_dir_override=None,\n",
        "            default_dataset_dir=default_dir,\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        # AUTOMÁTICO (enrollment + negativos persistentes/câmera/pasta extra)\n",
        "        _ = run_offline_eval_from_enrollment(\n",
        "            neg_target_min=30,\n",
        "            capture_batch=30,\n",
        "            capture_sleep_sec=0.15,\n",
        "            eval_max_images=max_imgs,\n",
        "            conf_threshold=th,\n",
        "            use_camera=use_camera,            # aqui a câmera ainda vale dentro da função\n",
        "            negatives_src_dir=neg_src,\n",
        "            dataset_dir_override=None,\n",
        "            default_dataset_dir=None,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c1ab4dd",
      "metadata": {
        "id": "9c1ab4dd"
      },
      "source": [
        "\n",
        "## 15) FaceNet (Embeddings) — Reconhecedor Opcional\n",
        "\n",
        "Esta seção adiciona **FaceNet** (via `facenet-pytorch`) como alternativa ao LBPH.  \n",
        "A autenticação passa a comparar **vetores de embeddings** por **cosine similarity** (ou L2).\n",
        "\n",
        "> **Pré-requisitos**: funcionar melhor em GPU (Colab). Em CPU roda, mas fica mais lento.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "596d4860",
      "metadata": {
        "id": "596d4860"
      },
      "source": [
        "\n",
        "### 15.1) Instalação (Colab/Local)\n",
        "Se estiver no **Colab**, instale as dependências abaixo. Em ambiente local, use seu gerenciador de pacotes preferido.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EfHO1mkn_wHq",
      "metadata": {
        "id": "EfHO1mkn_wHq"
      },
      "source": [
        "Colab (CUDA 12.x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E6Iotnr70qEi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E6Iotnr70qEi",
        "outputId": "e6bdf82f-6ac5-4453-9bb6-7a61c68126d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 2.8.0\n",
            "Uninstalling torch-2.8.0:\n",
            "  Successfully uninstalled torch-2.8.0\n",
            "Found existing installation: torchvision 0.17.0\n",
            "Uninstalling torchvision-0.17.0:\n",
            "  Successfully uninstalled torchvision-0.17.0\n",
            "Found existing installation: torchaudio 2.8.0\n",
            "Uninstalling torchaudio-2.8.0:\n",
            "  Successfully uninstalled torchaudio-2.8.0\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (780.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.4/780.4 MB\u001b[0m \u001b[31m663.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp312-cp312-linux_x86_64.whl (7.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m690.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m599.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.1.0 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m461.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Collecting sympy==1.13.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.8.93)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (10.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Installing collected packages: triton, sympy, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.4.0\n",
            "    Uninstalling triton-3.4.0:\n",
            "      Successfully uninstalled triton-3.4.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.3\n",
            "    Uninstalling sympy-1.13.3:\n",
            "      Successfully uninstalled sympy-1.13.3\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.8.90\n",
            "    Uninstalling nvidia-nvtx-cu12-12.8.90:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.8.90\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
            "    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.8.90\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.8.90:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.8.90\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.8.93\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.8.93:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.8.93\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.8.90\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.8.90:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.8.90\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "facenet-pytorch 2.6.0 requires torch<2.3.0,>=2.2.0, but you have torch 2.5.1+cu121 which is incompatible.\n",
            "facenet-pytorch 2.6.0 requires torchvision<0.18.0,>=0.17.0, but you have torchvision 0.20.1+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.21.5 nvidia-nvtx-cu12-12.1.105 sympy-1.13.1 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121 triton-3.1.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "fee2c43d90b44f438a01c8c919b6d1b8",
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: facenet-pytorch in /usr/local/lib/python3.12/dist-packages (2.6.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (1.26.4)\n",
            "Requirement already satisfied: Pillow<10.3.0,>=10.2.0 in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (10.2.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (2.32.4)\n",
            "Collecting torch<2.3.0,>=2.2.0 (from facenet-pytorch)\n",
            "  Using cached torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting torchvision<0.18.0,>=0.17.0 (from facenet-pytorch)\n",
            "  Using cached torchvision-0.17.2-cp312-cp312-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from facenet-pytorch) (4.67.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.0.106)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch<2.3.0,>=2.2.0->facenet-pytorch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.12/dist-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.3.0,>=2.2.0->facenet-pytorch) (12.8.93)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet-pytorch) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch<2.3.0,>=2.2.0->facenet-pytorch) (1.3.0)\n",
            "Using cached torch-2.2.2-cp312-cp312-manylinux1_x86_64.whl (755.5 MB)\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# 1) Limpe versões conflitantes (opcional, mas recomendado)\n",
        "!pip uninstall -y torch torchvision torchaudio\n",
        "# 2) Instale PyTorch compatível com a CUDA do Colab (12.1 hoje)\n",
        "!pip install -U --index-url https://download.pytorch.org/whl/cu121 torch torchvision torchaudio\n",
        "# 3) Instale o facenet-pytorch\n",
        "!pip install -U facenet-pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xpU7YuaF_poJ",
      "metadata": {
        "id": "xpU7YuaF_poJ"
      },
      "source": [
        "CPU only (sem GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ztqCzXq-1ItH",
      "metadata": {
        "id": "ztqCzXq-1ItH"
      },
      "outputs": [],
      "source": [
        "#!pip uninstall -y torch torchvision torchaudio\n",
        "#!pip install -U --index-url https://download.pytorch.org/whl/cpu torch torchvision torchaudio\n",
        "#!pip install -U facenet-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ESO5PNCj0_gk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "08d5e8b536974168b23a469e3d749cd6",
            "55ad963f2b8a4ca28265d8a8aebdbe20",
            "4755416cf99e4fa396e0f6ea355845ab",
            "7f817e7ff41e4f9bab61540507b30cf3",
            "915fb19be01c4dcc8110992535ecdddb",
            "be4df0928ab247ee8573d6f75cd3ae07",
            "96a521bea3524fc8b3503710fb0f449a",
            "f5748b2fb6494e4d9abcd7067307649e",
            "3d40a722f79a46ba8a63635d626b81b7",
            "2ee413e79deb4989aba8db560234d590",
            "49f123251241473c98e17a740c29e0e8"
          ]
        },
        "id": "ESO5PNCj0_gk",
        "outputId": "896501a7-29a4-48b6-d80a-9f10826c2543"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch: 2.5.1+cu121 cuda? False\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08d5e8b536974168b23a469e3d749cd6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/107M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/facenet_pytorch/models/inception_resnet_v1.py:329: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(cached_file)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "facenet-pytorch OK\n"
          ]
        }
      ],
      "source": [
        "import torch, torchvision\n",
        "print(\"torch:\", torch.__version__, \"cuda?\", torch.cuda.is_available())\n",
        "import facenet_pytorch\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "_ = InceptionResnetV1(pretrained='vggface2').eval()\n",
        "print(\"facenet-pytorch OK\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44b21cff",
      "metadata": {
        "id": "44b21cff"
      },
      "source": [
        "\n",
        "### 15.2) Carregamento do Modelo & Utilitários\n",
        "\n",
        "Usamos **InceptionResnetV1 (VGGFace2)** como extrator de embeddings e o **detector já existente** (`detect_faces`) deste notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0442aaf9",
      "metadata": {
        "id": "0442aaf9"
      },
      "outputs": [],
      "source": [
        "\n",
        "from typing import Dict, Tuple, Optional\n",
        "import numpy as np\n",
        "\n",
        "# Tentamos importar aqui; se falhar, o usuário instala na célula anterior\n",
        "try:\n",
        "    import torch\n",
        "    from facenet_pytorch import InceptionResnetV1\n",
        "except Exception as e:\n",
        "    print(\"Não foi possível importar FaceNet. Execute a célula de instalação (15.1) e tente novamente. Erro:\", e)\n",
        "    torch = None\n",
        "    InceptionResnetV1 = None\n",
        "\n",
        "FACENET_DEVICE = \"cuda\" if (hasattr(torch, \"cuda\") and torch.cuda.is_available()) else \"cpu\"\n",
        "\n",
        "_FACENET_MODEL = None\n",
        "\n",
        "def get_facenet_model():\n",
        "    global _FACENET_MODEL\n",
        "    if _FACENET_MODEL is None:\n",
        "        if InceptionResnetV1 is None:\n",
        "            raise RuntimeError(\"Facenet não está disponível. Instale facenet-pytorch/torch e reexecute.\")\n",
        "        _FACENET_MODEL = InceptionResnetV1(pretrained='vggface2').eval().to(FACENET_DEVICE)\n",
        "    return _FACENET_MODEL\n",
        "\n",
        "def bgr_to_rgb(img_bgr):\n",
        "    return img_bgr[:, :, ::-1]\n",
        "\n",
        "def crop_and_resize_face(img_bgr, x, y, w, h, size=160):\n",
        "    # recorte com margem opcional\n",
        "    x0 = max(0, x); y0 = max(0, y)\n",
        "    x1 = min(img_bgr.shape[1], x + w)\n",
        "    y1 = min(img_bgr.shape[0], y + h)\n",
        "    face = img_bgr[y0:y1, x0:x1]\n",
        "    face = cv2.resize(face, (size, size), interpolation=cv2.INTER_LINEAR)\n",
        "    face_rgb = bgr_to_rgb(face).astype(np.float32) / 255.0\n",
        "    # normaliza como tensor [C,H,W]\n",
        "    face_rgb = np.transpose(face_rgb, (2, 0, 1))\n",
        "    return face_rgb\n",
        "\n",
        "def facenet_embedding_from_bgr_face(img_bgr, x, y, w, h):\n",
        "    model = get_facenet_model()\n",
        "    face_rgb_chw = crop_and_resize_face(img_bgr, x, y, w, h, size=160)\n",
        "    with torch.no_grad():\n",
        "        tens = torch.from_numpy(face_rgb_chw).unsqueeze(0).to(FACENET_DEVICE)  # [1,3,160,160]\n",
        "        emb = model(tens).cpu().numpy().reshape(-1)  # 512-dim\n",
        "    return emb\n",
        "\n",
        "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    if a.ndim > 1: a = a.ravel()\n",
        "    if b.ndim > 1: b = b.ravel()\n",
        "    denom = (np.linalg.norm(a) * np.linalg.norm(b)) + 1e-9\n",
        "    return float(np.dot(a, b) / denom)\n",
        "\n",
        "def l2_distance(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    return float(np.linalg.norm(a - b))\n",
        "\n",
        "def build_facenet_db(enroll_dir: str) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"Cria um banco simples com 1 embedding médio por usuário a partir das imagens de ENROLL_DIR.\n",
        "    Reaproveita a função detect_faces() já definida no notebook.\n",
        "    \"\"\"\n",
        "    import os, glob\n",
        "    db = {}\n",
        "    for user in sorted(os.listdir(enroll_dir)):\n",
        "        user_dir = os.path.join(enroll_dir, user)\n",
        "        if not os.path.isdir(user_dir):\n",
        "            continue\n",
        "        embs = []\n",
        "        for p in glob.glob(os.path.join(user_dir, \"*\")):\n",
        "            try:\n",
        "                img = cv2.imread(p)\n",
        "                if img is None:\n",
        "                    continue\n",
        "                faces = detect_faces(img)  # usa o detector configurado (Haar/DNN)\n",
        "                if len(faces) == 0:\n",
        "                    continue\n",
        "                # pega a face com maior score (ou a primeira)\n",
        "                faces_sorted = sorted(faces, key=lambda f: f[-1] if len(f) == 5 else 0.0, reverse=True)\n",
        "                x, y, w, h = faces_sorted[0][:4]\n",
        "                emb = facenet_embedding_from_bgr_face(img, x, y, w, h)\n",
        "                embs.append(emb)\n",
        "            except Exception as _:\n",
        "                pass\n",
        "        if embs:\n",
        "            db[user] = np.mean(np.stack(embs, axis=0), axis=0)\n",
        "    return db\n",
        "\n",
        "def facenet_predict_user(embedding: np.ndarray, db: Dict[str, np.ndarray],\n",
        "                         metric: str = \"cosine\") -> Tuple[Optional[str], float]:\n",
        "    \"\"\"Retorna (user, score).\n",
        "    Para cosine: quanto MAIOR melhor (1.0 = idêntico).\n",
        "    Para L2: quanto MENOR melhor (0.0 = idêntico).\n",
        "    \"\"\"\n",
        "    if not db:\n",
        "        return None, 0.0\n",
        "    best_user = None\n",
        "    best_score = -1.0 if metric == \"cosine\" else 1e9\n",
        "    for user, ref in db.items():\n",
        "        if metric == \"cosine\":\n",
        "            s = cosine_similarity(embedding, ref)\n",
        "            if s > best_score:\n",
        "                best_score = s\n",
        "                best_user = user\n",
        "        else:\n",
        "            s = l2_distance(embedding, ref)\n",
        "            if s < best_score:\n",
        "                best_score = s\n",
        "                best_user = user\n",
        "    return best_user, float(best_score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01d9efbc",
      "metadata": {
        "id": "01d9efbc"
      },
      "source": [
        "\n",
        "### 15.3) Autenticação com FaceNet (1:1 e 1:N)\n",
        "\n",
        "- **1:1**: confere se a face capturada corresponde ao `expected_user` usando similaridade **cosine** (padrão).  \n",
        "- **1:N**: identifica o usuário mais próximo no banco de embeddings.\n",
        "- Integração com **liveness** opcional, reutilizando a função já existente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "04b3ab6a",
      "metadata": {
        "id": "04b3ab6a"
      },
      "outputs": [],
      "source": [
        "import time, os, statistics, collections\n",
        "import numpy as np, cv2\n",
        "\n",
        "def _check_prereqs():\n",
        "    missing = []\n",
        "    for name in [\"detect_faces\", \"facenet_embedding_from_bgr_face\", \"build_facenet_db\", \"ENROLL_DIR\", \"update_display_img\"]:\n",
        "        if name not in globals():\n",
        "            missing.append(name)\n",
        "    if missing:\n",
        "        raise RuntimeError(f\"As seguintes funções/variáveis precisam estar definidas antes: {missing}\")\n",
        "    _ = get_facenet_model()  # garante FaceNet carregado (gera mensagem clara caso falhe)\n",
        "\n",
        "def _draw_label(img, text, y=24, color=(0,255,0)):\n",
        "    cv2.putText(img, text, (10, y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2, cv2.LINE_AA)\n",
        "\n",
        "def authenticate_facenet_preview_runner(\n",
        "    mode=\"1vN\",                 # \"1v1\" ou \"1vN\"\n",
        "    expected_user=None,         # obrigatório em 1v1\n",
        "    metric=\"cosine\",            # \"cosine\" ou \"l2\"\n",
        "    cosine_accept=0.7,          # limiar p/ cosine\n",
        "    l2_accept=1.0,              # limiar p/ L2 (ajuste conforme dataset)\n",
        "    require_liveness=False,\n",
        "    max_frames=200,\n",
        "    width=640,\n",
        "    height=480,\n",
        "    show_raw=True,              # mostra o <video> ao vivo no Colab\n",
        "    log_every=5,                # log a cada N frames com face\n",
        "    max_evidences=3             # nº máx de imagens salvas\n",
        "):\n",
        "    _check_prereqs()\n",
        "\n",
        "    # Monta DB (1 embedding médio por usuário)\n",
        "    db = build_facenet_db(ENROLL_DIR)\n",
        "    if not db:\n",
        "        print(\"⚠️ Nenhum usuário no ENROLL_DIR; faça 'novo' (enrollment) primeiro.\")\n",
        "        return {\"status\": \"fail\", \"reason\": \"empty_db\"}\n",
        "\n",
        "    print(f\"[FaceNet Runner] mode={mode} metric={metric} cos_accept={cosine_accept} l2_accept={l2_accept}\")\n",
        "    if mode == \"1v1\" and not expected_user:\n",
        "        print(\"⚠️ expected_user não informado para 1v1\")\n",
        "        return {\"status\": \"fail\", \"reason\": \"missing_expected_user\"}\n",
        "\n",
        "    # Estado de liveness\n",
        "    prev_gray = None\n",
        "    energy_acc, frames_count = 0.0, 0\n",
        "\n",
        "    # Logs e evidências\n",
        "    pred_logs = []              # lista de (user, score)\n",
        "    evid_paths = []\n",
        "    users_counter = collections.Counter()\n",
        "\n",
        "    os.makedirs(EVIDENCE_DIR, exist_ok=True)\n",
        "\n",
        "    t0 = time.time()\n",
        "    ok_faces = 0\n",
        "    try:\n",
        "        for i in range(max_frames):\n",
        "            frame = capture_one(width=width, height=height, show_raw=show_raw)\n",
        "            if frame is None:\n",
        "                time.sleep(0.05)\n",
        "                continue\n",
        "\n",
        "            disp = frame.copy()\n",
        "\n",
        "            # Liveness (opcional)\n",
        "            if require_liveness:\n",
        "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "                if prev_gray is not None:\n",
        "                    diff = cv2.absdiff(gray, prev_gray)\n",
        "                    energy_acc += float(np.mean(diff))\n",
        "                prev_gray = gray\n",
        "                frames_count += 1\n",
        "\n",
        "            faces = detect_faces(frame)\n",
        "            if len(faces) > 0:\n",
        "                ok_faces += 1\n",
        "                faces_sorted = sorted(faces, key=lambda f: f[-1] if len(f)==5 else 0.0, reverse=True)\n",
        "                x, y, w, h = faces_sorted[0][:4]\n",
        "                cv2.rectangle(disp, (x,y), (x+w, y+h), (0,255,0), 2)\n",
        "\n",
        "                emb = facenet_embedding_from_bgr_face(frame, x, y, w, h)\n",
        "\n",
        "                if mode == \"1v1\":\n",
        "                    ref = db.get(expected_user)\n",
        "                    if ref is None:\n",
        "                        _draw_label(disp, f\"{expected_user} não encontrado\", 24, (0,0,255))\n",
        "                    else:\n",
        "                        if metric == \"cosine\":\n",
        "                            score = cosine_similarity(emb, ref)\n",
        "                            verdict = (score >= cosine_accept)\n",
        "                            label = f\"{expected_user} | cos={score:.3f} | {'OK' if verdict else 'NEG'}\"\n",
        "                        else:\n",
        "                            score = l2_distance(emb, ref)\n",
        "                            verdict = (score <= l2_accept)\n",
        "                            label = f\"{expected_user} | l2={score:.3f} | {'OK' if verdict else 'NEG'}\"\n",
        "                        _draw_label(disp, label, 24, (0,255,0) if verdict else (0,0,255))\n",
        "                        pred_logs.append((expected_user, float(score)))\n",
        "                        users_counter[expected_user] += 1\n",
        "\n",
        "                        # salva evidências (1v1) — primeiras N imagens\n",
        "                        if len(evid_paths) < max_evidences:\n",
        "                            ts = int(time.time()*1000)\n",
        "                            out_path = os.path.join(EVIDENCE_DIR, f\"facenet_1v1_{expected_user}_{score:.3f}_{ts}.jpg\")\n",
        "                            cv2.imwrite(out_path, disp)\n",
        "                            evid_paths.append(out_path)\n",
        "\n",
        "                else:\n",
        "                    # 1:N — encontra o mais próximo e loga\n",
        "                    if metric == \"cosine\":\n",
        "                        best_user = max(db.keys(), key=lambda u: cosine_similarity(emb, db[u]))\n",
        "                        best_score = cosine_similarity(emb, db[best_user])\n",
        "                        label = f\"{best_user} | cos={best_score:.3f}\"\n",
        "                    else:\n",
        "                        best_user = min(db.keys(), key=lambda u: l2_distance(emb, db[u]))\n",
        "                        best_score = l2_distance(emb, db[best_user])\n",
        "                        label = f\"{best_user} | l2={best_score:.3f}\"\n",
        "\n",
        "                    _draw_label(disp, label, 24, (255,255,255))\n",
        "\n",
        "                    pred_logs.append((best_user, float(best_score)))\n",
        "                    users_counter[best_user] += 1\n",
        "\n",
        "                    # salva evidências (1vN) — primeiras N imagens\n",
        "                    if len(evid_paths) < max_evidences:\n",
        "                        ts = int(time.time()*1000)\n",
        "                        out_path = os.path.join(EVIDENCE_DIR, f\"facenet_1vN_{best_user}_{best_score:.3f}_{ts}.jpg\")\n",
        "                        cv2.imwrite(out_path, disp)\n",
        "                        evid_paths.append(out_path)\n",
        "\n",
        "                    # log a cada N frames com face\n",
        "                    if ok_faces % log_every == 0:\n",
        "                        print(f\"[pred] frame#{i:03d} -> user={best_user} score={best_score:.3f}\")\n",
        "\n",
        "            # Liveness status on-screen\n",
        "            if require_liveness and frames_count > 0:\n",
        "                avg_energy = energy_acc / max(1, frames_count)\n",
        "                live_ok = (avg_energy >= LIVENESS_MIN_ENERGY)\n",
        "                _draw_label(disp, f\"liveness={'OK' if live_ok else 'LOW'} ({avg_energy:.1f})\", 50, (255,255,0))\n",
        "\n",
        "            # atualiza a <img id=\"output\"> no Colab\n",
        "            update_display_img(disp)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"⏹️ Interrompido pelo usuário.\")\n",
        "    finally:\n",
        "        try:\n",
        "            stop_camera_ui()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    dt = time.time() - t0\n",
        "\n",
        "    # ===== Resumo final =====\n",
        "    mean_score = statistics.mean([s for (_, s) in pred_logs]) if pred_logs else 0.0\n",
        "    top_user = users_counter.most_common(1)[0][0] if users_counter else None\n",
        "\n",
        "    print(f\"[FaceNet Runner] terminado em {dt:.1f}s | frames_com_face={ok_faces}/{max_frames}\")\n",
        "    if mode == \"1vN\":\n",
        "        print(f\"[Resumo 1vN] top_user={top_user} | mean_score={mean_score:.3f} | n_preds={len(pred_logs)}\")\n",
        "    else:\n",
        "        print(f\"[Resumo 1v1] user={expected_user} | mean_score={mean_score:.3f} | n_preds={len(pred_logs)}\")\n",
        "    if evid_paths:\n",
        "        print(\"Evidências salvas:\")\n",
        "        for p in evid_paths:\n",
        "            print(\" -\", p)\n",
        "\n",
        "    return {\n",
        "        \"status\": \"ok\",\n",
        "        \"mode\": mode,\n",
        "        \"metric\": metric,\n",
        "        \"faces_seen\": ok_faces,\n",
        "        \"elapsed_sec\": dt,\n",
        "        \"mean_score\": mean_score,\n",
        "        \"top_user\": top_user,\n",
        "        \"pred_counts\": dict(users_counter),\n",
        "        \"evidences\": evid_paths,\n",
        "        \"n_preds\": len(pred_logs),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "974cf8bb",
      "metadata": {
        "id": "974cf8bb"
      },
      "source": [
        "\n",
        "### 15.4) Runner (FaceNet) — Exemplo de Uso\n",
        "\n",
        "Use este runner independente para testar **FaceNet** sem alterar o runner original do notebook.  \n",
        "- Para **1:1**: faça `novo` (enrollment) pelo runner original, depois execute abaixo com `expected_user`.\n",
        "- Para **1:N**: basta ter **dois ou mais** diretórios em `ENROLL_DIR`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "00f86858",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "00f86858",
        "outputId": "3857f8f5-753e-4bf6-a0f8-a046621ab85d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[FaceNet Runner] mode=1v1 metric=cosine cos_accept=0.7 l2_accept=1.0\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n        (async () => {\n          let container = document.getElementById('camera-container');\n          if (!container) {\n            container = document.createElement('div');\n            container.id = 'camera-container';\n            container.style = 'display:flex; gap:12px; align-items:flex-start; justify-content:center; flex-wrap:wrap;';\n            document.body.appendChild(container);\n            container.innerHTML = `\n              <div id=\"cam-left\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <video id=\"webcam\" autoplay playsinline muted style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\"></video>\n                <small style=\"color:#555\">Pré-visualização (ao vivo)</small>\n              </div>\n              <div id=\"cam-right\" style=\"display:flex; flex-direction:column; align-items:center; gap:6px;\">\n                <img id=\"output\" style=\"max-width:640px; border:1px solid #ddd; border-radius:8px;\">\n                <small style=\"color:#555\">Frame processado</small>\n              </div>\n              <canvas id=\"canvas\" style=\"display:none;\"></canvas>\n            `;\n          } else {\n            const left = document.getElementById('cam-left');\n            if (left) left.style.display = \"flex\";\n          }\n\n          if (!window._colabStream || window._colabStreamInactive) {\n            try {\n              window._colabStream = await navigator.mediaDevices.getUserMedia({ video: {width:640, height:480}, audio:false });\n              window._colabStreamInactive = false;\n            } catch (e) {\n              console.error('getUserMedia failed', e);\n              return false;\n            }\n          }\n          const video = document.getElementById('webcam');\n          if (video && video.srcObject !== window._colabStream) {\n            video.srcObject = window._colabStream;\n          }\n\n          const canvas = document.getElementById('canvas');\n          window.captureFrame = () => {\n            const ctx = canvas.getContext('2d');\n            const vw = (video && video.videoWidth) ? video.videoWidth : 640;\n            const vh = (video && video.videoHeight) ? video.videoHeight : 480;\n            canvas.width = vw; canvas.height = vh;\n            if (video) ctx.drawImage(video, 0, 0, vw, vh);\n            return canvas.toDataURL('image/jpeg', 0.9);\n          };\n          window.updateProcessed = (b64) => {\n            const img = document.getElementById('output');\n            if (img) img.src = 'data:image/jpeg;base64,' + b64;\n          };\n          window.stopColabCamera = () => {\n            try {\n              if (window._colabStream && !window._colabStreamInactive) {\n                window._colabStream.getTracks().forEach(t => t.stop());\n                window._colabStreamInactive = true;\n              }\n            } catch (e) { console.warn(e); }\n            const c = document.getElementById('camera-container');\n            if (c) c.remove();\n          };\n          return true;\n        } )();\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⏹️ Interrompido pelo usuário.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "if (window.stopColabCamera) window.stopColabCamera();",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[FaceNet Runner] terminado em 19.6s | frames_com_face=20/200\n",
            "[Resumo 1v1] user=teste | mean_score=0.762 | n_preds=20\n",
            "Evidências salvas:\n",
            " - cv_colab_data/evidence/facenet_1v1_teste_0.774_1759493393125.jpg\n",
            " - cv_colab_data/evidence/facenet_1v1_teste_0.784_1759493393941.jpg\n",
            " - cv_colab_data/evidence/facenet_1v1_teste_0.757_1759493394752.jpg\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'status': 'ok',\n",
              " 'mode': '1v1',\n",
              " 'metric': 'cosine',\n",
              " 'faces_seen': 20,\n",
              " 'elapsed_sec': 19.62465262413025,\n",
              " 'mean_score': 0.7624066815190306,\n",
              " 'top_user': 'teste',\n",
              " 'pred_counts': {'teste': 20},\n",
              " 'evidences': ['cv_colab_data/evidence/facenet_1v1_teste_0.774_1759493393125.jpg',\n",
              "  'cv_colab_data/evidence/facenet_1v1_teste_0.784_1759493393941.jpg',\n",
              "  'cv_colab_data/evidence/facenet_1v1_teste_0.757_1759493394752.jpg'],\n",
              " 'n_preds': 20}"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1:N — identifica entre os usuários cadastrados em ENROLL_DIR\n",
        "\"\"\"\n",
        "authenticate_facenet_preview_runner(\n",
        "    mode=\"1vN\",\n",
        "    metric=\"cosine\",\n",
        "    cosine_accept=0.7,     # ajuste 0.6~0.8 conforme seu caso\n",
        "    require_liveness=False,\n",
        "    max_frames=200,\n",
        "    width=640, height=480,\n",
        "    show_raw=True          # mostra o <video> \"ao vivo\" no Colab\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "# 1:1 — compara contra um usuário específico\n",
        "authenticate_facenet_preview_runner(\n",
        "    mode=\"1v1\",\n",
        "    expected_user=\"teste\",\n",
        "    metric=\"cosine\",\n",
        "    cosine_accept=0.7,\n",
        "    require_liveness=True,\n",
        "    max_frames=200\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26e1cf2f",
      "metadata": {
        "id": "26e1cf2f"
      },
      "source": [
        "\n",
        "> **Notas**  \n",
        "> • Limiar típico para **cosine**: `0.6–0.8` (quanto maior, mais estrito). Ajuste conforme seu dataset.  \n",
        "> • O banco usa o **médio dos embeddings** por usuário (robusto a variações).  \n",
        "> • A detecção de faces vem da **célula 5**, então o desempenho depende de `DETECTION_MODEL` (Haar/DNN).  \n",
        "> • Em cenários críticos, considere normalizar iluminação e alinhar o rosto antes do embedding.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "08d5e8b536974168b23a469e3d749cd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_55ad963f2b8a4ca28265d8a8aebdbe20",
              "IPY_MODEL_4755416cf99e4fa396e0f6ea355845ab",
              "IPY_MODEL_7f817e7ff41e4f9bab61540507b30cf3"
            ],
            "layout": "IPY_MODEL_915fb19be01c4dcc8110992535ecdddb"
          }
        },
        "2ee413e79deb4989aba8db560234d590": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d40a722f79a46ba8a63635d626b81b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4755416cf99e4fa396e0f6ea355845ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5748b2fb6494e4d9abcd7067307649e",
            "max": 111898327,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3d40a722f79a46ba8a63635d626b81b7",
            "value": 111898327
          }
        },
        "49f123251241473c98e17a740c29e0e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55ad963f2b8a4ca28265d8a8aebdbe20": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be4df0928ab247ee8573d6f75cd3ae07",
            "placeholder": "​",
            "style": "IPY_MODEL_96a521bea3524fc8b3503710fb0f449a",
            "value": "100%"
          }
        },
        "7f817e7ff41e4f9bab61540507b30cf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ee413e79deb4989aba8db560234d590",
            "placeholder": "​",
            "style": "IPY_MODEL_49f123251241473c98e17a740c29e0e8",
            "value": " 107M/107M [00:00&lt;00:00, 174MB/s]"
          }
        },
        "915fb19be01c4dcc8110992535ecdddb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96a521bea3524fc8b3503710fb0f449a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be4df0928ab247ee8573d6f75cd3ae07": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5748b2fb6494e4d9abcd7067307649e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
